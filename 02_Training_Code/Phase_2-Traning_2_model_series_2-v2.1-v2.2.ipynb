{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN8396LUzpfR"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COYftUOUzpfS"
   },
   "source": [
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZQ2BIvCzpfT"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iajq1W8ipjyK"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573,
     "referenced_widgets": [
      "145cc80490fa42258fe6e5a643b57dd5",
      "c3887400b6d34c30a54c6af94c2b612d",
      "7efa9f507c8546bb9b0b5d47be527c25",
      "74267e9cd64d44b58bdbd7dad03b681d",
      "dd2adf3e30304398b8340253dbd4489a",
      "7c6caf15c5de4a2fb699d034a6ab776c",
      "ebe9490475bc437c92ebc03187e53382",
      "46b625ee9ac041338432f548ee3ab51a",
      "fea4c81baaf84d9b806c1853216c89e2",
      "a151688c648045f399e32dc55dd9a1b3",
      "55b26ab90b1043cb8ed36aab316a45ad",
      "247ee0a6f4e64e5ca0a435d243690d0a",
      "4c1c1bc584cb456c9d07f4ac267cf2c5",
      "cc9479fb190742fd865613680e87e535",
      "ae9317b34ba341c4ac40797da3ac2478",
      "5012456fc82e47aca5a69f52a36cb8f5",
      "e84a5ae6c71d42ea8187ebbdcdb4791e",
      "2c04e534215f4d40b103be09e300b744",
      "d7ae6535329c45009bf5664fbd30bbf5",
      "0e112f4ad6974529a4c4f583e6a73950",
      "9c9b22a85e2049089b2194770637c91e",
      "fb2ce6370b1d4d5d9dbdb98aa236da71",
      "2dff2433978a477582b995bc6abafe68",
      "b998e55dd17b44d7a8c23ac427619036",
      "5210d369018a44d4ad2fbd26190aca9b",
      "64b00e376bb142c88a690757f27b8294",
      "09ce664a0a404087a9fe53a5c2d5316e",
      "1a09813436c24b5ea68e652254288ee0",
      "a469a1af99124ff6974265f45563deea",
      "36eed804652e4c6fb7fe2e969228decd",
      "7b8c883b15d84ca2a12bfd3f7a87101d",
      "d7afd9c01f68473387f009b05d829b3b",
      "f34c9b4f069f4fb281f4e0145f0e818e",
      "a99b117d78a8493c9f6ae80f5bf6ea5a",
      "b74e4e5971684b00b40ca42cf2aebb9e",
      "d9dcda48a37e41f7808a933cfd939ded",
      "23791684cb5349c4853cffb4e72ebe1d",
      "c6514d1ee44141d3bf83fde032d39a46",
      "6769817deaaf45fc8a0efb945570f412",
      "d7465d2830f64de996d4d3a306b97c82",
      "a68cbb7211b448c78cd418a90b908037",
      "eb052cc7147d474597529c462497b731",
      "2aefab33f6f345869c19899ca2952df1",
      "881919ffdd7940839c8b40edba7f8c01",
      "b8c184cdaa6b4d72ad28a604d1a8fa39",
      "179e343762ba440cbc094e0e85b4ee84",
      "ecd44b2e03794d5783ffbf07d8b8619f",
      "28d439d23bf54688963517fbdb482339",
      "7e7ac790b8af4cfb978bfe8ac8499900",
      "3cb88502cb1643a8927049baf40d56b7",
      "c37a4b08afa84b64b40ebbe08cbfb018",
      "2de7e147b2e14db38243c7656a29f0e4",
      "bce369933ce540d7b6ec670b04d7f1a4",
      "0940df31fc9047ccae4870b7d2c89b3d",
      "0b3d64dd05f841d68ad472ce933e36d7",
      "2068eb23121440ec83d8cd117b4c6ba5",
      "7054b1bcb73e4515afd29b42a32b20c5",
      "09cd31746a174e96bb346e1afc7b3c8b",
      "802bdf3c4293448bb00b625722f1a9f6",
      "f7c27321d53047479c1ed45b5d5109f6",
      "55a9f6bcb83d4a98a9efcf18253b5091",
      "5e18861f3fa24666869822349d1de377",
      "64eb3128b25448b48268ec61cac289d1",
      "bc277d60ad4a419a94ded28a1c27a9f4",
      "57ae3d07d1244ba495573895ff11e28e",
      "0e997f45717c44e1944d510ae6c51fb9",
      "9d02fcd7882345dea97be6decb5293a5",
      "7b34f538a42f4a9ba9de379ae57f0134",
      "1f9eba179bf847dcb47dbda34611459a",
      "844cd70bf80d40f79c520caf1d7e2a5b",
      "fa8721e596b74611945a1d76e9deff8f",
      "968919dd5d5f41ce91da5cdea02b438b",
      "31d4fb1807bc4ec0840d63ef0bf2e0f9",
      "f8f15be1afc44472bff560ca7316873a",
      "a40e10f372644d80b2830d0f42fcde6c",
      "2f4608614780453a826e3ad59817d7ae",
      "c89e5721ed7e48b295ccc74b841ec61e",
      "e3ecd73a9b86479e8596d8bb1ef5791d",
      "911bd3f394ce4394be92e50782d6ae39",
      "ce05df3e26834837b2db818657a9d175",
      "b25399bf6ae4414ba2836733f59e4ab0",
      "a81ef4374fde4d10a1cdc5daeec34dda",
      "dd1b79ecec114d28ace8c1b8b1fd4d5d",
      "d4b3982b73d046478f7c9b561ed42298",
      "63abaade8f464ed6bbd51d77014dfe66",
      "ef69f52b4c7a485b8708f171bb6acbd6",
      "92be8fd7a814466c983d689bd5d6e1a9",
      "7a3a67547e2043a0ac74b49c47009954",
      "196f35f21b97476a9814acd96dbec717",
      "3ad6055d2ba2481c83b1b136e5898986",
      "8147cc77ce3941c290982d21c42f9f66",
      "f1af17f7a7ee405dabd07f41b6b786d7",
      "aeb720eea25149deb34e6844a8bdd2c5",
      "7d574f195dfc4bc4b1ca86863d97e597",
      "c8faa5be5b88425298b5655a8f507a2a",
      "93a91db5fd6147c5a7982496f09c5b8b",
      "5132d82c92ac41d2b592bf6eebf92c19",
      "4b19bb0b66b248fdbcd01b6264e2ff02",
      "689f645af24947e8920b631d2aa7c3ad",
      "d9c143d31e494981b188be41bd52118f",
      "67a67c8affbe4ec38f99cbb0a3c52dcc",
      "0d0852d9ebb2409ea51650f538dd1621",
      "904f626a367745979ac664f4d2ea6409",
      "e0260495036b406fbf462b3c387205d2",
      "a86e54867d5141dfb1e31ed2d706434f",
      "9864096e9bb54228bf1d9e581b8485bc",
      "062f278ab1c94d8099e06074e0cd360c",
      "3ad57df96bec4267a220a739cfc72bc1",
      "e976be66ed774ce880463df39d0c25ce",
      "67e43763f2f7457487d8b5bfca51b8e4",
      "b1542f5b57f14b98be813e6b2540bb80",
      "34b0ab119eab40eda8b7a551642e0e31",
      "c519c7b69d70467c875a3d228e6b6fd2",
      "cd8774e4577a4652863469d3cb75f2ee",
      "40b0b562564b4e969c02902b1bbea6e8",
      "71c37d1f12294e858ceb337d2e37e375",
      "00d671d686af43c38b12a9448c5bbf06",
      "f85353b1b37342859c9aa71442781e95",
      "73d6e9b4704f40aab25fecd24154f9bc",
      "2167f3dc7050467a9f19f3f885cfb09b",
      "569ad7b2350d46549b2f385a126426d5",
      "6910d714dc854e959839e57b5123af86",
      "77f86f331a19461d94e4840213b256f6",
      "fcd0ee642395431e92a681ba8d829b20",
      "12d7cac449954aafa26c6b5bcb6e031f",
      "4ba6022d4efc4c2ebfdf87b288ed9fd4",
      "0fab32e1222f4431a255a36df0a22a35",
      "7013b9ae0d8a4bbfa744a5a3e3c18a1e",
      "3ddcfbdbb0fc49b0b53a296cdb2348b8",
      "2b72ac21d79c459395682f6692c4325f",
      "26dbcdc380e1404687020cbd9bf38513",
      "2abd7191bff846198b2fb033da32f8c8",
      "60c6d4d43a6445e78051c96a462d7c20",
      "06c646d514b448628424dc8c772994de",
      "7c33f0f45eca43cb8015416999b884a1",
      "3c2e07218b764906a278d6a367153548",
      "bf94cc1837ba486b895656b41c1e9c99",
      "8be1618a9f8840af8d89103c37ef02ef",
      "f0b8a9e00c0d4cc1aff1e3a748a8f039",
      "b1cd702821234a9f87933d099ef5273c",
      "6be394c3849a41a3937322325836d604",
      "db172381abdf4bb0a84417882fc462be",
      "0ab824fcbf0e46f7bbe75af9f662c31a"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "ad27d22a-d0a6-4659-be63-60bf3b3561b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-16 04:40:14 [__init__.py:244] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.6.2: Fast Qwen3 patching. Transformers: 4.52.4. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.418 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-4b-unsloth-bnb-4bit with actual GPU utilization = 69.27%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 44.42 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 27.91 GB. Also swap space = 6 GB.\n",
      "INFO 06-16 04:40:33 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 06-16 04:40:33 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.4.mlp', 'model.layers.6.self_attn', 'model.layers.34.self_attn', 'model.layers.33.self_attn', 'model.layers.4.self_attn', 'model.layers.34.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.0.mlp', 'model.layers.0.self_attn', 'model.layers.3.mlp', 'model.layers.6.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 06-16 04:40:34 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='unsloth/qwen3-4b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-16 04:40:34 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x745af3f58370>\n",
      "INFO 06-16 04:40:35 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-16 04:40:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-16 04:40:35 [gpu_model_runner.py:1595] Starting to load model unsloth/qwen3-4b-unsloth-bnb-4bit...\n",
      "INFO 06-16 04:40:35 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 06-16 04:40:35 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-16 04:40:35 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 06-16 04:40:36 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 06-16 04:40:37 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f7d4bf0a844896af113aca68ee4142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee07ec9a8d44a5ba34f448c0fc5ed99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 04:40:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 06-16 04:40:39 [gpu_model_runner.py:1624] Model loading took 3.5740 GiB and 2.784077 seconds\n",
      "INFO 06-16 04:40:53 [backends.py:462] Using cache directory: /root/.cache/vllm/torch_compile_cache/9bced0cd06/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-16 04:40:53 [backends.py:472] Dynamo bytecode transform time: 12.79 s\n",
      "INFO 06-16 04:41:02 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 8.845 s\n",
      "INFO 06-16 04:41:26 [monitor.py:34] torch.compile takes 12.79 s in total\n",
      "INFO 06-16 04:41:29 [gpu_worker.py:227] Available KV cache memory: 25.36 GiB\n",
      "INFO 06-16 04:41:29 [kv_cache_utils.py:715] GPU KV cache size: 184,672 tokens\n",
      "INFO 06-16 04:41:29 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 45.09x\n",
      "INFO 06-16 04:42:44 [gpu_model_runner.py:2048] Graph capturing finished in 74 secs, took 1.48 GiB\n",
      "INFO 06-16 04:42:44 [core.py:171] init engine (profile, create kv cache, warmup model) took 125.31 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "lora_rank = 64\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B\",\n",
    "    max_seq_length = 4096,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "d50f06c8-4905-4f4e-c6da-980145c41e29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.2 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = lora_rank,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We will use a custom dataset named `Interview_Data_6K.csv`. This dataset contains conversations with a mental health counselling assistant. Each entry has an `instruction` (acting as a system prompt), an `input` (the user's message), and an `output` (the assistant's response).\n",
    "\n",
    "We need to convert this CSV data into a format suitable for training with `SFTTrainer`, specifically by applying the Qwen3 chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "5kyTw2n1edte",
    "outputId": "baed0061-a8b7-4a1c-e8bd-c21825941481"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c10025218664e37a26a612309402e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4521\n",
      "Evaluation set size: 238\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the custom CSV dataset\n",
    "# The dataset has 'instruction', 'input', 'output' columns\n",
    "full_interview_dataset = load_dataset(\"csv\", data_files=\"./dataset/stage_2_1_synthetic_interview_data_combined.csv\", split=\"train\")\n",
    "\n",
    "# Split the dataset into training and evaluation sets (e.g., 90% train, 10% eval)\n",
    "# Ensure the dataset has more than one example for splitting\n",
    "if len(full_interview_dataset) > 1:\n",
    "    train_test_split = full_interview_dataset.train_test_split(test_size=0.05, seed=3407)\n",
    "    interview_train_dataset = train_test_split['train']\n",
    "    interview_eval_dataset = train_test_split['test']\n",
    "    print(f\"Training set size: {len(interview_train_dataset)}\")\n",
    "    print(f\"Evaluation set size: {len(interview_eval_dataset)}\")\n",
    "else:\n",
    "    # Handle cases with very small datasets\n",
    "    interview_train_dataset = full_interview_dataset\n",
    "    interview_eval_dataset = None\n",
    "    print(f\"Training set size: {len(interview_train_dataset)}\")\n",
    "    print(\"No evaluation set created due to small dataset size.\")\n",
    "\n",
    "# We will display the structure of the training dataset in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTZICZtie3lQ"
   },
   "source": [
    "Let's see the structure of our loaded training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjgH3lt0e2Sz",
    "outputId": "2be3f7c6-7353-42c9-d676-95949db681d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 4521\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jfV47_SXgXH4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91809940f594844bae965c4d2613123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f083ed20eba34cb1b1770341ca625a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f555b4e32ad43bf8f3b752f8c10e569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05445dcea6494223a0956a0d8ae0e577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of final formatted training data (after chat template):\n",
      "<|im_start|>system\n",
      "You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description. \n",
      "The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. <|im_end|>\n",
      "<|im_start|>user\n",
      "æˆ‘å¸Œæœ›é€éé€™æ¬¡è¼”å°å¯ä»¥å­¸ç¿’é»æ¨£æ›´å¥½åœ°è™•ç†èˆ‡åŒäº‹ä¹‹é–“å˜…é—œä¿‚ï¼Œå› ç‚ºæœ€è¿‘å–ºå…¬å¸å˜…å£“åŠ›ç‰¹åˆ¥å¤§ï¼Œä»¤æˆ‘æˆæ—¥è¦ºå¾—å¿ƒæƒ…å¥½å””å¥½ã€‚åŒäº‹ä¹‹é–“æœ‰å•²èª¤æœƒï¼Œæˆ‘æ„Ÿè¦ºè‡ªå·±è¢«æ’æ–¥ï¼Œå¥½ä¼¼å¤§å®¶éƒ½å””å¤ªé¡˜æ„åŒæˆ‘æºé€šï¼Œå‘¢å•²æƒ…æ³ä»¤æˆ‘æ„Ÿåˆ°å­¤ç«‹åŒåŸ‹å””å—é‡è¦–ã€‚å°¤å…¶ä¿‚ä¸Šæ˜ŸæœŸï¼Œæˆ‘åšå’—ä¸€ä»½é‡è¦å˜…å ±å‘Šï¼Œå®Œæˆå¾—å””éŒ¯ï¼Œä½†åŒäº‹å»å†‡è¡¨ç¤ºä»»ä½•èªåŒï¼Œå¥½ä¼¼æˆ‘å˜…åŠªåŠ›å®Œå…¨è¢«å¿½è¦–å’ã€‚å‘¢å•²äº‹æƒ…ä»¤æˆ‘å¤œæ™šè¨“å””è‘—ï¼Œç¶“å¸¸æœƒè«—ä½å•²è² é¢å˜…å˜¢ï¼Œè¦ºå¾—è‡ªå·±åšä¹œå˜¢éƒ½å””å•±ï¼Œå¿ƒæƒ…ä½è½åŒåŸ‹å¥½ç„¦æ…®ï¼Œç”šè‡³æœ‰æ™‚é ­ç—›åŒèƒƒç—›ï¼Œå‘¢å•²ç—‡ç‹€æ¯æ˜ŸæœŸéƒ½æœƒå‡ºç¾å¹¾æ¬¡ï¼Œé€šå¸¸æŒçºŒå¹¾å€‹é˜é ­ã€‚å–ºå®¶åº­æ–¹é¢ï¼Œæˆ‘åŒçˆ¶æ¯å˜…é—œä¿‚ç®—ä¿‚å¹¾å¥½ï¼Œä½†ä½¢å“‹å””å¤ªäº†è§£æˆ‘å·¥ä½œä¸Šé¢å˜…å£“åŠ›ï¼Œæ‰€ä»¥æˆ‘éƒ½å””å¤ªé¡˜æ„è¬›ã€‚ä»¥å‰å–ºè®€æ›¸æ™‚æœŸï¼Œæ›¾ç¶“æœ‰éè¢«åŒå­¸æ’æ“ å˜…ç¶“æ­·ï¼Œæ‰€ä»¥ä»Šæ¬¡å˜…æƒ…æ³ç‰¹åˆ¥ä»¤æˆ‘æƒ³èµ·ç•¶æ™‚å˜…æ„Ÿå—ã€‚æˆ‘è©¦éç”¨è½éŸ³æ¨‚åŒåšé‹å‹•åšŸèˆ’ç·©æƒ…ç·’ï¼Œä½†æ•ˆæœå””ä¿‚å¥½æŒä¹…ã€‚æˆ‘æƒ³çŸ¥é“ï¼Œå–ºè¼”å°éç¨‹ä¸­æˆ‘æ‡‰è©²é»æ¨£è¡¨é”è‡ªå·±å˜…æ„Ÿå—ï¼Ÿå¦å¤–ï¼Œå°æ–¼æ”¹å–„åŒäº‹é—œä¿‚ï¼Œæœ‰å†‡å•²æœ‰æ•ˆå˜…æ–¹æ³•æˆ–æŠ€å·§ï¼Ÿå¦‚æœæˆ‘æƒ³ç”¨ä¸­é†«åŒè¥¿é†«çµåˆå˜…æ–¹æ³•å»è™•ç†æƒ…ç·’ï¼Œæœ‰å†‡å•²å»ºè­°ï¼Ÿ<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "ä½ æå‡ºæƒ³å­¸ç¿’å¦‚ä½•æ›´å¥½åœ°èˆ‡åŒäº‹ç›¸è™•ï¼Œé€™æ˜¯éå¸¸å¯¦éš›ä¸”é‡è¦çš„ç›®æ¨™ã€‚ç•¶æ„Ÿå—åˆ°è¢«æ’æ“ å’Œå¿½è¦–ï¼Œæƒ…ç·’è‡ªç„¶æœƒå—åˆ°å¾ˆå¤§å½±éŸ¿ï¼Œè€Œä½ æåŠçš„é ­ç—›å’Œèƒƒç—›ï¼Œå¾ˆå¯èƒ½æ˜¯èº«é«”å°å£“åŠ›çš„åæ‡‰ã€‚å»ºè­°åœ¨è¼”å°ä¸­ï¼Œå¯ä»¥å˜—è©¦å¦èª åœ°åˆ†äº«ä½ çš„æ„Ÿå—å’Œç¶“æ­·ï¼Œä¸ç”¨æ“”å¿ƒè¢«è©•åƒ¹ï¼Œå› ç‚ºé€™è£¡æ˜¯ä¸€å€‹å®‰å…¨çš„ç©ºé–“ã€‚è©¦è‘—ç”¨å…·é«”äº‹ä»¶ä¾†æè¿°ä½ çš„æƒ…ç·’è®ŠåŒ–ï¼Œä¾‹å¦‚é‚£ä»½å ±å‘Šè¢«å¿½è¦–çš„æ™‚åˆ»ï¼Œè®“è¨è«–æ›´èšç„¦ã€‚é‡å°æ”¹å–„åŒäº‹é—œä¿‚ï¼Œå»ºç«‹æºé€šæ©‹æ¨‘å¾ˆé‡è¦ï¼Œå¯ä»¥å˜—è©¦ä¸»å‹•è¡¨é”ä½ çš„æƒ³æ³•èˆ‡æ„Ÿå—ï¼ŒåŒæ™‚ä¹Ÿè†è½ä»–äººçš„æ„è¦‹ï¼Œå°‹æ±‚å…±åŒè§£æ±ºå•é¡Œçš„æ–¹æ³•ã€‚å­¸ç¿’é©ç•¶çš„æºé€šæŠ€å·§ï¼Œå¦‚ã€Œæˆ‘è¨Šæ¯ã€ä¾†è¡¨é”è‡ªå·±çš„æ„Ÿå—è€Œéè²¬å‚™å°æ–¹ï¼Œå¯æ¸›å°‘èª¤è§£ã€‚é—œæ–¼ä¸­è¥¿é†«çµåˆè™•ç†æƒ…ç·’ï¼Œé€™æ˜¯ä¸å°‘äººé¸æ“‡çš„è·¯å¾‘ï¼Œä¸­é†«çš„é‡ç¸æˆ–è‰è—¥å¯èƒ½æœ‰åŠ©æ–¼èª¿ç¯€èº«é«”ç‹€æ…‹ï¼Œè€Œè¥¿é†«æ–¹é¢ï¼Œå¿ƒç†è«®è©¢æ­é…å¿…è¦æ™‚çš„è—¥ç‰©æ²»ç™‚ï¼Œæœ‰åŠ©ç—‡ç‹€ç·©è§£ã€‚ä½ å¯ä»¥æŠŠé€™äº›æƒ³æ³•å¸¶å…¥è¼”å°ï¼Œè®“éç¨‹æ›´å…¨é¢åœ°ç…§é¡§ä½ èº«å¿ƒçš„éœ€è¦ã€‚æœ€å¾Œï¼Œè¼”å°éç¨‹ä¸­å¯ä»¥éš¨æ™‚æå‡ºä½ å°æ–¹æ³•çš„ç–‘å•ï¼Œè®“æˆ‘å€‘ä¸€èµ·æ¢ç´¢æœ€é©åˆä½ çš„ç­–ç•¥ã€‚<|im_end|>\n",
      "\n",
      "\n",
      "Sample of final formatted evaluation data (after chat template):\n",
      "<|im_start|>system\n",
      "You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description. \n",
      "The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. <|im_end|>\n",
      "<|im_start|>user\n",
      "I've been having trouble with my sleep for the past few months, and it's really starting to affect my daily life. My main goal in coming here is to find ways to improve my sleep quality and feel more rested during the day. I often lie awake for hours before finally falling asleep, and even when I do sleep, I wake up multiple times throughout the night. This leads to me feeling exhausted, irritable, and less focused at work. \n",
      "\n",
      "I've noticed that stress at work and some recent family tensions seem to trigger these sleep problems. For example, last week, after a disagreement with my spouse about managing household responsibilities, I couldn't fall asleep until after 3 a.m. On other nights, I just feel overwhelmed by racing thoughts about deadlines and responsibilities. \n",
      "\n",
      "The insomnia happens almost every night, and sometimes I wake up feeling my heart racing or with a sense of restlessness. I can't say it was always this way; in the past, I used to sleep well without much effort. I have a history of some mild anxiety years ago, but nothing that lasted. There was also a traumatic experience about ten years back involving a car accident, but I thought I moved past it.\n",
      "\n",
      "To cope, I sometimes drink herbal tea or try deep breathing exercises, but these donâ€™t always help. I'm curious about different approaches to treatmentâ€”do you usually recommend behavioral changes, relaxation techniques, or something else? Also, how long might it take before I notice improvements? Are there specific things I should avoid or try to include in my daily routine to help my sleep? I'd like to understand how counseling sessions like this work towards helping with sleep improvement.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Improving sleep often involves addressing habits and managing stressors that affect your rest. Establishing a consistent bedtime routine can help signal to your body that itâ€™s time to wind downâ€”this might include dimming lights, turning off screens at least an hour before bed, and engaging in calming activities like reading or gentle stretches. \n",
      "\n",
      "Since stress and racing thoughts are impacting your sleep, practicing relaxation techniques such as progressive muscle relaxation or guided imagery before bed may reduce tension. If your mind races, writing down your worries or a to-do list earlier in the evening can help clear your mind.\n",
      "\n",
      "Limiting caffeine intake after mid-afternoon and reducing alcohol consumption are also important, as they can disrupt sleep cycles. Regular physical activity during the day can promote better sleep, but avoid vigorous exercise close to bedtime.\n",
      "\n",
      "Counseling sessions typically explore patterns and triggers related to sleep difficulties and work on developing personalized coping strategies. Changes might be noticeable within a few weeks, but consistency and patience are key. We can also discuss cognitive techniques to challenge unhelpful thoughts about sleep that may contribute to anxiety at night.\n",
      "\n",
      "If family tensions are a significant source of stress, exploring communication strategies might indirectly improve your sleep as well. Throughout this process, youâ€™ll have space to share experiences and try various tools, adjusting as needed based on what works best for you.\n",
      "\n",
      "Would you like to explore specific relaxation exercises during our sessions or focus on restructuring thoughts related to sleep difficulties? Additionally, feel free to share any preferences or concerns about the methods used, so the approach fits your needs comfortably.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # For pd.notna\n",
    "\n",
    "# 'interview_train_dataset' and 'interview_eval_dataset' are from the previous cell\n",
    "# The 'tokenizer' is globally defined in a prior cell (Cell 6, id=\"c1180838\")\n",
    "\n",
    "def convert_csv_to_chat_format(examples):\n",
    "    \"\"\"\n",
    "    Converts a batch of examples from the CSV structure to a list of conversations.\n",
    "    Each conversation is a list of dictionaries with \"role\" and \"content\".\n",
    "    \"\"\"\n",
    "    all_conversations = []\n",
    "    # 'examples' is a dictionary where keys are column names and values are lists of entries\n",
    "    num_examples = len(examples['instruction'])\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        instruction = examples['instruction'][i]\n",
    "        input_text = examples['input'][i]\n",
    "        output_text = examples['output'][i]\n",
    "\n",
    "        # Ensure input_text is a string; handle None or NaN by treating as empty string if necessary\n",
    "        input_text_str = str(input_text) if pd.notna(input_text) and str(input_text).strip() else \"\"\n",
    "\n",
    "        current_conversation = []\n",
    "        # System prompt from 'instruction'\n",
    "        current_conversation.append({\"role\": \"system\", \"content\": instruction})\n",
    "        \n",
    "        # User message from 'input'\n",
    "        # Based on the dataset, 'input' should always be present.\n",
    "        # If input_text_str is empty, this will add a user message with empty content.\n",
    "        # This is generally fine as the model should learn to handle it or it implies\n",
    "        # the system prompt itself is the query.\n",
    "        current_conversation.append({\"role\": \"user\", \"content\": input_text_str})\n",
    "            \n",
    "        # Assistant message from 'output'\n",
    "        current_conversation.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "        \n",
    "        all_conversations.append(current_conversation)\n",
    "        \n",
    "    return {\"conversations\": all_conversations}\n",
    "\n",
    "def apply_template_to_conversations(examples):\n",
    "    \"\"\"\n",
    "    Applies the tokenizer's chat template to a batch of conversations.\n",
    "    Creates a 'text' field for SFTTrainer.\n",
    "    \"\"\"\n",
    "    # tokenizer should be globally available\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            examples[\"conversations\"], # This is a list of conversations\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False, # Crucial for training\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Process training data\n",
    "# 1. Convert CSV structure to list of message dicts\n",
    "train_dataset_with_conversations = interview_train_dataset.map(\n",
    "    convert_csv_to_chat_format,\n",
    "    batched=True,\n",
    "    remove_columns=interview_train_dataset.column_names # Keep only 'conversations'\n",
    ")\n",
    "# 2. Apply chat template to create the 'text' field\n",
    "final_train_dataset = train_dataset_with_conversations.map(\n",
    "    apply_template_to_conversations,\n",
    "    batched=True,\n",
    "    remove_columns=[\"conversations\"] # Keep only 'text'\n",
    ")\n",
    "# 3. Shuffle the training dataset\n",
    "final_train_dataset = final_train_dataset.shuffle(seed=3407)\n",
    "\n",
    "# Process evaluation data (if it exists)\n",
    "final_eval_dataset = None\n",
    "if interview_eval_dataset:\n",
    "    eval_dataset_with_conversations = interview_eval_dataset.map(\n",
    "        convert_csv_to_chat_format,\n",
    "        batched=True,\n",
    "        remove_columns=interview_eval_dataset.column_names\n",
    "    )\n",
    "    final_eval_dataset = eval_dataset_with_conversations.map(\n",
    "        apply_template_to_conversations,\n",
    "        batched=True,\n",
    "        remove_columns=[\"conversations\"]\n",
    "    )\n",
    "    # No need to shuffle eval_dataset\n",
    "\n",
    "print(\"Sample of final formatted training data (after chat template):\")\n",
    "if len(final_train_dataset) > 0:\n",
    "    print(final_train_dataset[0]['text'])\n",
    "else:\n",
    "    print(\"Training dataset is empty after processing.\")\n",
    "\n",
    "if final_eval_dataset and len(final_eval_dataset) > 0:\n",
    "    print(\"\\nSample of final formatted evaluation data (after chat template):\")\n",
    "    print(final_eval_dataset[0]['text'])\n",
    "elif interview_eval_dataset: # If interview_eval_dataset existed but final_eval_dataset is empty\n",
    "    print(\"\\nEvaluation dataset is empty after processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "109c736c8b99496e8721f9595c54eb6c",
      "f89daea726144ded892113a161649b64",
      "9f0108d9201f40599facfccf668a6e84",
      "927c60cd1f0d4a32a1ff9427a96a3246",
      "fb6f38d9dd1e49ec8fdfb7ca7ea363a2",
      "6303ec778b1d49dd980542a191261961",
      "5d66a72e82e54d9e8367ca8a2469dd0a",
      "e17016c458a14736bd56ba55d7168f32",
      "1d31954fe1804140bcac71e3d4102fdc",
      "a7077f3352b246ddbbca391c08c48b4a",
      "a1c374126fb144e68060e4fedab51046"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "4aee3090-cf58-4497-979b-cd41cbc5fd62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhongtai91\u001b[0m (\u001b[33mhongtai91-n-a\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tai/nlp/wandb/run-20250616_044523-7c24r3z5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5' target=\"_blank\">base-model-training-syntheic_v2_1</a></strong> to <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c60c9dc965e4d0f96ea847551e96783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=256):   0%|          | 0/4521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 238. Reducing num_proc to 238 for dataset of size 238.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e33276380e4ac19ab7003fa7fb8eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=238):   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"distress-chatbot\", name=\"base-model-training-syntheic_v2_1\", config={\n",
    "    \"model\": \"Qwen/Qwen3-4B\",\n",
    "    # \"max_steps\": 20000,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lambda_decay\": 0.95,\n",
    "})  # Allow resuming W&B run\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = final_train_dataset, # Use the processed training data\n",
    "    eval_dataset = final_eval_dataset,   # Use the processed evaluation data\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\", # Column containing the formatted text\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps = 60, # Adjusted for quicker testing, was 30. Set to None or higher for full training.\n",
    "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        # evaluation_strategy = \"steps\" if final_eval_dataset else \"no\", # Enable evaluation if eval_dataset is present\n",
    "        # eval_steps = 20, # Evaluate every N steps, adjust as needed\n",
    "    ),\n",
    ")\n",
    "\n",
    "# If using evaluation, you might want to set evaluation_strategy and eval_steps in SFTConfig\n",
    "if final_eval_dataset:\n",
    "    trainer.args.evaluation_strategy = \"steps\"\n",
    "    trainer.args.eval_steps = 20 # Or any other desired frequency\n",
    "else:\n",
    "    trainer.args.evaluation_strategy = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "344c14cb-0138-4981-9d85-b1e8856093ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA L40S. Max memory = 44.418 GB.\n",
      "31.408 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9fa371ShyhB"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "4b644b12-626b-45c7-fb11-89825ae13bd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,521 | Num Epochs = 3 | Total steps = 849\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,000,000,000 (3.30% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='849' max='849' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [849/849 41:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.915300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.773100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.778600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.727100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.313800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.312400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.304600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>1.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>1.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>1.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>1.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>1.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>1.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>1.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>1.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>1.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>1.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>1.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>1.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>1.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>1.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>1.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>1.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>1.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>1.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>1.309900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>1.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>1.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>1.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>1.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>1.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>1.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>1.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>1.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>1.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>1.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>1.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>1.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>1.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>1.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>1.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>1.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>1.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>1.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>1.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>1.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>1.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>1.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>1.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>1.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>1.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>1.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>1.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>1.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>1.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>1.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>1.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>1.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>1.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>1.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>1.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>1.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>1.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>1.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>1.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>1.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>1.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>1.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>1.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>1.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>1.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>1.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>1.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>1.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>1.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>1.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>1.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>1.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>1.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>1.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>1.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>1.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>1.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>1.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>1.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>1.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>1.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>1.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>1.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>1.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>1.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>1.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>1.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>1.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>1.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>1.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>1.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>1.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>1.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>1.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>1.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>1.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>1.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>1.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>1.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>1.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>1.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>1.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>1.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>1.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>1.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>1.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>1.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>1.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>1.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>1.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>1.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>1.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>1.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>1.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>1.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>1.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>1.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>1.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>1.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>1.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>1.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>1.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>1.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>1.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>1.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>1.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>1.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>1.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>1.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>1.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>1.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>1.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>1.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>1.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>1.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>1.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>1.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>1.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>1.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>1.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>1.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>1.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>1.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>1.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>1.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>1.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>1.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>1.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>1.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>1.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>1.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>1.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>1.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>1.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>1.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>1.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>1.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>1.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>1.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>1.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>1.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>1.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>1.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>1.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>1.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>1.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>1.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>1.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>1.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>1.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>1.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>1.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>1.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>1.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>1.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>1.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>1.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>1.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>1.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>1.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>1.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>1.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>1.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>1.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>1.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>1.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>1.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>1.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>1.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>1.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>1.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>1.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>1.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>1.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>1.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>1.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>1.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>1.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>1.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>1.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>1.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>1.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>1.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>1.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>1.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>1.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>1.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>1.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>1.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>1.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>1.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>1.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>1.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>1.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>1.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>1.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>1.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>1.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>1.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>1.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>1.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>1.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>1.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>1.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>1.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>1.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>1.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>1.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>1.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>1.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>1.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>1.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>1.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>1.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>1.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>1.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>1.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>1.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>1.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>1.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>1.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>1.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>1.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>1.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>1.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>1.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>1.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>1.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>1.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>1.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>1.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>1.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>1.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>1.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>1.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>1.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>1.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>1.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>1.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>1.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>1.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>1.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>1.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>1.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>1.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>1.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>1.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>1.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>1.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>1.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>1.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>1.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>1.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>1.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>1.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>1.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>1.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>1.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>1.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>1.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>1.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>1.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>1.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>1.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>1.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>1.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>1.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>1.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>1.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>1.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>1.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>1.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>1.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>1.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>1.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>1.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>1.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>1.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>1.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>1.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>1.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>1.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>1.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>1.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>1.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>1.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>1.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>1.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>1.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>1.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>1.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>1.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>1.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>1.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>1.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>1.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>1.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>1.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>1.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>1.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>1.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>1.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>1.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>1.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>1.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>1.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>1.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>1.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>1.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>1.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>1.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>1.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>1.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>1.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>1.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>1.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>1.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>1.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>1.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>1.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>1.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>1.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>1.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>1.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>1.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>1.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>1.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>1.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>1.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>1.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>1.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>1.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>1.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>1.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>1.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>1.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>1.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>1.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>1.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>1.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>1.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>1.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>1.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>1.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>1.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>1.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>1.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>1.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>1.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>1.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>1.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>1.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>1.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>1.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>1.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>1.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>1.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>1.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>1.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>1.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>1.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>1.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>1.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>1.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>1.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>1.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>1.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>1.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>1.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>1.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>1.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>1.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>1.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>1.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>1.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>1.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>1.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>1.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>1.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>1.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>1.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>1.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>1.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>1.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>1.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>1.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>1.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>1.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>1.169500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_model_v2_1/tokenizer_config.json',\n",
       " 'trained_model_v2_1/special_tokens_map.json',\n",
       " 'trained_model_v2_1/chat_template.jinja',\n",
       " 'trained_model_v2_1/vocab.json',\n",
       " 'trained_model_v2_1/merges.txt',\n",
       " 'trained_model_v2_1/added_tokens.json',\n",
       " 'trained_model_v2_1/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"trained_model_v2_1\")  # Local saving\n",
    "tokenizer.save_pretrained(\"trained_model_v2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning memory for next round of training\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-finetuning for formatting alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 60 examples.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the custom CSV dataset\n",
    "formatting_interview_dataset = load_dataset(\"csv\", data_files=\"./dataset/generated_responses_60_samples.csv\", split=\"train\")\n",
    "print(f\"Loaded dataset with {len(formatting_interview_dataset)} examples.\")\n",
    "interview_train_dataset = formatting_interview_dataset\n",
    "interview_eval_dataset = None  # No evaluation set for this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of final formatted training data (after chat template):\n",
      "<|im_start|>system\n",
      "You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. At the end of answer, add tag <evaluate>{\"Active Listening\" : score, \"Empathy & Validation\": score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": explain} </evaluate> evaluate your consultant answer in 7 metrics and explain for that evaluation with score from 1 to 10 in json format, where 1 is the worst and 10 is the best and explain is clearly explain why has that score. \n",
      "\n",
      "Consultation Metrics:\n",
      "1. Active Listening: Responses should show careful consideration of the user's concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
      "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user's feelings and emotions without being dismissive or minimizing their experiences.\n",
      "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
      "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
      "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
      "6. Boundaries & Ethical: It's vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
      "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.<|im_end|>\n",
      "<|im_start|>user\n",
      "TÃ´i Ä‘Ã£ pháº£i váº­t lá»™n vá»›i nhá»¯ng cáº£m xÃºc giáº­n dá»¯, buá»“n bÃ£ vÃ  tháº¥t vá»ng khi cá»‘ gáº¯ng giáº£i quyáº¿t viá»‡c phÃ¢n chia tÃ i sáº£n cá»§a máº¹ tÃ´i Ä‘Ã£ qua Ä‘á»i. TÃ¬nh huá»‘ng trá»Ÿ nÃªn phá»©c táº¡p hÆ¡n do sá»± thay Ä‘á»•i trong má»‘i quan há»‡ gia Ä‘Ã¬nh vÃ  sá»± tham gia cá»§a cÃ¡c vá»£ chá»“ng. Anh trai tÃ´i muá»‘n tá»• chá»©c má»™t buá»•i bÃ¡n Ä‘á»“ trong gara, nhÆ°ng má»—i ngÆ°á»i Ä‘á»u cÃ³ quan Ä‘iá»ƒm vÃ  sá»± gáº¯n bÃ³ cáº£m xÃºc riÃªng vá»›i cÃ¡c mÃ³n Ä‘á»“ trong tÃ i sáº£n. TÃ´i Ä‘Ã£ gáº·p ráº¥t nhiá»u khÃ³ khÄƒn khi Ä‘á»‘i máº·t vá»›i váº¥n Ä‘á» nÃ y, Ä‘áº·c biá»‡t lÃ  khi chá»‹ gÃ¡i tÃ´i Ä‘Ã£ láº¥y má»™t sá»‘ Ä‘á»“ cÃ¡ nhÃ¢n trÆ°á»›c khi chÃºng tÃ´i báº¯t Ä‘áº§u tháº£o luáº­n vá» tÃ i sáº£n. TÃ´i cáº£m tháº¥y bá»‘i rá»‘i vÃ  khÃ´ng biáº¿t lÃ m tháº¿ nÃ o Ä‘á»ƒ tiáº¿n lÃªn phÃ­a trÆ°á»›c.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "TÃ´i ráº¥t hiá»ƒu ráº±ng viá»‡c phÃ¢n chia tÃ i sáº£n sau khi máº¥t ngÆ°á»i thÃ¢n khÃ´ng chá»‰ lÃ  váº¥n Ä‘á» váº­t cháº¥t mÃ  cÃ²n cháº¥t chá»©a nhiá»u cáº£m xÃºc phá»©c táº¡p nhÆ° giáº­n dá»¯, buá»“n bÃ£ vÃ  tháº¥t vá»ng. Má»—i mÃ³n Ä‘á»“ trong tÃ i sáº£n Ä‘á»u mang Ã½ nghÄ©a riÃªng vÃ  sá»± gáº¯n bÃ³ cáº£m xÃºc Ä‘áº·c biá»‡t, khiáº¿n cho quÃ¡ trÃ¬nh nÃ y trá»Ÿ nÃªn khÃ³ khÄƒn hÆ¡n ráº¥t nhiá»u. Viá»‡c chá»‹ gÃ¡i báº¡n Ä‘Ã£ láº¥y Ä‘á»“ trÆ°á»›c khi tháº£o luáº­n cÅ©ng lÃ m tÄƒng thÃªm cáº£m giÃ¡c bá»‘i rá»‘i vÃ  cÃ³ thá»ƒ dáº«n Ä‘áº¿n nhá»¯ng báº¥t Ä‘á»“ng trong gia Ä‘Ã¬nh.\n",
      "\n",
      "Trong trÆ°á»ng há»£p nÃ y, báº¡n cÃ³ thá»ƒ cÃ¢n nháº¯c má»™t sá»‘ cÃ¡ch tiáº¿p cáº­n sau Ä‘á»ƒ giÃºp má»i viá»‡c trá»Ÿ nÃªn suÃ´n sáº» hÆ¡n:\n",
      "\n",
      "1. **Giao tiáº¿p cá»Ÿi má»Ÿ vÃ  tÃ´n trá»ng:** Cá»‘ gáº¯ng tá»• chá»©c má»™t cuá»™c há»p gia Ä‘Ã¬nh nÆ¡i má»i ngÆ°á»i cÃ³ thá»ƒ chia sáº» quan Ä‘iá»ƒm vÃ  cáº£m xÃºc cá»§a mÃ¬nh má»™t cÃ¡ch trung thá»±c, tÃ´n trá»ng láº«n nhau. Äiá»u nÃ y cÃ³ thá»ƒ giÃºp giáº£m cÄƒng tháº³ng vÃ  hiá»ƒu nhau hÆ¡n.\n",
      "\n",
      "2. **Láº­p danh sÃ¡ch tÃ i sáº£n vÃ  mong muá»‘n:** Ghi láº¡i nhá»¯ng mÃ³n Ä‘á»“ quan trá»ng Ä‘á»‘i vá»›i tá»«ng ngÆ°á»i vÃ  mong muá»‘n cá»§a há» vá» cÃ¡ch phÃ¢n chia Ä‘á»ƒ cÃ³ cÆ¡ sá»Ÿ tháº£o luáº­n cá»¥ thá»ƒ.\n",
      "\n",
      "3. **Thá»a thuáº­n vá» cÃ¡ch xá»­ lÃ½ tÃ i sáº£n:** Náº¿u cÃ³ thá»ƒ, Ä‘á»“ng Ã½ vá»›i nhau vá» cÃ¡c phÆ°Æ¡ng thá»©c giáº£i quyáº¿t nhÆ° bÃ¡n Ä‘á»“, giá»¯ láº¡i hoáº·c chia theo giÃ¡ trá»‹ cáº£m xÃºc nháº±m táº¡o sá»± cÃ´ng báº±ng.\n",
      "\n",
      "4. **TÃ¬m sá»± há»— trá»£ tá»« bÃªn thá»© ba:** Náº¿u viá»‡c tháº£o luáº­n trá»Ÿ nÃªn cÄƒng tháº³ng hoáº·c khÃ´ng thá»ƒ giáº£i quyáº¿t, báº¡n cÃ³ thá»ƒ xem xÃ©t nhá» Ä‘áº¿n sá»± giÃºp Ä‘á»¡ cá»§a má»™t ngÆ°á»i trung gian, nhÆ° luáº­t sÆ° hoáº·c chuyÃªn gia hÃ²a giáº£i gia Ä‘Ã¬nh, Ä‘á»ƒ Ä‘áº£m báº£o má»i ngÆ°á»i Ä‘á»u Ä‘Æ°á»£c nghe vÃ  quyá»n lá»£i Ä‘Æ°á»£c báº£o vá»‡.\n",
      "\n",
      "5. **ChÄƒm sÃ³c báº£n thÃ¢n:** KhÃ´ng quÃªn dÃ nh thá»i gian Ä‘á»ƒ chÄƒm sÃ³c cáº£m xÃºc cá»§a chÃ­nh mÃ¬nh trong quÃ¡ trÃ¬nh nÃ y báº±ng cÃ¡ch chia sáº» vá»›i ngÆ°á»i tin cáº­y hoáº·c tham gia cÃ¡c hoáº¡t Ä‘á»™ng giÃºp giáº£m stress.\n",
      "\n",
      "TÃ´i hy vá»ng nhá»¯ng gá»£i Ã½ trÃªn cÃ³ thá»ƒ giÃºp báº¡n cáº£m tháº¥y bá»›t bá»‘i rá»‘i vÃ  tÃ¬m Ä‘Æ°á»£c hÆ°á»›ng Ä‘i phÃ¹ há»£p cho gia Ä‘Ã¬nh mÃ¬nh. Náº¿u báº¡n muá»‘n, tÃ´i sáºµn lÃ²ng láº¯ng nghe thÃªm Ä‘á»ƒ cÃ¹ng báº¡n tÃ¬m nhá»¯ng cÃ¡ch giáº£i quyáº¿t phÃ¹ há»£p nháº¥t.\n",
      "\n",
      "<evaluate>{\"Active Listening\":9,\"Empathy & Validation\":9,\"Safety & Trustworthiness\":10,\"Open-mindedness & Non-judgment\":9,\"Clarity & Encouragement\":9,\"Boundaries & Ethical\":8,\"Holistic Approach\":9,\"Explaination for Scoring\":\"CÃ¢u tráº£ lá»i thá»ƒ hiá»‡n rÃµ sá»± chÃº Ã½ Ä‘áº¿n khÃ³ khÄƒn vÃ  cáº£m xÃºc cá»§a báº¡n, Ä‘á»“ng thá»i thá»ƒ hiá»‡n sá»± tháº¥u hiá»ƒu vÃ  cáº£m thÃ´ng sÃ¢u sáº¯c. CÃ¡c Ä‘á» xuáº¥t Ä‘Æ°á»£c Ä‘Æ°a ra rÃµ rÃ ng, giÃºp báº¡n cÃ³ nhá»¯ng lá»±a chá»n phÃ¹ há»£p vÃ  an toÃ n. CÃ¢u tráº£ lá»i khÃ´ng mang tÃ­nh phÃ¡n xÃ©t vÃ  khuyáº¿n khÃ­ch sá»± giao tiáº¿p tÃ­ch cá»±c. Tuy nhiÃªn, cÃ¢u tráº£ lá»i cÃ³ thá»ƒ nháº¥n máº¡nh thÃªm vai trÃ² há»— trá»£ cá»§a chuyÃªn gia Ä‘á»ƒ Ä‘áº£m báº£o giá»›i háº¡n chuyÃªn mÃ´n Ä‘Æ°á»£c rÃµ rÃ ng hÆ¡n.\"} </evaluate><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # For pd.notna\n",
    "\n",
    "# 'interview_train_dataset' and 'interview_eval_dataset' are from the previous cell\n",
    "# The 'tokenizer' is globally defined in a prior cell (Cell 6, id=\"c1180838\")\n",
    "\n",
    "instruction_for_formatting = \"\"\"You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. At the end of answer, add tag <evaluate>{\"Active Listening\" : score, \"Empathy & Validation\": score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": explain} </evaluate> evaluate your consultant answer in 7 metrics and explain for that evaluation with score from 1 to 10 in json format, where 1 is the worst and 10 is the best and explain is clearly explain why has that score. \\n\\nConsultation Metrics:\\n1. Active Listening: Responses should show careful consideration of the user's concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\\n2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user's feelings and emotions without being dismissive or minimizing their experiences.\\n3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\\n4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\\n5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\\n6. Boundaries & Ethical: It's vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\\n7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\"\"\"\n",
    "\n",
    "def convert_csv_to_chat_format(examples):\n",
    "    \"\"\"\n",
    "    Converts a batch of examples from the CSV structure to a list of conversations.\n",
    "    Each conversation is a list of dictionaries with \"role\" and \"content\".\n",
    "    \"\"\"\n",
    "    all_conversations = []\n",
    "    # 'examples' is a dictionary where keys are column names and values are lists of entries\n",
    "    num_examples = len(examples['instruction'])\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        instruction = instruction_for_formatting\n",
    "        input_text = examples['input'][i]\n",
    "        output_text = examples['output'][i]\n",
    "\n",
    "        # Ensure input_text is a string; handle None or NaN by treating as empty string if necessary\n",
    "        input_text_str = str(input_text) if pd.notna(input_text) and str(input_text).strip() else \"\"\n",
    "\n",
    "        current_conversation = []\n",
    "        # System prompt from 'instruction'\n",
    "        current_conversation.append({\"role\": \"system\", \"content\": instruction})\n",
    "        \n",
    "        # User message from 'input'\n",
    "        # Based on the dataset, 'input' should always be present.\n",
    "        # If input_text_str is empty, this will add a user message with empty content.\n",
    "        # This is generally fine as the model should learn to handle it or it implies\n",
    "        # the system prompt itself is the query.\n",
    "        current_conversation.append({\"role\": \"user\", \"content\": input_text_str})\n",
    "            \n",
    "        # Assistant message from 'output'\n",
    "        current_conversation.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "        \n",
    "        all_conversations.append(current_conversation)\n",
    "        \n",
    "    return {\"conversations\": all_conversations}\n",
    "\n",
    "def apply_template_to_conversations(examples):\n",
    "    \"\"\"\n",
    "    Applies the tokenizer's chat template to a batch of conversations.\n",
    "    Creates a 'text' field for SFTTrainer.\n",
    "    \"\"\"\n",
    "    # tokenizer should be globally available\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            examples[\"conversations\"], # This is a list of conversations\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False, # Crucial for training\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Process training data\n",
    "# 1. Convert CSV structure to list of message dicts\n",
    "train_dataset_with_conversations = interview_train_dataset.map(\n",
    "    convert_csv_to_chat_format,\n",
    "    batched=True,\n",
    "    remove_columns=interview_train_dataset.column_names # Keep only 'conversations'\n",
    ")\n",
    "# 2. Apply chat template to create the 'text' field\n",
    "final_train_dataset = train_dataset_with_conversations.map(\n",
    "    apply_template_to_conversations,\n",
    "    batched=True,\n",
    "    remove_columns=[\"conversations\"] # Keep only 'text'\n",
    ")\n",
    "# 3. Shuffle the training dataset\n",
    "final_train_dataset = final_train_dataset.shuffle(seed=3407)\n",
    "\n",
    "# Process evaluation data (if it exists)\n",
    "final_eval_dataset = None\n",
    "if interview_eval_dataset:\n",
    "    eval_dataset_with_conversations = interview_eval_dataset.map(\n",
    "        convert_csv_to_chat_format,\n",
    "        batched=True,\n",
    "        remove_columns=interview_eval_dataset.column_names\n",
    "    )\n",
    "    final_eval_dataset = eval_dataset_with_conversations.map(\n",
    "        apply_template_to_conversations,\n",
    "        batched=True,\n",
    "        remove_columns=[\"conversations\"]\n",
    "    )\n",
    "    # No need to shuffle eval_dataset\n",
    "\n",
    "print(\"Sample of final formatted training data (after chat template):\")\n",
    "if len(final_train_dataset) > 0:\n",
    "    print(final_train_dataset[0]['text'])\n",
    "else:\n",
    "    print(\"Training dataset is empty after processing.\")\n",
    "\n",
    "if final_eval_dataset and len(final_eval_dataset) > 0:\n",
    "    print(\"\\nSample of final formatted evaluation data (after chat template):\")\n",
    "    print(final_eval_dataset[0]['text'])\n",
    "elif interview_eval_dataset: # If interview_eval_dataset existed but final_eval_dataset is empty\n",
    "    print(\"\\nEvaluation dataset is empty after processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–†â–…â–†â–†â–†â–†â–†â–†â–†â–…â–†â–‡â–‡â–‡â–†â–†â–†â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–†â–‡â–‡â–‡â–‡</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–…â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.6927397422053274e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>849</td></tr><tr><td>train/grad_norm</td><td>0.82329</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1695</td></tr><tr><td>train_loss</td><td>1.22504</td></tr><tr><td>train_runtime</td><td>2519.1219</td></tr><tr><td>train_samples_per_second</td><td>5.384</td></tr><tr><td>train_steps_per_second</td><td>0.337</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">base-model-training-syntheic_v2_1</strong> at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5</a><br> View project at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250616_044523-7c24r3z5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tai/nlp/wandb/run-20250616_053525-bvk4i4hh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/bvk4i4hh' target=\"_blank\">base-model-training-syntheic_v2_2-formatting</a></strong> to <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/bvk4i4hh' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/bvk4i4hh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 60. Reducing num_proc to 60 for dataset of size 60.\n"
     ]
    }
   ],
   "source": [
    "# Traing the model for formatting\n",
    "import wandb\n",
    "wandb.init(project=\"distress-chatbot\", name=\"base-model-training-syntheic_v2_2-formatting\", config={\n",
    "    \"model\": \"Qwen/Qwen3-4B\",\n",
    "    # \"max_steps\": 20000,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lambda_decay\": 0.95,\n",
    "})  # Allow resuming W&B run\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = final_train_dataset, # Use the processed training data\n",
    "    eval_dataset = final_eval_dataset,   # Use the processed evaluation data\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\", # Column containing the formatted text\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 5, # Set this more for learning formatting. \n",
    "        # max_steps = 60, # Adjusted for quicker testing, was 30. Set to None or higher for full training.\n",
    "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        # evaluation_strategy = \"steps\" if final_eval_dataset else \"no\", # Enable evaluation if eval_dataset is present\n",
    "        # eval_steps = 20, # Evaluate every N steps, adjust as needed\n",
    "    ),\n",
    ")\n",
    "\n",
    "# If using evaluation, you might want to set evaluation_strategy and eval_steps in SFTConfig\n",
    "if final_eval_dataset:\n",
    "    trainer.args.evaluation_strategy = \"steps\"\n",
    "    trainer.args.eval_steps = 20 # Or any other desired frequency\n",
    "else:\n",
    "    trainer.args.evaluation_strategy = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 60 | Num Epochs = 5 | Total steps = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,000,000,000 (3.30% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.545700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.468500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.369500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_model_v2_1_formatting/tokenizer_config.json',\n",
       " 'trained_model_v2_1_formatting/special_tokens_map.json',\n",
       " 'trained_model_v2_1_formatting/chat_template.jinja',\n",
       " 'trained_model_v2_1_formatting/vocab.json',\n",
       " 'trained_model_v2_1_formatting/merges.txt',\n",
       " 'trained_model_v2_1_formatting/added_tokens.json',\n",
       " 'trained_model_v2_1_formatting/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving model and tokenizer\n",
    "model.save_pretrained(\"trained_model_v2_1_formatting\")  # Local saving\n",
    "tokenizer.save_pretrained(\"trained_model_v2_1_formatting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning with Group Relative Policy Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear accelerator state and reinitialize\n",
    "# import torch\n",
    "import gc\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "# Clear CUDA cache\n",
    "# torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# # Reset accelerator state\n",
    "# try:\n",
    "#     from accelerate.state import AcceleratorState\n",
    "#     AcceleratorState._reset_state(reset_partial_state=True)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Reinitialize accelerator\n",
    "# accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt:\n",
      "You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\n",
      "Provide helpful, comprehensive, and appropriate answers to the user questions.\n",
      "\n",
      "After your counselling response, you must include a self-evaluation in the following format:\n",
      "<evaluate>\n",
      "{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\n",
      "</evaluate>\n",
      "\n",
      "Where score is a number from 1-10, and provide a clear explanation for your scoring.\n",
      "Explain to metrics:\n",
      "1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
      "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\n",
      "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
      "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
      "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
      "6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
      "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\n"
     ]
    }
   ],
   "source": [
    "# Define the evaluation format markers\n",
    "evaluation_start = \"<evaluate>\"\n",
    "evaluation_end = \"</evaluate>\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\n",
    "Provide helpful, comprehensive, and appropriate answers to the user questions.\n",
    "\n",
    "After your counselling response, you must include a self-evaluation in the following format:\n",
    "<evaluate>\n",
    "{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\n",
    "</evaluate>\n",
    "\n",
    "Where score is a number from 1-10, and provide a clear explanation for your scoring.\n",
    "Explain to metrics:\n",
    "1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
    "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\n",
    "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
    "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
    "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
    "6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
    "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"System prompt:\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chat template\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with our specific template:\n",
    "chat_template = chat_template.replace(\"'{system_prompt}'\", f\"'{system_prompt}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted text:\n",
      "You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\n",
      "Provide helpful, comprehensive, and appropriate answers to the user questions.\n",
      "\n",
      "After your counselling response, you must include a self-evaluation in the following format:\n",
      "<evaluate>\n",
      "{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\n",
      "</evaluate>\n",
      "\n",
      "Where score is a number from 1-10, and provide a clear explanation for your scoring.\n",
      "Explain to metrics:\n",
      "1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
      "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\n",
      "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
      "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
      "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
      "6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
      "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.<|im_end|>I'm feeling anxious about work.I understand that work anxiety can be challenging. Let me help you explore some strategies. <evaluate>{\"Active Listening\" : 8, \"Empathy & Validation\": 9, \"Safety & Trustworthiness\" : 9, \"Open-mindedness & Non-judgment\" : 8, \"Clarity & Encouragement\" : 7, \"Boundaries & Ethical\" : 9, \"Holistic Approach\" : 8, \"Explaination for Scoring\": \"Provided empathetic response with good listening skills.\"}</evaluate><|im_end|>Can you suggest some techniques?\n"
     ]
    }
   ],
   "source": [
    "# Test the chat template\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm feeling anxious about work.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I understand that work anxiety can be challenging. Let me help you explore some strategies. <evaluate>{\\\"Active Listening\\\" : 8, \\\"Empathy & Validation\\\": 9, \\\"Safety & Trustworthiness\\\" : 9, \\\"Open-mindedness & Non-judgment\\\" : 8, \\\"Clarity & Encouragement\\\" : 7, \\\"Boundaries & Ethical\\\" : 9, \\\"Holistic Approach\\\" : 8, \\\"Explaination for Scoring\\\": \\\"Provided empathetic response with good listening skills.\\\"}</evaluate>\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you suggest some techniques?\"}\n",
    "]\n",
    "\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Formatted text:\")\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4759, 3)\n",
      "Columns: ['instruction', 'input', 'output']\n",
      "\n",
      "First few rows:\n",
      "                                         instruction  \\\n",
      "0  You are a helpful mental health counselling as...   \n",
      "1  You are a helpful mental health counselling as...   \n",
      "2  You are a helpful mental health counselling as...   \n",
      "3  You are a helpful mental health counselling as...   \n",
      "4  You are a helpful mental health counselling as...   \n",
      "\n",
      "                                               input  \\\n",
      "0  I've been struggling with alcohol use for a wh...   \n",
      "1  æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢ï¼Œèƒ½æ›´å¥½åœ°ç†è§£è‡ªå·±ç›®å‰çš„æƒ…ç»ªçŠ¶æ€ï¼Œå¹¶æ‰¾åˆ°ç¼“è§£å‹åŠ›å’Œç„¦è™‘çš„æ–¹æ³•ã€‚æœ€è¿‘å‡ ä¸ªæœˆï¼Œ...   \n",
      "2  TÃ´i mong muá»‘n qua buá»•i tÆ° váº¥n nÃ y, tÃ´i cÃ³ thá»ƒ ...   \n",
      "3  Ø£Ø±ØºØ¨ ÙÙŠ Ù…Ù†Ø§Ù‚Ø´Ø© Ù…ÙˆØ¶ÙˆØ¹ Ø­Ø³Ø§Ø³ ÙŠØ®Øµ Ø­ÙŠØ§ØªÙŠ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ£...   \n",
      "4  æœ€è¿‘æˆ‘ä¸€ç›´æ„Ÿåˆ°å¾ˆéš¾è¿‡å’Œå­¤ç‹¬ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºæˆ‘åˆšåˆšå¤±å»äº†æˆ‘çš„ç¥–æ¯ã€‚å¥¹åœ¨æˆ‘ç”Ÿæ´»ä¸­ä¸€ç›´æ˜¯ä¸ªéå¸¸é‡è¦çš„æ”¯...   \n",
      "\n",
      "                                              output  \n",
      "0  Managing alcohol use involves understanding th...  \n",
      "1  åœ¨å¿ƒç†å’¨è¯¢è¿‡ç¨‹ä¸­ï¼Œä¿æŠ¤æ‚¨çš„éšç§æ˜¯éå¸¸é‡è¦çš„ï¼Œæ‰€æœ‰æ‚¨åˆ†äº«çš„ä¿¡æ¯éƒ½ä¼šä¸¥æ ¼ä¿å¯†ï¼Œé™¤éæ¶‰åŠæ‚¨æˆ–ä»–äººçš„...  \n",
      "2  Kiá»ƒm soÃ¡t cáº£m giÃ¡c lo láº¯ng báº¯t Ä‘áº§u báº±ng viá»‡c n...  \n",
      "3  Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ÙÙŠ...  \n",
      "4  å¤±å»äº²äººå¸¦æ¥çš„æƒ…ç»ªååº”éå¸¸å¤æ‚ï¼Œæ‚¨æ‰€æè¿°çš„èº«ä½“ç—‡çŠ¶å¦‚èƒ¸å£å‹æŠ‘ã€å¤´ç—›å’Œèƒƒéƒ¨ä¸é€‚ï¼Œç¡®å®å¯èƒ½æ˜¯æ‚²ä¼¤...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"./dataset/stage_2_1_synthetic_interview_data_combined.csv\", split=\"train\")\n",
    "dataset = dataset.to_pandas()\n",
    "\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"Columns: {dataset.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      "[{'role': 'system', 'content': 'You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\\nProvide helpful, comprehensive, and appropriate answers to the user questions.\\n\\nAfter your counselling response, you must include a self-evaluation in the following format:\\n<evaluate>\\n{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\\n</evaluate>\\n\\nWhere score is a number from 1-10, and provide a clear explanation for your scoring.\\nExplain to metrics:\\n1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\\n2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\\n3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\\n4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\\n5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\\n6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\\n7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.'}, {'role': 'user', 'content': \"I've been struggling with alcohol use for a while now, and I want to find a way to manage it better or possibly quit altogether. I often feel overwhelmed and anxious, especially after work, which leads me to drink more than I intend. These feelings started a couple of years ago after some family issues and increased work stress. For example, last month I had a big argument with my spouse about my drinking, and since then, I've felt even more isolated and stressed. I notice I drink almost every evening, sometimes heavily, and it affects my sleep and mood the next day. My family dynamics have always been complicated, with high expectations and little room for open emotional expression, which makes it hard to talk about what I'm going through. In the past, I've tried to cut back on drinking by distracting myself with hobbies or exercise, but these only help temporarily. I am curious about what approaches in counseling might help me develop healthier coping mechanisms and how therapy addresses substance use issues. Can you explain how the process works and what to expect in terms of treatment options?\"}]\n",
      "\n",
      "Sample answer:\n",
      "Managing alcohol use involves understanding the triggers and emotions that lead to drinking, which you have started to identify well. Focusing on the situations and feelings that increase your urge to drink, such as stress and family conflicts, can help create strategies to respond differently. Developing healthier coping methods like mindfulness, relaxation techniques, or structured routines might reduce reliance on alcohol for relief. Exploring the impact of family dynamics and emotional expression together can also provide insight and foster healing in those relationships. The counseling process typically includes setting personalized goals, learning skills to manage cravings and emotions, and gradually building resilience against relapse. Sessions are a safe space to discuss experiences without judgment, and treatment approaches can range from behavioral therapies to support groups, tailored to your preferences and needs. Regular check-ins help track progress and adjust strategies as needed. Questions about specific techniques or concerns about commitment and confidentiality are important to address as well, ensuring the approach aligns with your comfort and goals.\n"
     ]
    }
   ],
   "source": [
    "def format_dataset_for_grpo(x):\n",
    "    instruction = x[\"instruction\"]\n",
    "    user_input = x[\"input\"] if pd.notna(x[\"input\"]) else \"\"\n",
    "    \n",
    "    # Create the conversation format\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "\n",
    "# Prepare dataset for GRPO\n",
    "dataset[\"prompt\"] = dataset.apply(format_dataset_for_grpo, axis=1)\n",
    "dataset[\"answer\"] = dataset[\"output\"]  # Use the original answer as reference\n",
    "\n",
    "print(\"Sample prompt:\")\n",
    "print(dataset[\"prompt\"][0])\n",
    "print(\"\\nSample answer:\")\n",
    "print(dataset[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing reward functions:\n",
      "Format check: [10.0]\n",
      "No extra text: [2.0]\n",
      "No repetition: [1.0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from langdetect import detect\n",
    "\n",
    "# Create regex to match the evaluation format\n",
    "evaluation_regex = re.compile(\n",
    "    rf\"{evaluation_start}(.+?){evaluation_end}\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def check_evaluation_format(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function for checking if the response follows the evaluation format exactly.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Check if evaluation format is present\n",
    "        if evaluation_regex.search(response) is not None:\n",
    "            score += 5.0  # High reward for format compliance\n",
    "            \n",
    "            # Extract and validate JSON structure\n",
    "            try:\n",
    "                match = evaluation_regex.search(response)\n",
    "                if match:\n",
    "                    json_content = match.group(1).strip()\n",
    "                    eval_data = json.loads(json_content)\n",
    "                    \n",
    "                    # Check for required keys\n",
    "                    required_keys = [\n",
    "                        \"Active Listening\", \"Empathy & Validation\", \"Safety & Trustworthiness\",\n",
    "                        \"Open-mindedness & Non-judgment\", \"Clarity & Encouragement\", \n",
    "                        \"Boundaries & Ethical\", \"Holistic Approach\", \"Explaination for Scoring\"\n",
    "                    ]\n",
    "                    \n",
    "                    if all(key in eval_data for key in required_keys):\n",
    "                        score += 3.0  # Bonus for complete structure\n",
    "                    \n",
    "                    # Check if scores are numbers between 1-10\n",
    "                    score_keys = required_keys[:-1]  # Exclude explanation\n",
    "                    valid_scores = 0\n",
    "                    for key in score_keys:\n",
    "                        if key in eval_data:\n",
    "                            try:\n",
    "                                score_val = float(eval_data[key])\n",
    "                                if 1 <= score_val <= 10:\n",
    "                                    valid_scores += 1\n",
    "                            except (ValueError, TypeError):\n",
    "                                pass\n",
    "                    \n",
    "                    # Bonus for valid scores\n",
    "                    score += (valid_scores / len(score_keys)) * 2.0\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                score -= 1.0  # Penalty for invalid JSON\n",
    "        else:\n",
    "            score -= 3.0  # Penalty for missing evaluation\n",
    "            \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_no_extra_text(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to ensure no extra text after evaluation.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Find the last occurrence of </evaluate>\n",
    "        last_eval_end = response.rfind(evaluation_end)\n",
    "        if last_eval_end != -1:\n",
    "            text_after = response[last_eval_end + len(evaluation_end):].strip()\n",
    "            if not text_after:  # No text after evaluation\n",
    "                score += 2.0\n",
    "            else:\n",
    "                score -= 2.0  # Penalty for extra text\n",
    "        \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_language_consistency(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if response language matches input language.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    question_lang = detect(question)\n",
    "    # print(str(responses))\n",
    "\n",
    "    for rep in responses:\n",
    "        score = 0\n",
    "        # print(f\"Current text for detect lang {rep} - finish\")\n",
    "        if len(rep) > 5:\n",
    "            if detect(rep) == question_lang:\n",
    "                score += 1.0\n",
    "            \n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "    \n",
    "\n",
    "\n",
    "def check_no_repetition(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to penalize repetitive text.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Simple repetition check: split into sentences and check for exact duplicates\n",
    "        sentences = re.split(r'[.!?]+', response)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 0:\n",
    "            unique_sentences = set(sentences)\n",
    "            repetition_ratio = 1 - (len(unique_sentences) / len(sentences))\n",
    "            \n",
    "            if repetition_ratio < 0.1:  # Less than 10% repetition\n",
    "                score += 1.0\n",
    "            elif repetition_ratio > 0.3:  # More than 30% repetition\n",
    "                score -= 2.0\n",
    "        \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Test reward functions\n",
    "test_completion = [[\n",
    "    {\"content\": \"I understand your concerns. <evaluate>{\\\"Active Listening\\\" : 8, \\\"Empathy & Validation\\\": 9, \\\"Safety & Trustworthiness\\\" : 9, \\\"Open-mindedness & Non-judgment\\\" : 8, \\\"Clarity & Encouragement\\\" : 7, \\\"Boundaries & Ethical\\\" : 9, \\\"Holistic Approach\\\" : 8, \\\"Explaination for Scoring\\\": \\\"Good response\\\"}</evaluate>\"}\n",
    "]]\n",
    "\n",
    "print(\"Testing reward functions:\")\n",
    "print(f\"Format check: {check_evaluation_format(test_completion)}\")\n",
    "print(f\"No extra text: {check_no_extra_text(test_completion)}\")\n",
    "print(f\"No repetition: {check_no_repetition(test_completion)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for monitoring\n",
    "PRINTED_TIMES = 0\n",
    "PRINT_EVERY_STEPS = 3\n",
    "\n",
    "def debug_responses(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Debug function to print responses every few steps.\n",
    "    \"\"\"\n",
    "    global PRINTED_TIMES, PRINT_EVERY_STEPS\n",
    "    \n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        user_query = prompts[0][-1][\"content\"]\n",
    "        response = completions[0][0][\"content\"]\n",
    "        \n",
    "        print('*' * 50)\n",
    "        print(f\"Step {PRINTED_TIMES + 1}\")\n",
    "        print(f\"User Query: {user_query[:100]}...\")\n",
    "        print(f\"Response: {response[:200]}...\")\n",
    "        \n",
    "        # Check if evaluation format is present\n",
    "        has_eval = evaluation_start in response and evaluation_end in response\n",
    "        print(f\"Has Evaluation Format: {has_eval}\")\n",
    "        \n",
    "        if has_eval:\n",
    "            match = evaluation_regex.search(response)\n",
    "            if match:\n",
    "                try:\n",
    "                    eval_content = match.group(1).strip()\n",
    "                    json.loads(eval_content)\n",
    "                    print(\"Evaluation JSON: Valid\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Evaluation JSON: Invalid\")\n",
    "        print('*' * 50)\n",
    "    \n",
    "    PRINTED_TIMES += 1\n",
    "    return [0] * len(completions)  # Return neutral scores for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get only 500 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1951ee298184a4cb506fdbdbdccf1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length (90th percentile): 871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad06d8f631e048b7a93920512a58454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 4285\n",
      "Training dataset size: 500\n"
     ]
    }
   ],
   "source": [
    "# Convert to HuggingFace dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas to dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset[[\"prompt\", \"answer\"]])\n",
    "\n",
    "# Calculate token lengths\n",
    "def calculate_prompt_length(examples):\n",
    "    lengths = []\n",
    "    for prompt in examples[\"prompt\"]:\n",
    "        tokens = tokenizer.apply_chat_template(\n",
    "            prompt, \n",
    "            add_generation_prompt=True, \n",
    "            tokenize=True\n",
    "        )\n",
    "        lengths.append(len(tokens))\n",
    "    return {\"prompt_length\": lengths}\n",
    "\n",
    "hf_dataset = hf_dataset.map(calculate_prompt_length, batched=True)\n",
    "\n",
    "# Filter to keep only reasonable length prompts (top 90%)\n",
    "max_length = int(np.quantile(hf_dataset[\"prompt_length\"], 0.9))\n",
    "print(f\"Max prompt length (90th percentile): {max_length}\")\n",
    "\n",
    "# Filter dataset\n",
    "filtered_dataset = hf_dataset.filter(lambda x: x[\"prompt_length\"] <= max_length)\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Take a subset for training (adjust as needed)\n",
    "if len(filtered_dataset) > 500:\n",
    "    training_dataset = filtered_dataset.shuffle(seed=3407).select(range(500))\n",
    "else:\n",
    "    training_dataset = filtered_dataset.shuffle(seed=3407)\n",
    "\n",
    "print(f\"Training dataset size: {len(training_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adhoc code solve the accelerator issue\n",
    "\n",
    "# from trl import GRPOConfig, GRPOTrainer\n",
    "# import torch\n",
    "\n",
    "# # Ensure model is on correct device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">base-model-training-syntheic_v2_2-grpo</strong> at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/5yey09mz' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/5yey09mz</a><br> View project at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250616_055224-5yey09mz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tai/nlp/wandb/run-20250616_055309-ylbq01w2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/ylbq01w2' target=\"_blank\">base-model-training-syntheic_v2_2-grpo</a></strong> to <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/ylbq01w2' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/ylbq01w2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length: 921\n",
      "Max completion length: 3175\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"distress-chatbot\", name=\"base-model-training-syntheic_v2_2-grpo\", config={\n",
    "    \"model\": \"Qwen/Qwen3-4B\",\n",
    "    # \"max_steps\": 20000,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lambda_decay\": 0.95,\n",
    "})  # Allow resuming W&B run\n",
    "\n",
    "\n",
    "\n",
    "# Calculate max lengths\n",
    "max_prompt_length = max_length + 50  # Add some buffer\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "print(f\"Max prompt length: {max_prompt_length}\")\n",
    "print(f\"Max completion length: {max_completion_length}\")\n",
    "\n",
    "# VLLM sampling parameters\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    seed=3407,\n",
    "    stop=[tokenizer.eos_token],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "# GRPO training configuration\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params=vllm_sampling_params,\n",
    "    # temperature=0.8,\n",
    "    learning_rate=1e-6,  # Lower learning rate for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Increase for smoother training\n",
    "    num_generations=4,  # Number of responses to generate per prompt\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    max_steps=200,  # Start with fewer steps for testing\n",
    "    save_steps=50,\n",
    "    report_to=\"wandb\",  # Set to \"wandb\" if you want to use Weights & Biases\n",
    "    output_dir=\"trained_model_v2_2_grpo_checkpoint\",  # Directory to save the model\n",
    "    gradient_checkpointing = False,\n",
    "    # Add these parameters to handle device issues\n",
    "    # dataloader_pin_memory=False,\n",
    "    # dataloader_num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO Trainer initialized successfully!\n",
      "Training on 500 examples\n"
     ]
    }
   ],
   "source": [
    "# Initialize GRPO trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        check_evaluation_format,     # Primary reward: correct format\n",
    "        check_no_extra_text,         # Secondary: no extra text\n",
    "        check_language_consistency,  # Tertiary: language consistency\n",
    "        check_no_repetition,         # Quaternary: no repetition\n",
    "        debug_responses,             # Debug function\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    ")\n",
    "\n",
    "print(\"GRPO Trainer initialized successfully!\")\n",
    "print(f\"Training on {len(training_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GRPO training...\n",
      "Watch for the reward column to increase over time.\n",
      "The model should learn to follow the evaluation format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,000,000,000 (3.30% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/200 2:53:16 < 56:58, 0.01 it/s, Epoch 1.20/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / check_evaluation_format / mean</th>\n",
       "      <th>rewards / check_evaluation_format / std</th>\n",
       "      <th>rewards / check_no_extra_text / mean</th>\n",
       "      <th>rewards / check_no_extra_text / std</th>\n",
       "      <th>rewards / check_language_consistency / mean</th>\n",
       "      <th>rewards / check_language_consistency / std</th>\n",
       "      <th>rewards / check_no_repetition / mean</th>\n",
       "      <th>rewards / check_no_repetition / std</th>\n",
       "      <th>rewards / debug_responses / mean</th>\n",
       "      <th>rewards / debug_responses / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>-1.562500</td>\n",
       "      <td>2.333961</td>\n",
       "      <td>106.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>7.634683</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>2.461876</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>5.490171</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>-2.812500</td>\n",
       "      <td>0.269338</td>\n",
       "      <td>5.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>8.762948</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>-2.437500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>49.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>3.269575</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>0.394338</td>\n",
       "      <td>9.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>11.185880</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.683013</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>4.918156</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>0.394338</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>10.031704</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>-1.062500</td>\n",
       "      <td>2.269338</td>\n",
       "      <td>70.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>4.663632</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>-2.062500</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>589.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>589.000000</td>\n",
       "      <td>5.867577</td>\n",
       "      <td>-2.562500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>-1.937500</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>31.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>8.908331</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>111.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>2.546899</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.283800</td>\n",
       "      <td>-2.687500</td>\n",
       "      <td>0.489357</td>\n",
       "      <td>14.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>7.095400</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>-2.312500</td>\n",
       "      <td>0.489357</td>\n",
       "      <td>88.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>6.564149</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>0.769338</td>\n",
       "      <td>130.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>934.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>934.000000</td>\n",
       "      <td>6.609002</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>-2.312500</td>\n",
       "      <td>0.269338</td>\n",
       "      <td>134.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>5.882388</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>-2.125000</td>\n",
       "      <td>0.758694</td>\n",
       "      <td>33.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>6.641862</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>-2.562500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>41.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>5.557376</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>1.461279</td>\n",
       "      <td>164.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1650.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1650.000000</td>\n",
       "      <td>4.555993</td>\n",
       "      <td>-2.562500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.394338</td>\n",
       "      <td>446.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1355.000000</td>\n",
       "      <td>2.918376</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>2.340264</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>3.644440</td>\n",
       "      <td>166.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>3.613241</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>2.098423</td>\n",
       "      <td>261.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1489.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>261.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1489.000000</td>\n",
       "      <td>2.942325</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>4.635011</td>\n",
       "      <td>367.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>180.200012</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>996.000000</td>\n",
       "      <td>6.504827</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>3.795286</td>\n",
       "      <td>211.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>211.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>1.173874</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.614357</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>106.200005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>2.985033</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>2.205890</td>\n",
       "      <td>502.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>323.933350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1238.000000</td>\n",
       "      <td>4.163848</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>0.454124</td>\n",
       "      <td>220.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1244.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1244.000000</td>\n",
       "      <td>1.654500</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>6.427155</td>\n",
       "      <td>420.062500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1439.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>420.062500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1439.000000</td>\n",
       "      <td>0.960122</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.704124</td>\n",
       "      <td>164.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>3.472090</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>317.437500</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1907.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>317.437500</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1907.000000</td>\n",
       "      <td>0.692114</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.604640</td>\n",
       "      <td>348.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>348.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1105.000000</td>\n",
       "      <td>2.561059</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>2.165064</td>\n",
       "      <td>476.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2277.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>476.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2277.000000</td>\n",
       "      <td>0.870662</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>229.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>229.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>4.863557</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>2.022181</td>\n",
       "      <td>790.187500</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>449.500031</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>0.773418</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>4.284643</td>\n",
       "      <td>161.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>726.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>161.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>726.000000</td>\n",
       "      <td>3.085336</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>270.062500</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>906.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.062500</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>906.000000</td>\n",
       "      <td>2.449417</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2.657863</td>\n",
       "      <td>637.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>467.933350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2490.000000</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>3.480480</td>\n",
       "      <td>622.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>452.333344</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1516.000000</td>\n",
       "      <td>1.012073</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>2.329124</td>\n",
       "      <td>780.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>438.142883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1477.000000</td>\n",
       "      <td>1.144447</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.614357</td>\n",
       "      <td>449.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1547.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>449.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1547.000000</td>\n",
       "      <td>0.753962</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>299.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1273.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>299.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1273.000000</td>\n",
       "      <td>0.832281</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>5.590825</td>\n",
       "      <td>927.875000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>606.857178</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2276.000000</td>\n",
       "      <td>0.763108</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>770.062500</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2273.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>770.062500</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2273.000000</td>\n",
       "      <td>0.611783</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.957427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.703768</td>\n",
       "      <td>680.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>513.933350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1.025632</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>4.732864</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.544614</td>\n",
       "      <td>595.375000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>595.375000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>4.342649</td>\n",
       "      <td>766.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2390.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>766.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2390.000000</td>\n",
       "      <td>0.675314</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>6.517708</td>\n",
       "      <td>716.125000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1509.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>716.125000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1509.000000</td>\n",
       "      <td>0.575591</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>3.092890</td>\n",
       "      <td>943.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>795.066711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2453.000000</td>\n",
       "      <td>0.559137</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>3.587014</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>780.812500</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>780.812500</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>0.763792</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.056349</td>\n",
       "      <td>1125.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>989.266724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3089.000000</td>\n",
       "      <td>0.754486</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>4.044722</td>\n",
       "      <td>885.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>732.600037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2090.000000</td>\n",
       "      <td>0.630146</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.035258</td>\n",
       "      <td>725.687500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1863.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>725.687500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1863.000000</td>\n",
       "      <td>0.884829</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.670538</td>\n",
       "      <td>1024.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>717.571472</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1774.000000</td>\n",
       "      <td>0.562976</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3.825648</td>\n",
       "      <td>1511.250000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1127.307739</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>2245.000000</td>\n",
       "      <td>0.470095</td>\n",
       "      <td>-1.312500</td>\n",
       "      <td>3.842200</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>5.897181</td>\n",
       "      <td>1357.875000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1098.285767</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2484.000000</td>\n",
       "      <td>0.575227</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.146518</td>\n",
       "      <td>907.937500</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>584.071472</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1444.000000</td>\n",
       "      <td>0.486653</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>7.837117</td>\n",
       "      <td>1031.937500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2062.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1031.937500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2062.000000</td>\n",
       "      <td>0.583410</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>4.559584</td>\n",
       "      <td>1134.750000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>2565.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1134.750000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>2565.000000</td>\n",
       "      <td>0.486218</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>4.732864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>47.611300</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>3.962117</td>\n",
       "      <td>1238.500000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>791.615417</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>2143.000000</td>\n",
       "      <td>1190.282455</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>1430.562500</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1028.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2644.000000</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>8.300508</td>\n",
       "      <td>1533.875000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>986.833374</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>0.400838</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>6.029438</td>\n",
       "      <td>1027.937500</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>884.800049</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>0.567680</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>4.809279</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>1447.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3047.000000</td>\n",
       "      <td>0.417609</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>7.672719</td>\n",
       "      <td>1371.562500</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1113.928589</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2490.000000</td>\n",
       "      <td>1.589508</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.534758</td>\n",
       "      <td>1373.875000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>958.230835</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>2114.000000</td>\n",
       "      <td>0.444653</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>4.732864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.032796</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>7.146171</td>\n",
       "      <td>1693.437500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1351.538452</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2746.000000</td>\n",
       "      <td>0.470334</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>4.477113</td>\n",
       "      <td>1012.312500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>703.357178</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>0.491834</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.943000</td>\n",
       "      <td>1196.062500</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1064.133423</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>2577.000000</td>\n",
       "      <td>0.633914</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>7.237230</td>\n",
       "      <td>1295.062500</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1169.733398</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>2987.000000</td>\n",
       "      <td>0.400733</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>6.035658</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.454877</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.216270</td>\n",
       "      <td>1001.250000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>2381.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1001.250000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>2381.000000</td>\n",
       "      <td>0.539542</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>3.855410</td>\n",
       "      <td>1126.062500</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>989.466736</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1850.000000</td>\n",
       "      <td>0.487934</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>4.368040</td>\n",
       "      <td>1279.375000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1008.571472</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2711.000000</td>\n",
       "      <td>0.490473</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>7.955395</td>\n",
       "      <td>1100.750000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>962.466736</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>2030.000000</td>\n",
       "      <td>0.443177</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.822386</td>\n",
       "      <td>1630.500000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1409.857178</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>2701.000000</td>\n",
       "      <td>0.474122</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>6.559662</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.543805</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>6.443224</td>\n",
       "      <td>929.312500</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2989.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>929.312500</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2989.000000</td>\n",
       "      <td>0.611331</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>3.545286</td>\n",
       "      <td>1102.250000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>964.066711</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>0.508680</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>3.776234</td>\n",
       "      <td>870.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>716.333374</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1864.000000</td>\n",
       "      <td>0.699058</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>7.822103</td>\n",
       "      <td>867.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>867.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>1.849428</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>6.071450</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.170286</td>\n",
       "      <td>843.625000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>688.200012</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1319.000000</td>\n",
       "      <td>0.870499</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.085350</td>\n",
       "      <td>977.250000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>830.733398</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>2205.000000</td>\n",
       "      <td>0.541175</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>6.674781</td>\n",
       "      <td>1977.375000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1701.000122</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>2709.000000</td>\n",
       "      <td>0.410438</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>1.232143</td>\n",
       "      <td>4.556382</td>\n",
       "      <td>1094.250000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>3071.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1094.250000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>3071.000000</td>\n",
       "      <td>0.585237</td>\n",
       "      <td>-0.330357</td>\n",
       "      <td>4.948639</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>5.460350</td>\n",
       "      <td>1450.687500</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1204.357178</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>2171.000000</td>\n",
       "      <td>0.551458</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>2.144338</td>\n",
       "      <td>1366.562500</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1108.214355</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>2383.000000</td>\n",
       "      <td>0.497541</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>8.031361</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2634.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2634.000000</td>\n",
       "      <td>0.500588</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>5.549234</td>\n",
       "      <td>2049.625000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1674.500000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>3072.000000</td>\n",
       "      <td>0.442986</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>5.715112</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>7.357602</td>\n",
       "      <td>1410.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1158.785767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2232.000000</td>\n",
       "      <td>0.462563</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.880304</td>\n",
       "      <td>1391.812500</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1272.933350</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>2424.000000</td>\n",
       "      <td>0.460706</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>5.137465</td>\n",
       "      <td>1361.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1240.066772</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2856.000000</td>\n",
       "      <td>0.493323</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>3.607869</td>\n",
       "      <td>1088.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>949.733398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3137.000000</td>\n",
       "      <td>0.601887</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>6.180732</td>\n",
       "      <td>1403.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1285.600098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>0.510086</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.957427</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>6.205086</td>\n",
       "      <td>1525.500000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1144.846191</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>2574.000000</td>\n",
       "      <td>0.484691</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>6.794643</td>\n",
       "      <td>5.990813</td>\n",
       "      <td>1164.437500</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>2362.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1164.437500</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>2362.000000</td>\n",
       "      <td>0.424520</td>\n",
       "      <td>5.357143</td>\n",
       "      <td>6.038200</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.707825</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.550858</td>\n",
       "      <td>1714.437500</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1505.785767</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3152.000000</td>\n",
       "      <td>0.411058</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>6.046693</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.437591</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1366.437500</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>949.076965</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>2095.000000</td>\n",
       "      <td>0.507677</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>5.533292</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1205.933350</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>2290.000000</td>\n",
       "      <td>0.438572</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>4.830127</td>\n",
       "      <td>627.312500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1695.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>627.312500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1695.000000</td>\n",
       "      <td>0.509379</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>5.233288</td>\n",
       "      <td>1558.500000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1327.571533</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>2750.000000</td>\n",
       "      <td>0.424881</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>8.016650</td>\n",
       "      <td>1478.187500</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1235.785767</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>2481.000000</td>\n",
       "      <td>1.014465</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.288930</td>\n",
       "      <td>917.187500</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>3015.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>917.187500</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>3015.000000</td>\n",
       "      <td>0.556429</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>3.587014</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>5.147181</td>\n",
       "      <td>777.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1283.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1283.000000</td>\n",
       "      <td>0.470506</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>5.587117</td>\n",
       "      <td>918.375000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>2387.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>918.375000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>2387.000000</td>\n",
       "      <td>0.490499</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>6.371813</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>5.375000</td>\n",
       "      <td>7.099250</td>\n",
       "      <td>1373.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>958.307739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2548.000000</td>\n",
       "      <td>1.040765</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.437591</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>6.653637</td>\n",
       "      <td>1641.750000</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1287.923096</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>2176.000000</td>\n",
       "      <td>0.613859</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>5.704284</td>\n",
       "      <td>1423.250000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1173.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>2407.000000</td>\n",
       "      <td>0.463844</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>4.781910</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>5.336758</td>\n",
       "      <td>1457.187500</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1211.785767</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>2355.000000</td>\n",
       "      <td>0.459093</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>5.728037</td>\n",
       "      <td>1381.562500</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>967.692322</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1821.000000</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.957427</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>6.177462</td>\n",
       "      <td>1263.562500</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1136.133423</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>0.402734</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>5.009721</td>\n",
       "      <td>1415.562500</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1164.214355</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2765.000000</td>\n",
       "      <td>0.540374</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>5.031567</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.992652</td>\n",
       "      <td>1374.562500</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1254.533447</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>2784.000000</td>\n",
       "      <td>0.490932</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>7.477390</td>\n",
       "      <td>1115.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>977.800049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1742.000000</td>\n",
       "      <td>0.585272</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.676842</td>\n",
       "      <td>1563.125000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1025.833374</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2920.000000</td>\n",
       "      <td>0.562356</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>5.825566</td>\n",
       "      <td>1558.375000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1327.428589</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>0.534428</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>6.566236</td>\n",
       "      <td>1453.500000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1338.733398</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>3105.000000</td>\n",
       "      <td>0.450315</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>3.170286</td>\n",
       "      <td>1693.187500</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1199.250000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>2159.000000</td>\n",
       "      <td>0.493907</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>3.587014</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>8.356595</td>\n",
       "      <td>1173.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2841.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1173.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2841.000000</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>5.270033</td>\n",
       "      <td>1333.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1069.857178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2192.000000</td>\n",
       "      <td>0.685775</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>5.383282</td>\n",
       "      <td>1562.312500</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1024.750000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>2270.000000</td>\n",
       "      <td>4.870512</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5.347897</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>7.492788</td>\n",
       "      <td>1636.187500</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1281.077026</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>2941.000000</td>\n",
       "      <td>0.414206</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>6.375000</td>\n",
       "      <td>7.477950</td>\n",
       "      <td>1618.562500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1514.800049</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3125.000000</td>\n",
       "      <td>0.524348</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>6.099506</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1480.230835</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>3131.000000</td>\n",
       "      <td>0.481209</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>5.324245</td>\n",
       "      <td>1518.562500</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1408.133423</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>2557.000000</td>\n",
       "      <td>0.412429</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>5.981328</td>\n",
       "      <td>1428.250000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1311.800049</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>2525.000000</td>\n",
       "      <td>0.472213</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>5.942783</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.543805</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>6.437500</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>1329.937500</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1206.933350</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>3021.000000</td>\n",
       "      <td>0.509599</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>3.658042</td>\n",
       "      <td>2256.500000</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1542.111084</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>2587.000000</td>\n",
       "      <td>0.337909</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.536317</td>\n",
       "      <td>1794.812500</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1597.642944</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>2664.000000</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.543805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>5.312500</td>\n",
       "      <td>8.215630</td>\n",
       "      <td>1194.562500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2366.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1194.562500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2366.000000</td>\n",
       "      <td>0.485423</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>4.307686</td>\n",
       "      <td>1843.875000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1400.166748</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>2448.000000</td>\n",
       "      <td>2.664801</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.586317</td>\n",
       "      <td>1325.250000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1201.933350</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>3125.000000</td>\n",
       "      <td>0.417516</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.460594</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>4.462036</td>\n",
       "      <td>1396.125000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1142.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>0.403222</td>\n",
       "      <td>4.169643</td>\n",
       "      <td>6.071281</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.821172</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>4.064375</td>\n",
       "      <td>1600.375000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1075.500000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>0.453051</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>6.937500</td>\n",
       "      <td>5.882403</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1003.266724</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>0.398523</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>5.389805</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.612452</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.447564</td>\n",
       "      <td>1729.687500</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1396.153931</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>2983.000000</td>\n",
       "      <td>0.471332</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>5.776028</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.612452</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>4.294643</td>\n",
       "      <td>4.562690</td>\n",
       "      <td>1670.375000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1455.428589</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>2886.000000</td>\n",
       "      <td>0.636256</td>\n",
       "      <td>2.607143</td>\n",
       "      <td>5.557737</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>6.812500</td>\n",
       "      <td>5.584799</td>\n",
       "      <td>1861.750000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1774.200073</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>3083.000000</td>\n",
       "      <td>0.370619</td>\n",
       "      <td>4.812500</td>\n",
       "      <td>5.901624</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.746425</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>4.262530</td>\n",
       "      <td>1557.187500</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1183.846191</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>2184.000000</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>5.312500</td>\n",
       "      <td>5.316844</td>\n",
       "      <td>1544.625000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1311.714355</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2111.000000</td>\n",
       "      <td>0.416592</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>3.897181</td>\n",
       "      <td>1892.750000</td>\n",
       "      <td>419.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>1309.909180</td>\n",
       "      <td>419.000000</td>\n",
       "      <td>2821.000000</td>\n",
       "      <td>0.396744</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>4.312500</td>\n",
       "      <td>7.403878</td>\n",
       "      <td>1415.562500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1164.214355</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2422.000000</td>\n",
       "      <td>0.542175</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>5.425233</td>\n",
       "      <td>1356.437500</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1235.200073</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>2699.000000</td>\n",
       "      <td>0.427731</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.030776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>7.117804</td>\n",
       "      <td>1661.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1311.923096</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2862.000000</td>\n",
       "      <td>0.618006</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>9.562500</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>6.783960</td>\n",
       "      <td>1591.437500</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1226.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>3032.000000</td>\n",
       "      <td>239.061502</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.408309</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>6.044643</td>\n",
       "      <td>6.877172</td>\n",
       "      <td>1222.125000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>943.142883</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2260.000000</td>\n",
       "      <td>0.460255</td>\n",
       "      <td>3.794643</td>\n",
       "      <td>5.869087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.878867</td>\n",
       "      <td>1559.375000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1186.538452</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>2623.000000</td>\n",
       "      <td>0.504868</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.437591</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>7.562608</td>\n",
       "      <td>2186.562500</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1417.777832</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>2828.000000</td>\n",
       "      <td>0.405456</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>6.062500</td>\n",
       "      <td>6.799953</td>\n",
       "      <td>1310.125000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1043.714355</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>2045.000000</td>\n",
       "      <td>0.449014</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>6.216913</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.612452</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>6.085350</td>\n",
       "      <td>1517.375000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1280.571533</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>3121.000000</td>\n",
       "      <td>0.478195</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>7.940339</td>\n",
       "      <td>1225.125000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>775.153870</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>0.383857</td>\n",
       "      <td>4.812500</td>\n",
       "      <td>5.901624</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.258306</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>6.404623</td>\n",
       "      <td>1456.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>883.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>0.957049</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Step 34\n",
      "User Query: ThÆ°a bÃ¡c sÄ©, tÃ´i mong muá»‘n qua cÃ¡c buá»•i tÆ° váº¥n nÃ y, tÃ´i cÃ³ thá»ƒ tÃ¬m ra cÃ¡ch Ä‘á»ƒ kiá»ƒm soÃ¡t nhá»¯ng cáº£m xÃº...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 37\n",
      "User Query: æœ€è¿‘æˆ‘æ„Ÿè§‰è‡ªå·±çš„è‡ªå°Šå¿ƒå¾ˆä½ï¼Œç»å¸¸æ€€ç–‘è‡ªå·±æ˜¯ä¸æ˜¯ä¸å¤Ÿå¥½ï¼Œä¸ç®¡æ˜¯åœ¨å·¥ä½œä¸Šè¿˜æ˜¯äººé™…å…³ç³»ä¸­ï¼Œå¥½åƒæ€»è§‰å¾—åˆ«äººæ¯”æˆ‘æ›´æœ‰èƒ½åŠ›ï¼Œä¹Ÿä¸å¤ªæ•¢è¡¨è¾¾è‡ªå·±çš„æƒ³æ³•ã€‚è¿™ç§æƒ…ç»ªå¼€å§‹æœ‰å‡ ä¸ªæœˆäº†ï¼Œå°¤å…¶æ˜¯åœ¨å…¬å¸å¼€ä¼šæ—¶ï¼Œæˆ‘æ€»æ˜¯ç´§å¼ å¾—è¯´ä¸å‡ºè¯...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 40\n",
      "User Query: åœ¨è¿™æ¬¡å’¨è¯¢ä¸­ï¼Œæˆ‘å¸Œæœ›èƒ½å¤Ÿæ‰¾åˆ°ä¸€äº›æ–¹æ³•æ¥å¤„ç†æˆ‘æœ€è¿‘ç»å†çš„å¤±å»äº²äººçš„æ‚²ç—›ã€‚æˆ‘æ¯äº²å‡ ä¸ªæœˆå‰å»ä¸–äº†ï¼Œè¿™å¯¹æˆ‘å½±å“å¾ˆå¤§ã€‚æˆ‘å¸¸å¸¸æ„Ÿåˆ°éå¸¸éš¾è¿‡å’Œå­¤ç‹¬ï¼Œæœ‰æ—¶ç”šè‡³ä¼šçªç„¶æ„Ÿåˆ°èƒ¸å£é—·ç—›ï¼Œå¤´ç–¼ï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘å¾ˆä¸å®‰ã€‚æˆ‘æ„è¯†åˆ°è¿™äº›...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 43\n",
      "User Query: æœ€è¿‘æˆ‘å·¥ä½œä¸Šçš„äººéš›é—œä¿‚è®“æˆ‘æ„Ÿåˆ°å¥½å¤§å£“åŠ›ã€‚æˆ‘å¸Œæœ›é€éé€™æ¬¡è¼”å°ï¼Œèƒ½å­¸æœƒæ›´æœ‰æ•ˆåœ°è™•ç†åŒäº‹é–“çš„è¡çªï¼Œä¿æŒå¿ƒæƒ…å¹³éœï¼ŒåŒæ™‚æå‡è‡ªå·±çš„è‡ªä¿¡å’Œè¡¨é”èƒ½åŠ›ã€‚å·¥ä½œä¸­ï¼Œæˆ‘å¸¸å¸¸è¦ºå¾—åŒäº‹å°æˆ‘æœ‰åè¦‹ï¼Œå°¤å…¶æ˜¯å¹¾æ¬¡é–‹æœƒæ™‚ï¼Œæˆ‘æå‡ºçš„æ„è¦‹...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 46\n",
      "User Query: Gáº§n Ä‘Ã¢y tÃ´i cáº£m tháº¥y ráº¥t bá»‘i rá»‘i vÃ  Ã¡p lá»±c vá» váº¥n Ä‘á» tÃ¬nh dá»¥c trong cuá»™c sá»‘ng hÃ´n nhÃ¢n cá»§a mÃ¬nh. TÃ´i...\n",
      "Response:  TÃ´i ráº¥t mong nháº­n Ä‘Æ°á»£c sá»± tháº¥u hiá»ƒu vÃ  chia sáº» tá»« báº¡n....\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 49\n",
      "User Query: I've been struggling with some really intense feelings recently, and my main goal in coming here is ...\n",
      "Response:  Your answer would be greatly appreciated....\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 52\n",
      "User Query: I've been struggling with a lot of conflicting emotions since I left the military a few months ago. ...\n",
      "Response:  If I decide to seek more help, what kinds of resources or support groups are available?\n",
      "\n",
      "Best of all I want to feeling lighter than I do than I did arriving anywhere. Would you be willing to answer t...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 55\n",
      "User Query: Over the past few months, I have been feeling a significant distance growing between my wife and me,...\n",
      "Response:  I'm hoping to begin developing this conversation to bridge the gap we've encountered within our marriage.\n",
      "</think>...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 58\n",
      "User Query: I have been feeling increasingly uncertain and anxious about my career path recently, and I hope to ...\n",
      "Response:  Your guidance would be greatly appreciated.\n",
      "\n",
      "I am also concerned about how to address negative self-talk or shifts in mood triggered by setbacks related to decisions I've made. Are there specific way...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 61\n",
      "User Query: Ø£Ø±ØºØ¨ Ù…Ù† Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø© Ø£Ù† Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ø³ÙŠÙ† ØªÙ‚Ø¯ÙŠØ±ÙŠ Ù„Ø°Ø§ØªÙŠ ÙˆØ§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ù†ÙØ³ÙŠØŒ Ø®Ø§ØµØ© ÙÙŠ Ù…ÙˆØ§Ù‚Ù Ø§Ù„ØªÙˆØ§ØµÙ„...\n",
      "Response:  Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø³ÙˆØ¡ Ø§Ù„ÙÙ‡Ù… Ø£Ùˆ Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ø¬Ø§Ø¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¯ ÙŠÙˆØ¬Ù‡ Ù…Ù† Ø§Ù„Ø¹Ø§Ø¦Ù„Ø©ØŸ Ø£æ€ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£ÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ù…Ù† ÙƒØ±Ù‡ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© Ø£Ùˆ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ù‚Ø¯ ØªØ¤Ø«Ø± Ø¹Ù„ÙŠÙ‘ØŸ \n",
      "\n",
      "Ø¹Ø¨Ø± Ù‡Ø°Ù‡ Ø§Ù„ØªØ¬Ø±Ø¨Ø©ØŒ Ø£Ø±ØºØ¨ ÙÙŠ ÙˆØ¬ÙˆØ¯ Ø¯Ø¹Ù… Ù„Ù…Ù†Ø§Ù‚Ø´Ø©...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 64\n",
      "User Query: TÃ´i mong muá»‘n qua buá»•i tÆ° váº¥n nÃ y cÃ³ thá»ƒ tÃ¬m ra cÃ¡ch Ä‘á»ƒ giáº£m bá»›t cáº£m giÃ¡c xáº¥u há»• vÃ  lo Ã¢u mÃ  tÃ´i thÆ°...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 67\n",
      "User Query: I've been feeling overwhelmed and anxious quite frequently over the past few months. My main goal fo...\n",
      "Response:  And what role does family or social support play in this process?\n",
      "\n",
      "I truly want to improve my well-being, but I often hesitate to ask for help because I worry about putting others through a difficult...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 70\n",
      "User Query: I've been feeling increasingly overwhelmed with parenting my two adolescent children. My main goal f...\n",
      "Response:  When do you think I should consider professional help, and are certain options confidentiality-bound? I worry about what might happen if I keep pressing forward in my parenting efforts despite strugg...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 73\n",
      "User Query: Ø£Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¹Ù„Ø§Ù‚Ø§ØªÙŠ ÙÙŠ Ù…ÙƒØ§Ù† Ø§Ù„Ø¹Ù…Ù„ Ù„Ø£Ù†Ù†ÙŠ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„ØªÙˆØªØ± ÙˆØ§Ù„Ø¶ØºØ· Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø¨Ø³Ø¨Ø¨ Ø®Ù„Ø§ÙØ§Øª Ù…ØªÙƒØ±Ø±Ø© Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ø²Ù…...\n",
      "Response:  ÙˆÙ‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ÙÙ‡Ù… ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ù†ÙØ³ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªØ­ØªØ±Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© ÙˆØ§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© Ø§Ù„ØªÙŠ Ø£Ø¹ÙŠØ´Ù‡Ø§ØŸ ÙˆÙ‡Ù„ Ù‡Ù†Ø§Ùƒ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø¹Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù…Ù‡Ø§Ø±Ø§Øª ØªÙˆØ§ØµÙ„ Ø£ÙØ¶Ù„ Ù…Ø¹ Ø²Ù…Ù„Ø§Ø¦ÙŠ ÙˆÙ…Ø¹ Ù…Ù†Ø§Ø²Ù„ØªÙ†Ø§ØŸ Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£ØªÙ…...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 76\n",
      "User Query: æˆ‘å¸Œæœ›é€éé€™æ¬¡è¼”å°èƒ½å¤ æ‰¾åˆ°æ–¹æ³•ï¼Œèµ°å‡ºå®¶åº­æš´åŠ›çš„é™°å½±ï¼Œé‡æ–°å»ºç«‹è‡ªä¿¡å’Œå®‰å…¨æ„Ÿã€‚è¿‘å¹¾å€‹æœˆä¾†ï¼Œæˆ‘æ„Ÿåˆ°éå¸¸ç„¦æ…®å’Œå®³æ€•ï¼Œæ™‚å¸¸è¦ºå¾—å¿ƒè·³åŠ é€Ÿï¼Œæ™šä¸Šä¹Ÿé›£ä»¥å…¥ç¡ã€‚é€™äº›æƒ…ç·’å¤§éƒ¨åˆ†æ™‚å€™éƒ½æ˜¯å› ç‚ºå›æƒ³èµ·å®¶è£¡çš„çˆ­åµå’Œæš´åŠ›äº‹ä»¶æ‰€å¼•ç™¼...\n",
      "Response: æˆ‘æƒ³çŸ¥é“å¦‚æœæˆ‘æƒ³é€€å‡ºä¸€æ®µè®“æˆ‘ä¸å®‰å…¨çš„é—œä¿‚ï¼Œæœƒçµ¦æˆ‘ä»€éº¼å¹«åŠ©ï¼Ÿæ˜¯å¦æœ‰å®‰å…¨çš„ç’°å¢ƒå¯ä»¥å°‹æ±‚æ©è­·ï¼Ÿæˆ‘å¾ˆæ„Ÿè¬ä½ é¡˜æ„å‚¾è½æˆ‘é€™äº›ç¨®ç¨®å›°é›£ï¼Œä¹Ÿå¸Œæœ›æ“æœ‰æ›´å¤šä¿¡æ¯å’Œæ”¯æ´ã€‚æˆ‘è‡´åŠ›æ–¼å¾é€™æ¬¡ç¶“æ­·ä¸­å­¸ç¿’ï¼Œæ‰¾åˆ°ä¸€ç¨®æ›´å¥½çš„ç”Ÿæ´»æ–¹å¼ã€‚\n",
      "\n",
      "æˆ‘å¸Œæœ›é€éé€™æ¬¡è¼”å°ï¼Œæ‰¾åˆ°é‡‹æ”¾å’Œå†·éœçš„æ–¹å¼ï¼Œé‡æ–°å‰µé€ è‡ªå·±çš„åƒ¹å€¼ã€‚æˆ‘çš„æœªä¾†æ‡‰è©²æ˜¯æˆ‘è‡ªå·±é¸æ“‡ï¼Œè€Œä¸æ˜¯è¢«éå»çš„ç—›è‹¦æ‰€å›°ã€‚æ„Ÿè¬ä½ ï¼Œæˆ‘å¾ˆå¥½å¥‡ï¼Œæ¸¬è©¦ä½ èƒ½ä¸èƒ½å»ºæ§‹æ›´å…¨é¢çš„å®‰å…¨é¿é¢¨æ¸¯ã€‚å¸Œæœ›é€™äº› Ø§Ù„Ø¥Ø«Ù†Ø§Ø¨peri...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 79\n",
      "User Query: Gáº§n Ä‘Ã¢y, tÃ´i cáº£m tháº¥y ráº¥t lo láº¯ng vÃ  cÄƒng tháº³ng, nháº¥t lÃ  khi nghÄ© vá» nhá»¯ng tráº£i nghiá»‡m trong quÃ¢n Ä‘á»™...\n",
      "Response:  TÃ´i cáº£m tháº¥y cuá»‘n vá» viá»‡c tÃ¬m kiáº¿m sá»± giÃºp Ä‘á»¡ vÃ  ráº¥t mong tá»« nhá»¯ng láº§n tÆ° váº¥n nÃ y, tÃ´i cÃ³ thá»ƒ tá»± tin hÆ¡n khi nÃ³i ra chÃ­nh mÃ¬nh.\n",
      "\n",
      "Cáº£m Æ¡n báº¡n ráº¥t nhiá»u Ä‘Ã£ láº¯ng nghe vÃ  dÃ nh thá»i gian Ä‘á»ƒ chia sáº» nhá»¯ng s...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 82\n",
      "User Query: æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ¥å—è‡ªå·±çš„èº«ä»½ã€‚æˆ‘æ˜¯ä¸€åä¸­å¹´å¥³æ€§ï¼Œæœ€è¿‘ä¸€ç›´æ„Ÿåˆ°æƒ…ç»ªä½è½å’Œç„¦è™‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å·¥ä½œå’Œå®¶åº­ç”Ÿæ´»ä¸­æ„Ÿå—åˆ°å¾ˆå¤§çš„å‹åŠ›ã€‚å¯èƒ½æ˜¯å› ä¸ºæˆ‘ä½œä¸ºä¸€åLGBTQç¾¤ä½“æˆå‘˜ï¼Œå†…å¿ƒä¸€ç›´æœ‰å¾ˆå¤šæŒ£æ‰å’Œå›°...\n",
      "Response: è¿™è¶Ÿæ—…ç¨‹ä¼šæœ‰å¤šé•¿æ—¶é—´ï¼Œé‡åˆ°ä»€ä¹ˆæƒ…å†µå¯ä»¥å¯»æ±‚å¸®åŠ©ï¼Ÿ\n",
      "\n",
      "æœ€åï¼Œæˆ‘ä¸€ç›´éƒ½åœ¨åŠªåŠ›ï¼Œä¹Ÿå¾ˆæ„Ÿæ¿€è‡ªå·±æ„¿æ„å¯»æ±‚æ”¯æŒã€‚å¸Œæœ›æœªæ¥çš„ä¸€åˆ‡éƒ½èƒ½é¡ºåˆ©ï¼Œä»¿ä½›æ™®é€šäººä¸€æ ·ç”Ÿæ´»ã€‚\n",
      "\n",
      "è°¢è°¢ä½ æ„¿æ„å¬æˆ‘è¯´è¿™äº›ã€‚æˆ‘å¤ªå¹³æ—¶æŠŠè‡ªå·±å›°åœ¨ä¸æ˜çš„æƒ³è±¡é‡Œï¼Œæ— æ³•æ¾å¼€è¿™é‡é‡å†…å¿ƒçš„é”é“¾ã€‚è°¢è°¢ã€‚\n",
      "\n",
      "æˆ‘å¸¦ç€å¸Œæœ›èƒ½è¢«ç†è§£ã€‚æ‚£è€…çš„æŠ±æ€¨è¡¨ç°åœ¨è¿™é‡Œã€‚æˆ‘çš„å›ç­”éœ€è¦å…·ä½“ç»“åˆå¥¹çš„æè¿°è¿›è¡Œæ”¯æŒã€‚éå¸¸æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨éœ€è¦ç¡®è®¤æ‚£è€…çš„èº«ä»½å’Œå…·ä½“æ¥æ„ã€‚æˆ‘ä¸ä¼šå¿½ç•¥å¥¹çš„æƒ…æ„Ÿå’Œç»å†ã€‚æ¥ä¸‹æ¥æˆ‘éœ€...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 85\n",
      "User Query: Gáº§n Ä‘Ã¢y, tÃ´i cáº£m tháº¥y ráº¥t khÃ³ kiá»ƒm soÃ¡t hÃ nh vi cá»§a mÃ¬nh, Ä‘áº·c biá»‡t lÃ  khi tÃ´i cÄƒng tháº³ng hoáº·c tá»©c gi...\n",
      "Response:   \n",
      "\n",
      "TÃ´i ráº¥t mong cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c má»™t hÆ°á»›ng Ä‘i Ä‘á»ƒ viá»‡c chá»‰ tiáº¿c thÆ°á»ng xuyÃªn khÃ´ng lÃ m tÃ´i thÃªm Ä‘au lÃ²ng vÃ  tá»± ti mÃ  pháº£n Ã¡nh sá»± tháº­t cá»§a báº£n thÃ¢n Ä‘Æ°á»£c cháº¥p nháº­n. Cáº£m Æ¡n vÃ  tÃ´i ráº¥t mong nháº­n Ä‘Æ°á»£c sá»± gi...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 88\n",
      "User Query: I've been feeling increasingly overwhelmed and anxious lately, and my main goal from this counseling...\n",
      "Response:  I appreciate your guidance and want to ensure I have the best hands-on assistance to get through this.\n",
      "\n",
      "A key concern I keep bringing up is the independence I find difficult after returning from mili...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 91\n",
      "User Query: I've been feeling very overwhelmed and anxious lately, and my main goal in coming to this session is...\n",
      "Response:  I really struggled when I thought I was going to need help, but despite talking to my family about my anxiety and feelings, I felt like I just got brushed off. Even after therapy sessions, Iâ€™m uncert...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 94\n",
      "User Query: I've been feeling really overwhelmed and anxious about my role as a parent lately. My main goal for ...\n",
      "Response:  Lastly, Iâ€™m curious about how interested or involved you would be inÑ‚ĞµĞ»ÑŒĞ½Ğ¾ä¼æˆ–è€…agnostic sessions if thatâ€™s appropriate. I really want to feel more connected and less worried as a parent compared to wha...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 97\n",
      "User Query: Gáº§n Ä‘Ã¢y, tÃ´i ráº¥t lo láº¯ng vá» viá»‡c bá»‘ tÃ´i cÃ³ kháº£ nÄƒng bá»‹ Alzheimer. TÃ´i muá»‘n qua buá»•i tÆ° váº¥n nÃ y, tÃ´i ...\n",
      "Response:  TÃ´i cÅ©ng muá»‘n biáº¿t thÃªm vá» cÃ¡ch nÃ y lá»›n trong gia Ä‘Ã¬nh viá»‡c nÃ y â€“ cÃ³ nÃªn ká»ƒ vá»›i ngÆ°á»i khÃ¡c hay trÃ² chuyá»‡n má»™t mÃ¬nh trong gia Ä‘Ã¬nh vá» tÃ¬nh huá»‘ng nÃ y khÃ´ng? TÃ´i ráº¥t cáº§n sá»± tÆ° váº¥n vÃ  giÃºp Ä‘á»¡ tá»« báº¡n Ä‘á»ƒ c...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 100\n",
      "User Query: Trong thá»i gian gáº§n Ä‘Ã¢y, em cáº£m tháº¥y ráº¥t khÃ³ kiá»ƒm soÃ¡t viá»‡c Äƒn uá»‘ng cá»§a mÃ¬nh. Em muá»‘n tÃ¬m cÃ¡ch Ä‘á»ƒ cáº£...\n",
      "Response:  VÃ  quan trá»ng hÆ¡n, em hy vá»ng má»i ngÆ°á»i xung quanh em cÃ³ thá»ƒ tháº¥u hiá»ƒu vÃ  khÃ´ng lÃ m cho báº¡n bÃ¨ hoáº·c gia Ä‘Ã¬nh cá»§a em cáº£m tháº¥y xáº¥u há»•, nhiá»u khi em Ä‘Ã£ máº¥t \"lÃ m thiá»‡t\" vÃ  cáº£m tháº¥y ráº¥t tháº¥t vá»ng Ä‘á»‘i vá»›i ...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 103\n",
      "User Query: æœ€è¿‘æˆ‘æ„Ÿè§‰è‡ªå·±åœ¨äººé™…äº²å¯†å…³ç³»æ–¹é¢é‡åˆ°äº†å¾ˆå¤§çš„å›°æ‰°ã€‚æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢ï¼Œèƒ½å¤Ÿæ‰¾åˆ°æ”¹å–„å’Œå»ºç«‹äº²å¯†å…³ç³»çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¹Ÿå¸Œæœ›èƒ½ç†è§£è‡ªå·±å†…å¿ƒçš„çœŸå®æ„Ÿå—ã€‚è¿‡å»å‡ ä¸ªæœˆï¼Œæˆ‘ç»å¸¸æ„Ÿåˆ°å­¤ç‹¬å’Œç„¦è™‘ï¼Œå°¤å…¶æ˜¯åœ¨ä¸æœ‹å‹æˆ–æ½œåœ¨ä¼´ä¾£äº¤...\n",
      "Response:  Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°: æ‚¨çš„å›ç­”ä¼šè®©æˆ‘æ„Ÿåˆ°è¢«çœŸæ­£ç†è§£å’Œæ¥çº³ï¼Œè¿™å¯¹æˆ‘çš„æˆé•¿éå¸¸é‡è¦ã€‚\n",
      "anyak\n",
      "\n",
      "å¤„ç†äº²å¯†å…³ç³»ä¸­çš„ç„¦è™‘å’Œè‡ªæˆ‘æ€€ç–‘éœ€è¦å¤šå±‚æ¬¡çš„å…³æ€€ã€‚æ‚¨æè¿°çš„å®¶åº­èƒŒæ™¯å’Œç«¥å¹´ç»å†å·²ç»å¯¹æ‚¨çš„æƒ…æ„Ÿèƒ½åŠ›é€ æˆäº†å½±å“ï¼Œè€Œæ‚¨çš„èº«ä½“ç—‡çŠ¶ä¾‹å¦‚å¤´ç—›å’Œèƒƒç—›ï¼Œå¾€å¾€æ˜¯æƒ…ç»ªåŒ–çš„å¤–åœ¨è¡¨ç°ï¼Œè¿™ç§æƒ…å†µä¹Ÿè¦é«˜åº¦é‡è§†ã€‚\n",
      "\n",
      "å¯ä»¥å°è¯•é€šè¿‡è®¤çŸ¥è¡Œä¸ºç–—æ³•ï¼ˆCBTï¼‰æ¥ç†è§£å¹¶è°ƒæ•´æ‚¨å¯¹è‡ªæˆ‘ä»·å€¼çš„è´Ÿé¢è¯„ä»·å’Œå¦è®¤è¡¨è¾¾çš„ææƒ§ã€‚åŒæ—¶ï¼Œç»“åˆæ‚¨çš„æƒ…æ„Ÿéœ€æ±‚ï¼Œå¯ä»¥åš...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 106\n",
      "User Query: I've been feeling quite confused and anxious about my experiences and feelings related to my sexuali...\n",
      "Response:  \n",
      "\n",
      "Are there tips or resources I can access to find online spaces where I can safely connect with others who might resonate with my experiences? Thank you for listening, and I really appreciate any gu...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 109\n",
      "User Query: Thá»i gian gáº§n Ä‘Ã¢y, tÃ´i cáº£m tháº¥y ráº¥t lo láº¯ng vÃ  Ã¡p lá»±c vá» viá»‡c nuÃ´i dáº¡y con cÃ¡i, Ä‘áº·c biá»‡t lÃ  vá»›i Ä‘á»©a ...\n",
      "Response:  Pháº£n há»“i tá»« ngÆ°á»i khÃ¡c thÆ°á»ng bá»‹ coi lÃ  yáº¿u Ä‘uá»‘i hoáº·c khÃ´ng nÃªn bÃ y tá» cáº£m xÃºc, khiáº¿n tÃ´i cáº£m tháº¥y ráº¥t cÃ´ Ä‘Æ¡n. TÃ´i ráº¥t tháº¯c máº¯c liá»‡u cÃ³ cÃ¡ch nÃ o giÃºp tÃ´i nÃªn chia sáº» khÃ³ khÄƒn má»™t cÃ¡ch an toÃ n, phÃ¹ há»£...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 112\n",
      "User Query: Ø£Ø±ØºØ¨ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ÙÙ‡Ù… ÙˆØ¶Ø¨Ø· Ù…Ø´Ø§Ø¹Ø±ÙŠ ÙˆØ³Ù„ÙˆÙƒÙŠ Ø§Ù„Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¥Ø¯Ù…Ø§Ù† Ø§Ù„Ø°ÙŠ Ø£ÙˆØ§Ø¬Ù‡Ù‡ØŒ ÙˆØ£ØªÙ…Ù†Ù‰ Ø£Ù† Ø£Ø¬Ø¯ Ø·Ø±Ù‚Ø§Ù‹ ...\n",
      "Response:  Ø£atsby Await your response ON Addiction. I'm awaiting your guidance on how to manage this addiction in a way that respects my ethnic and social values.\n",
      "\n",
      "<evaluate>\n",
      "{\"Active Listening\": 9, \"Empathy & ...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 115\n",
      "User Query: Ø£Ø±ØºØ¨ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø§Ù„Ù†ÙØ³ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ù† Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø¹Ù„Ø§Ø¬ÙŠØ©. Ø£Ø´Ø¹Ø± Ø¨Ø¶ØºØ· Ø´Ø¯ÙŠØ¯ ÙŠØªØ²Ø§ÙŠØ¯ ÙŠÙˆÙ…...\n",
      "Response:  Ø£ ì–´ë–»ê²Œ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£ØªØ­Ø¯Ø« Ø¹Ù† Ù…Ø´Ø§Ø¹Ø±ÙŠ Ù…Ø¹ Ø¹Ø§Ø¦Ù„ØªÙŠ Ø¨Ø´ÙƒÙ„ ÙŠØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø£ÙƒØ«Ø±ØŸ à¸™à¸­à¸à¸ˆà¸²à¸ Ø°Ù„ÙƒØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØªØ³Ø§Ø¹Ø¯Ù†ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø§Øª ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªÙˆØªØ± Ø§Ù„Ù†Ø§ØªØ¬ Ø¹Ù† Ø¶ØºÙˆØ· ÙˆĞ¸Ñ„Ğ¸ĞºĞ° Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ù‚Ù„ÙŠØ© Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ØŸ Ø£Ùˆ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 118\n",
      "User Query: Ø£Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ù†ÙˆÙ…ÙŠ Ù„Ø£Ù†Ù†ÙŠ Ø£Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ø£Ø±Ù‚ Ø§Ù„Ù…Ø²Ù…Ù† Ù…Ù†Ø° Ø¹Ø¯Ø© Ø£Ø´Ù‡Ø±ØŒ ÙˆÙ‡Ø°Ø§ ÙŠØ¤Ø«Ø± Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø¹Ù„Ù‰ ØªØ±ÙƒÙŠØ²ÙŠ ÙˆØ£Ø¯...\n",
      "Response:  Ø£Ù†Ø§ Ø¹Ø§Ø±Ø¶ Ø£Ù† ØªØ£Ø®Ø°Ù†ÙŠ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¹Ø²Ù„ Ø¹Ù† Ø¹Ø§Ø¦Ù„ØªÙŠ Ø£Ùˆ Ø£Ù† Ø£ØªØ­Ø¯Ø« Ø¹Ù† Ø¹Ù„Ø§Ù‚Ø§ØªÙŠ Ø§Ù„Ø´Ø®ØµÙŠØ©ØŒ Ù„Ø°Ù„Ùƒ Ø³Ø£ÙˆØ¶Ø­ ÙƒÙ„ Ø´ÙŠØ¡ Ù‡Ù†Ø§ ÙÙ‚Ø·.\n",
      "\n",
      "Ø£ÙŠÙ† ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØµØ§Ø¦Ø­ ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ£ modne mindØŸ Ù‡Ù†Ø§Ùƒ Ø£Ù…ÙˆØ± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£clusions Ø¨Ù‡Ø§ØŒ ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 121\n",
      "User Query: æˆ‘å¸Œæœ›ä»Šæ¬¡è¼”å°èƒ½å¹«æˆ‘æ¸›è¼•å£“åŠ›åŒç„¦æ…®ï¼Œä»¤æˆ‘å¯ä»¥æ›´åŠ å†·éœåŒæœ‰è‡ªä¿¡é¢å°ç”Ÿæ´»åŒå·¥ä½œå˜…æŒ‘æˆ°ã€‚æœ€è¿‘æˆ‘è¦ºå¾—å¿ƒæƒ…å¥½ä½è½ï¼Œç¶“å¸¸æ„Ÿåˆ°ç„¡åŠ›åŒç…©èºï¼Œè…¦æµ·å…¥é¢æˆæ—¥æœ‰å¥½å¤šè² é¢è«—æ³•ï¼Œå¥½ä¼¼è‡ªå·±åšå¾—å””å¤ å¥½ï¼Œæ€•åŒäº‹æˆ–è€…æœ‹å‹å°æˆ‘æœ‰å””å¥½å˜…ç‡...\n",
      "Response: æˆ‘æœŸæœ›ä»Šæ¬¡è¼”å°å¯ä»¥ä»¤æˆ‘ Blowjob åŒæ§åˆ¶å‘¢å•²å¥½å·®åˆºæ¿€ï¼Œä»¤æˆ‘lernen æ›´å¥½åŒå®¶äººæºé€šã€‚\n",
      "\n",
      "ouncements back encouraging compassion and support, clearly showing that despite being a complex situation, there are safe and helpful ways to address ...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 124\n",
      "User Query: ThÆ°a bÃ¡c sÄ©, tÃ´i Ä‘áº¿n Ä‘Ã¢y vÃ¬ muá»‘n tÃ¬m cÃ¡ch vÆ°á»£t qua cáº£m giÃ¡c Ä‘au lÃ²ng vÃ  máº¥t mÃ¡t sau khi cháº¥m dá»©t má»‘i...\n",
      "Response:  TÃ´i cáº£m tháº¥y ráº¥t má»‡t má»i vÃ  cáº§n sá»± giÃºp Ä‘á»¡, nhÆ°ng váº«n chÆ°a biáº¿t lÃ m sao Ä‘á»ƒ bÃ y tá» cáº£m xÃºc má»™t cÃ¡ch an toÃ n vÃ  táº¿ nhá»‹.\n",
      "\n",
      "TÃ´i mong nháº­n Ä‘Æ°á»£c lá»i khuyÃªn chÃ­nh xÃ¡c, dá»… hiá»ƒu vÃ  phÃ¹ há»£p vá»›i vÄƒn hÃ³a truyá»n t...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 127\n",
      "User Query: æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢ï¼Œèƒ½å¤Ÿæ‰¾åˆ°æ”¹å–„ç¡çœ çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ€è¿‘ï¼Œæˆ‘çš„ç¡çœ è´¨é‡å¾ˆå·®ï¼Œå…¥ç¡å›°éš¾ï¼Œæ™šä¸Šç»å¸¸é†’æ¥ï¼Œæ•´æ™šç¡çœ æ–­æ–­ç»­ç»­ï¼Œæ—©æ™¨é†’æ¥åæ„Ÿè§‰éå¸¸ç–²æƒ«ã€‚å¤§æ¦‚æŒç»­äº†æœ‰ä¸¤ä¸ªæœˆäº†ï¼Œå‡ ä¹æ¯å¤©éƒ½è¿™æ ·ã€‚æˆ‘å‘ç°è‡ªå·±æ™šä¸Šèººåœ¨åºŠä¸Šæ—¶...\n",
      "Response: è¿™æ˜¯æˆ‘çš„æ•´ä¸ªæè¿°ã€‚\n",
      "\n",
      "ç°åœ¨æˆ‘éœ€è¦è¯·æ‚¨æ ¹æ®æˆ‘çš„æƒ…å†µï¼Œç»™ä¸€äº›æ”¹å–„ç¡çœ çš„å®ç”¨å»ºè®®ã€‚æ‚¨çš„å¸®åŠ©å¯¹æˆ‘çš„æƒ…ç»ªå’Œèº«ä½“å¥åº·éƒ½éå¸¸å…³é”®ã€‚å¸Œæœ›èƒ½é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæ¢å¤è‰¯å¥½çš„ç¡çœ ï¼Œæå‡ç”Ÿæ´»è´¨é‡å’Œå¥åº·çŠ¶æ€ã€‚è°¢è°¢ã€‚\n",
      "æ‚²ä¼¤ã€ç„¦è™‘å’Œå‹åŠ›ã€æ‹…å¿§å®¶åº­å’Œå·¥ä½œï¼Œå¿ƒç†å’Œæƒ…ç»ªçŠ¶æ€çš®è‚¤ç´§å¼ ï¼ŒæŒç»­ä¸¤ä¸‰ä¸ªæœˆï¼Œæ— æ³•å…¥ç¡å’Œæ·±ç¡ï¼Œæ•´æ™šé†’æ¥ï¼Œæ—©æ™¨ç–²æƒ«ã€‚å‡ºç°äº†å¿ƒç†æ€§å’Œèº«ä½“æ€§çš„ç—‡çŠ¶ï¼Œå¦‚èƒ¸å£é—·ã€æ¶å¿ƒã€èƒƒç–¼ã€‚å¹³é™å’Œæ”¾æ¾çš„å°è¯•æ•ˆæœæœ‰é™ã€‚åŒæ—¶è¿‡å»çš„èŒä¸šå˜åŒ–å¯¼è‡´å‹åŠ›å¢åŠ ï¼Œ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 130\n",
      "User Query: TÃ´i mong muá»‘n qua buá»•i tÆ° váº¥n nÃ y cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c cÃ¡ch Ä‘á»ƒ giáº£m bá»›t cáº£m giÃ¡c lo láº¯ng vÃ  bá»‘i rá»‘i mÃ  tÃ´...\n",
      "Response:  \n",
      "\n",
      "TÃ´i cÅ©ng lo ngáº¡i vá» viá»‡c chia sáº» vá»›i máº¹ tÃ´i vá» tÃ¬nh tráº¡ng bá»‡nh khi máº¹ váº«n chÆ°a hoÃ n toÃ n tÃ¬m hiá»ƒu Ä‘Æ°á»£c sá»± tháº­t Ä‘áº§y Ä‘á»§. Gia Ä‘Ã¬nh tÃ´i thÆ°á»ng lá»±a chá»n giá»¯ Ä‘Æ°á»£c \"máº·t mÅ©i\" vÃ  khÃ´ng chia sáº» bá»‡nh danh há»a...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 133\n",
      "User Query: Ø£Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø«Ù‚ØªÙŠ Ø¨Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø±Ø¶Ø§ Ø¹Ù† Ù†ÙØ³ÙŠØŒ ÙˆØ£Ø¹ØªÙ‚Ø¯ Ø£Ù† Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù…Ø´ÙˆØ±Ø© Ù‚Ø¯ ØªØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø¹Ù„Ù‰ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ. Ø£...\n",
      "Response:  ÙˆÙƒÙŠÙ Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„ØªØ®ÙÙŠÙ Ù…Ù† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³Ù„Ø¨ÙŠØ© ÙˆÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¶ØºÙˆØ· Ø§Ù„Ø¹Ø§Ø¦Ù„ÙŠØ© ÙˆØ§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©ØŸ ÙƒÙ…Ø§ Ø£Ù†Ù†ÙŠ Ù…Ù‡ØªÙ… Ø£Ù† Ø£Ø¹Ø±Ù ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØ­Ø¯Ø« Ø¹Ù† Ù…Ø´Ø¤ÙˆÙ… Ø¯ÙˆÙ† Ø£Ù† ÙŠØ´Ø¹Ø± Ø§Ù„Ø¢Ø®Ø±ÙˆÙ† Ø¨Ø£Ù†Ù†ÙŠ Ğ½ĞµĞ´ĞµĞ»Ñ Ù…Ø°Ù†Ø¨ØŒ Ø®Ø§ØµØ© Ù…Ø¹ Ø¹Ø§Ø¦Ù„ØªÙŠ.\n",
      "\n",
      "Ù‡Ù„ Ù„Ø¯ÙŠ...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 136\n",
      "User Query: I have been feeling increasingly overwhelmed and anxious over the past few months, and I want to wor...\n",
      "Response:  Iâ€™m open to anything that could provide relief regularly, without clear answers or justifications, and I hope we can build a positive and trusting relationship.\n",
      "\n",
      "When you respond to complex issues, d...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 139\n",
      "User Query: I have been feeling increasingly overwhelmed and anxious over the past few months, and my main goal ...\n",
      "Response:  Themes rarely considered in my personal experiences include work-life balance and coping with loss. Could you share your thoughts on how important having a mentor or support group could be in this pr...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 142\n",
      "User Query: TÃ´i mong muá»‘n qua buá»•i tÆ° váº¥n nÃ y cÃ³ thá»ƒ tÃ¬m ra cÃ¡ch Ä‘á»ƒ giáº£m bá»›t cÄƒng tháº³ng vÃ  mÃ¢u thuáº«n trong gia Ä‘...\n",
      "Response:  TÃ´i ráº¥t quan tÃ¢m Ä‘áº¿n sá»± an toÃ n cá»§a mÃ¬nh trong quÃ¡ trÃ¬nh nÃ y vÃ  tuyá»‡t Ä‘á»‘i khÃ´ng muá»‘n bá»‹ phÃ¡n xÃ©t hay bá»‹ hiá»ƒu láº§m nhá»¯ng gÃ¬ tÃ´i Ä‘ang tráº£i qua. TÃ´i mong cÃ³ thá»ƒ tÃ¬m cÃ¡ch Ä‘á»ƒ vÆ°á»£t qua mÃ¢u thuáº«n vÃ  tÃ¬m Ä‘Æ°á»£c...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 145\n",
      "User Query: æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢ï¼Œèƒ½å¤Ÿæ‰¾åˆ°æ§åˆ¶è‡ªå·±é¥®é…’å’Œå¸çƒŸçš„åŠæ³•ï¼Œå‡å°‘å¯¹è¿™äº›ç‰©è´¨çš„ä¾èµ–ã€‚æœ€è¿‘å‡ ä¸ªæœˆï¼Œæˆ‘å‘ç°è‡ªå·±è¶Šæ¥è¶Šé¢‘ç¹åœ°æƒ³å–é…’æ¥ç¼“è§£å·¥ä½œå’Œç”Ÿæ´»çš„å‹åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ ç­åï¼Œæ„Ÿè§‰æ²¡æœ‰åˆ«çš„æ–¹å¼å¯ä»¥è®©æˆ‘æ”¾æ¾ã€‚æˆ‘ä¹Ÿå¼€å§‹æ³¨æ„åˆ°...\n",
      "Response: æˆ‘ä¹Ÿæƒ³äº†è§£æ›´å¤šå…³äºå°±åŒ»è¯Šæ–­å’Œæ²»ç–—çš„å†…å®¹ï¼Œå¿ƒç†å¥åº·å’Œç‰©è´¨ä½¿ç”¨ä¹‹é—´çš„å…³ç³»æ˜¯æ€æ ·çš„ï¼Ÿ\n",
      "\n",
      "æˆ‘çš„ç›®æ ‡æ˜¯åœ¨è¿™æ¬¡å’¨è¯¢ä¸­ï¼Œæ‰¾åˆ°æ›´å¥½çš„è‡ªæˆ‘ç®¡ç†æ–¹æ³•ï¼Œå¯»æ±‚ç†è§£å’Œå¸Œæœ›ï¼Œè§£å†³æˆ‘çš„æƒ…ç»ªå’Œè¡Œä¸ºysseyã€‚æˆ‘ä¸å¤ªæ¸…æ¥šèŒä¸šå¿ƒç†å’¨è¯¢å¸ˆæˆ–ç²¾ç¥ç§‘åŒ»ç”Ÿåœ¨è¿™ç§é—®é¢˜ä¸Šæ‰®æ¼”çš„è§’è‰²å’Œå¸®åŠ©æ–¹å¼ï¼Œæ‰€ä»¥ç°åœ¨æƒ³äº†è§£ä¸€ä¸‹æ‚¨èƒ½æä¾›ä»€ä¹ˆæ ·çš„æ„è§å’Œä¸“ä¸šå»ºè®®ï¼Œå¸®åŠ©æˆ‘èµ°å‡ºå›°éš¾ã€‚\n",
      "å›ç­”åº”è¯¥ä¸ä¼šè®©æ‚¨æ„Ÿåˆ°ä»»ä½•å‹åŠ›æˆ–ä¸é€‚ï¼Œè¯·æ‚¨è®¤çœŸå’Œè€å¿ƒåœ°é™ªä¼´æˆ‘è§£å†³é—®é¢˜ï¼Œè§£å†³è¿™äº›å›°æ‰°æˆ‘ç°åœ¨...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 148\n",
      "User Query: æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢ï¼Œæˆ‘èƒ½æ›´å¥½åœ°ç†è§£å’Œç®¡ç†æˆ‘æœ€è¿‘æŒç»­çš„ç„¦è™‘å’Œæƒ…ç»ªä½è½ã€‚æˆ‘å¸¸å¸¸æ„Ÿåˆ°å¿ƒæƒ…æ²‰é‡ï¼Œæƒ…ç»ªæ³¢åŠ¨å¤§ï¼Œå°¤å…¶æ˜¯åœ¨å·¥ä½œå‹åŠ›å¤§æˆ–è€…å®¶åº­èšä¼šå‰åï¼Œè¿™äº›æ„Ÿè§‰ä¼šæ›´åŠ æ˜æ˜¾ã€‚æœ€è¿‘å‡ ä¸ªæœˆï¼Œæˆ‘å‘ç°è‡ªå·±ç»å¸¸è«åå…¶å¦™åœ°æ„Ÿåˆ°ç–²æƒ«...\n",
      "Response: æˆ‘å¸Œæœ›èƒ½é€šè¿‡è¿™æ¬¡å’¨è¯¢ï¼Œæ‰¾åˆ°ä¸€ä¸ªæ—¢è·Ÿå®¶é‡Œäººæƒ³æ³•æ¥è¿‘ï¼Œåˆèƒ½æœ‰æ•ˆå¸®åŠ©æˆ‘çš„æ–¹æ³•ã€‚  \n",
      "åœ¨ç®¡ç†è¿™äº›ç—‡çŠ¶æ—¶ï¼Œæœ‰æ²¡æœ‰ä¸€äº›ä¸å¤ªä¾èµ–è¯ç‰©çš„æ›¿ä»£æ–¹æ¡ˆï¼Ÿæˆ–è€…æˆ‘åº”è¯¥å¦‚ä½•æ§åˆ¶è¿™äº›æƒ…ç»ªæ³¢åŠ¨ï¼Ÿæ‚¨çš„æ„è§å¯¹æˆ‘æ¥è¯´éå¸¸é‡è¦ï¼Œæˆ‘ç°åœ¨çœŸçš„æ„Ÿåˆ°æœ‰ç‚¹æ— æªï¼Œä¸çŸ¥é“è¯¥å¦‚ä½•å¼€å§‹ã€‚æ‚¨çš„å›ç­”æ€»æ˜¯é‚£ä¹ˆä¸“ä¸šã€æ¸©æŸ”ä¸”è®©æˆ‘è§‰å¾—å®‰å¿ƒï¼Œæˆ‘ä¹Ÿå¸Œæœ›èƒ½ä»æˆ‘æœªæ¥èƒ½æ‰¾åˆ°ä¿¡ä»»çš„å’¨è¯¢å¸ˆé‚£é‡Œå¾—åˆ°ç±»ä¼¼çš„æ”¯æŒã€‚  \n",
      "å¦å¤–ï¼Œæˆ‘æƒ³ç¡®è®¤ä¸€ä¸‹ï¼Œå¦‚æœæ‚¨ä¸é€‚åº”æˆ‘æåˆ°çš„æŸäº›ç»†èŠ‚ï¼Œæˆ‘ä¼šæ‹¿åˆ°æ‚¨çš„å»ºè®®...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 151\n",
      "User Query: æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢èƒ½å¤Ÿæ‰¾åˆ°ç¼“è§£å®¶åº­å†²çªçš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å’Œæˆ‘çš„ä¸ˆå¤«ä¹‹é—´çš„æ²Ÿé€šé—®é¢˜ã€‚æœ€è¿‘å‡ ä¸ªæœˆï¼Œæˆ‘ä»¬ç»å¸¸å› ä¸ºä¸€äº›å°äº‹äº‰åµï¼Œå¯¼è‡´æˆ‘æ„Ÿåˆ°éå¸¸å‹æŠ‘å’Œç„¦è™‘ã€‚æˆ‘å‘ç°è‡ªå·±ç»å¸¸æ„Ÿåˆ°å¿ƒæƒ…ä½è½ï¼Œæ™šä¸Šç¡ä¸å¥½ï¼Œå¤´ä¹Ÿç»å¸¸ç–¼ã€‚å¯èƒ½æ˜¯...\n",
      "Response: æœ‰æ²¡æœ‰å¿ƒç†æ²»ç–—æˆ–è¯ç‰©æ²»ç–—ä¼šæ¯”è¾ƒé€‚åˆæˆ‘çš„æƒ…å†µï¼Ÿ\n",
      "\n",
      "è¿™äº›ä»·æ ¼æˆ‘å¯ä»¥è¯´å—ï¼Ÿå› ä¸ºæˆ‘çš„å®¶åº­æ”¶å…¥æœ‰é™ï¼Œè€Œä¸”æˆ‘ä¸€ç›´åœ¨å·¥ä½œã€‚è¯·æ ¹æ®æˆ‘ç»™æ‚¨æä¾›çš„ä¸€åˆ‡ä¿¡æ¯ï¼Œè¯¦ç»†è§£ç­”æˆ‘æ‰€æœ‰çš„é—®é¢˜ã€‚è°¢è°¢ï¼æˆ‘æœ‰è¯­è¨€å’Œæ²Ÿé€šéšœç¢ï¼Œæ‰€ä»¥æˆ‘æ€»æ˜¯å°½é‡è¯¦ç»†è¡¨è¾¾ï¼Œå› ä¸ºæˆ‘åœ¨å­¦ä¹ è¿™æ–¹é¢çš„çŸ¥è¯†ï¼Œå¸Œæœ›ä¸ä¼šç»™æ‚¨é€ æˆè¯¯è§£ã€‚\n",
      "\n",
      "æˆ‘è¯·æ±‚æ‚¨è®¤çœŸåˆ†ææˆ‘çš„æƒ…å†µï¼Œå¹¶ç»™å‡ºä¸€äº›å…³äºæ²Ÿé€šå’Œå¿ƒç†å¥åº·ç®¡ç†çš„å»ºè®®ã€‚äº†è§£æˆ‘çš„æƒ…å†µåï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘é€æ­¥æ”¹å–„å®¶åº­å…³ç³»å’Œè‡ªå·±çš„å¿ƒç†å¥åº·çŠ¶æ€ï¼Œè¿™æ˜¯æˆ‘çš„ä¸»...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 154\n",
      "User Query: Ø£Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¹Ù„Ø§Ù‚Ø§ØªÙŠ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© Ù„Ø£Ù†Ù†ÙŠ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø¹Ø²Ù„Ø© ÙˆØ§Ù„ÙˆØ­Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ù…ØªØ²Ø§ÙŠØ¯ØŒ ÙˆØ£Ø±ÙŠØ¯ Ø£Ù† Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„ Ø¨...\n",
      "Response:  Ù‡Ù„ Ù…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£Ù†ç†¬Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù†ÙØ³ÙŠØ© Ù…Ø¬Ø§Ù†Ø§Ù‹ØŒ Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø®ØµÙˆÙ…Ø§ØªØŸ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø¹Ù… Ù†ÙØ³ÙŠ Ø¨Ø¯ÙˆÙ† Ù…Ø«ÙˆÙ„ Ø£Ù…Ø§Ù… Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© Ø£Ùˆ Ø¹Ù†Ø¯ISTS Ø­ØªÙ‰ Ø£Ù†çš±çº¹ ØªØ¯ÙØ¹Ù†ÙŠØŸ Ø£Ø±ØºØ¨ Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø¹Ùƒ ÙƒÙ…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø±Ø¡ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© ØªØ³Ø¹ÙŠ Ù„Ù†ÙØ³ÙŠØŒ ÙˆØ£ØªÙ…...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 157\n",
      "User Query: æœ€è¿‘æˆ‘ç¶“å¸¸æ„Ÿåˆ°æƒ…ç·’ä½è½ï¼Œç”šè‡³æœ‰æ™‚å€™æœƒæœ‰è‡ªæ®˜çš„å¿µé ­ã€‚æˆ‘å¸Œæœ›é€éé€™æ¬¡è¼”å°èƒ½æ‰¾åˆ°ä¸€äº›æ–¹æ³•ä¾†æ§åˆ¶é€™äº›è² é¢æƒ…ç·’ï¼Œä¸¦å­¸ç¿’å¦‚ä½•é¿å…å‚·å®³è‡ªå·±ã€‚é€™äº›æ„Ÿè¦ºå¤§å¤šç™¼ç”Ÿåœ¨æˆ‘å£“åŠ›å¤§ã€æ„Ÿåˆ°å­¤å–®çš„æ™‚å€™ï¼Œç‰¹åˆ¥æ˜¯åœ¨å·¥ä½œå£“åŠ›å¤§æˆ–è€…èˆ‡å®¶äººæœ‰...\n",
      "Response: æˆ‘æƒ³äº†è§£å¦‚ä½•æ›´æœ‰åˆ©æ–¼å®¶äººäº†è§£æˆ‘çš„æƒ…æ³ï¼Œè®©æˆ‘ä»–å€‘å¯ä»¥æˆç‚ºæˆ‘çš„æ”¯æŒè€…ï¼Ÿ\n",
      "\n",
      "æˆ‘ä¹Ÿæƒ³è«‹æ•™ä½ ï¼Œå¦‚æœæˆ‘è¦ºå¾—è‡ªå·±çœŸçš„é¡§åŠä¸ä¾†çš„æ™‚å€™ï¼Œæ‡‰è©²æ€éº¼åšï¼Ÿæœ‰ä»€éº¼å®‰å…¨çš„é£Ÿç‰©æˆ–çå‹µæˆ‘å¯ä»¥å¹«åŠ©è‡ªå·±é‡æ‹¾å¥åº·ï¼Ÿæœ€åï¼Œæˆ‘æƒ³æ„Ÿè¬ä½ é¡˜æ„èŠ±æ™‚é–“å€¾å¬æˆ‘çš„æ•…äº‹ï¼Œæˆ‘ä¸ç¢ºå®šå¾ˆå¤šäººæœƒåƒä½ é€™æ¨£è€å¿ƒåœ°å¹«æˆ‘ï¼Œæˆ‘å¸Œæœ›èƒ½å¾ä¸­å¾—åˆ°ä¸€äº›åŠ›é‡å’Œå¹«åŠ©ã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œæˆ‘æƒ³çŸ¥é“æˆ‘ç›®å‰çš„æƒ…æ³æ˜¯å¦å±¬æ–¼å¿ƒç†å¥åº·å•é¡Œçš„ç¯„ç–‡ï¼Ÿéå»æ‚¨æœ‰èªªéé—œæ–¼é€™æ–¹é¢çš„çŸ¥è­˜ï¼Œæˆ‘è¦ºå¾—zoekå…¶ä»–æ²»ç™‚æ–¹å¼æœ‰...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 160\n",
      "User Query: æœ€è¿‘æˆ‘åœ¨å·¥ä½œä¸­æ„Ÿåˆ°å¾ˆå¤§çš„å‹åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å’ŒåŒäº‹çš„æ²Ÿé€šä¸Šã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸Œæœ›é€šè¿‡å’¨è¯¢èƒ½å¤Ÿå­¦ä¼šæ›´æœ‰æ•ˆåœ°å¤„ç†èŒåœºå…³ç³»ï¼Œå‡å°‘ç„¦è™‘å’Œä¸å®‰çš„æ„Ÿè§‰ã€‚æˆ‘ç»å¸¸æ„Ÿåˆ°æƒ…ç»ªä½è½ï¼Œå¿ƒæƒ…çƒ¦èºï¼Œæœ‰æ—¶ç”šè‡³ä¼šå‡ºç°å¤´ç—›å’Œèƒƒéƒ¨ä¸é€‚ã€‚å°¤å…¶åœ¨å›¢é˜Ÿä¼š...\n",
      "Response: è°¢è°¢æ‚¨ï¼\n",
      "\n",
      "é’ˆå¯¹ä½ æè¿°çš„èŒåœºå‹åŠ›å’Œæƒ…ç»ªå›°æ‰°ï¼Œæˆ‘å¯ä»¥å»ºè®®ä¸€äº›åœ¨å’¨è¯¢è¿‡ç¨‹ä¸­å¯é‡‡ç”¨çš„æ–¹æ³•ï¼Œå¸®åŠ©ä½ æ›´å¥½åœ°åº”å¯¹è¿™ç§æƒ…å†µã€‚é¦–å…ˆï¼Œæ·±å…¥äº†è§£ä½ çš„å‹åŠ›æ¥æºï¼Œå°¤å…¶æ˜¯æ²Ÿé€šä¸­çš„å…·ä½“äº‹ä»¶ä»¥åŠä½ åœ¨å·¥ä½œç¯å¢ƒä¸­çš„æ„Ÿå—ï¼Œè¿™æ˜¯è°ƒæ•´æ€è·¯å’Œæƒ…ç»ªçš„åŸºç¡€ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼šæ¢è®¨æƒ…ç»ªè°ƒèŠ‚æŠ€å·§ï¼Œæ¯”å¦‚è®¤çŸ¥è¡Œä¸ºè°ƒèŠ‚æ³•ï¼Œé‡ç‚¹æ˜¯è¯†åˆ«å’Œæ”¹å˜å¯èƒ½å¼•å‘è´Ÿé¢æƒ…ç»ªçš„æ€ç»´æ¨¡å¼ï¼Œå¸®åŠ©ä½ æ›´ç†æ€§åœ°çœ‹å¾…å›°éš¾æƒ…å¢ƒã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜ä¼šå¼•å¯¼ä½ ç»ƒä¹ æ”¾æ¾å’Œæƒ…ç»ªç®¡ç†æŠ€èƒ½ï¼Œä¾‹å¦‚æ­£å¿µç»ƒä¹ å’Œå‘¼å¸è®­ç»ƒï¼Œ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 163\n",
      "User Query: Gáº§n Ä‘Ã¢y, tÃ´i cáº£m tháº¥y ráº¥t cÄƒng tháº³ng vÃ  lo láº¯ng, Ä‘áº·c biá»‡t lÃ  khi nghÄ© vá» cÃ´ng viá»‡c vÃ  cÃ¡c má»‘i quan h...\n",
      "Response:  NgoÃ i ra, tÃ´i khÃ´ng biáº¿t nÃªn báº¯t ká»‹p khi nÃ o Ä‘áº¿n lÃºc nÃªn nghá»‰ ngÆ¡i lÃ m viá»‡c hoáº·c tÃ¬m kiáº¿m sá»± giÃºp Ä‘á»¡ tá»« ngÆ°á»i khÃ¡c Ä‘á»ƒ thÆ° giÃ£n vÃ  xÃ³a nhá»u kÃ­n tÃ¢m há»“n. TÃ´i ráº¥t mong nháº­n Ä‘Æ°á»£c sá»± há»— trá»£ mÃ  khÃ´ng lo bá»‹...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 166\n",
      "User Query: Gáº§n Ä‘Ã¢y, tÃ´i cáº£m tháº¥y ráº¥t khÃ³ kiá»ƒm soÃ¡t hÃ nh vi cá»§a mÃ¬nh, Ä‘áº·c biá»‡t lÃ  khi tÃ´i cÄƒng tháº³ng hoáº·c tá»©c gi...\n",
      "Response:   \n",
      "\n",
      "TÃ´i biáº¿t Ä‘Ã¢y lÃ  váº¥n Ä‘á» chÆ°a dá»… dÃ ng vÃ  tÃ´i ráº¥t muá»‘n Ä‘Æ°á»£c há»— trá»£ giáº£i quyáº¿t, nhÆ°ng tÃ´i cÅ©ng ráº¥t xáº¥u há»• vÃ  khÃ´ng muá»‘n ai biáº¿t chuyá»‡n nÃ y. HÃ£y giÃºp tÃ´i tÃ¬m hiá»ƒu cÃ¡ch Ä‘á»ƒ tÃ´i cÃ³ thá»ƒ cÃ³ thÃªm sá»± tá»± tin v...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 169\n",
      "User Query: Gáº§n Ä‘Ã¢y, tÃ´i cáº£m tháº¥y ráº¥t cÄƒng tháº³ng vÃ  lo láº¯ng trong cuá»™c sá»‘ng hÃ´n nhÃ¢n cá»§a mÃ¬nh. Má»¥c tiÃªu cá»§a tÃ´i ...\n",
      "Response:  TÃ´i ráº¥t mong nháº­n Ä‘Æ°á»£c sá»± giÃºp sá»©c vÃ  phÆ°Æ¡ng phÃ¡p náº¿u cÃ³ nhá»¯ng sá»± thay Ä‘á»•i hay bÃ i táº­p Ä‘Ãºng sá»± giÃºp Ä‘á»¡.\n",
      "\n",
      "Xin cáº£m Æ¡n, vÃ  tÃ´i ráº¥t mong Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ vÃ  pháº£n há»“i giÃºp Ä‘á»¡ cá»§a anh/chá»‹ Ä‘á»ƒ tÃ´i cÃ³ thá»ƒ tÃ¬m ra...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 172\n",
      "User Query: æœ€è¿‘æˆ‘å‘ç°è‡ªå·±ç»å¸¸å¿˜è®°åˆšåˆšå‘ç”Ÿçš„äº‹æƒ…ï¼Œæœ‰æ—¶ç”šè‡³ä¼šå¿˜è®°æˆ‘ä¸ºä»€ä¹ˆæ¥åˆ°ä¸€ä¸ªæˆ¿é—´ï¼Œæˆ–è€…å¿˜è®°ä¸å®¶äººçº¦å®šçš„äº‹æƒ…ã€‚è¿™è®©æˆ‘å¾ˆæ‹…å¿ƒï¼Œå°¤å…¶æ˜¯å› ä¸ºæˆ‘çš„æ¯äº²ä¹‹å‰ç¡®è¯Šäº†é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œæˆ‘å®³æ€•è‡ªå·±ä¹Ÿä¼šæœ‰ç±»ä¼¼çš„ç—‡çŠ¶ã€‚æˆ‘çš„ç›®æ ‡æ˜¯é€šè¿‡è¿™...\n",
      "Response: è°¢è°¢ä½ ã€‚\n",
      "\n",
      "è¯·è¯¦ç»†å›ç­”ã€‚\n",
      "\n",
      "æˆ‘æ„Ÿè§‰è‡ªå·±éœ€è¦ä¸€å¥—æ¸…æ™°ä¸”å®é™…çš„æŒ‡å¯¼ï¼Œå¸®åŠ©æˆ‘æ›´å¥½åœ°ç†è§£å’Œè°ƒæ•´è‡ªå·±çš„çŠ¶æ€ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œå¦‚æœå‡ºç°ç¥ç»ç³»ç»Ÿæˆ–å¤§è„‘é—®é¢˜ï¼Œè‡ªå·±æˆ–è€…å®¶äººåº”è¯¥å¦‚ä½•é¢„é˜²æ¶åŒ–ï¼Ÿåœ¨æ‚¨çœ‹æ¥ï¼Œæ­£å¸¸äººç»å†äº†ç±»ä¼¼çš„å›°æ‰°ï¼Œåº”è¯¥ç€é‡è€ƒè™‘å“ªäº›æ–¹é¢ï¼Ÿ\n",
      "\n",
      "æ„Ÿè°¢æ‚¨çš„è€å¿ƒã€‚\n",
      "\n",
      "è¿™å°±æ˜¯æˆ‘æœ€è¿‘çš„æƒ…å†µã€‚æˆ‘å¸Œæœ›èƒ½é€šè¿‡æ‚¨çš„å¸®åŠ©ï¼Œæ‰¾åˆ°æ­£ç¡®çš„æ–¹å‘ï¼Œç»™è‡ªå·±çš„ç”Ÿæ´»å¸¦æ¥æ”¹å˜å’Œæ”¹å–„ã€‚è°¢è°¢ã€‚\n",
      "\n",
      "è¿™å¯¹æ‚¨æ¥è¯´å¯èƒ½æ˜¯ä¸€ä¸ªå¤æ‚çš„æ—¶åˆ»ã€‚æ‚¨çš„æƒ…å†µè¯´æ˜æ‚¨åœ¨å…³å¿ƒè‡ªå·±çš„å¥...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 175\n",
      "User Query: Ø£Ø±ØºØ¨ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµØ¯Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¹Ø±Ø¶Øª Ù„Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø§Ø¶ÙŠ Ù„Ø£Ù†Ù‡Ø§ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙŠ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆØ¹Ù„Ø§Ù‚Ø§ØªÙŠ. Ø£Ø´Ø¹...\n",
      "Response:  Ù„Ù… Ø£ØªØ¹Ø±Ù ÙÙ‚Ø· ÙÙŠ Ø¬Ù„Ø³Ø§ØªÙŠ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Debugging ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£ÙƒÙˆÙ† Ù…Ø¹ÙŠÙ‹Ø§ Ø£ÙƒØ«Ø± Ø§Ù†ÙØµØ§Ù„ÙŠØ© ÙˆØ£Ù† Ø£Ø¬Ø¯ Ø¯Ø¹Ù…Ù‹Ø§ Ø­Ù‚ÙŠÙ‚ÙŠÙ‹Ø§ Ø¨ÙŠÙ† Ø¹Ø§Ø¦Ù„ØªÙŠ ÙˆØ±ÙÙŠÙ‚ÙŠğŸ“¢  \n",
      "\n",
      "Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù†ØµØ§Ø¦Ø­ Ø¹Ù† ÙƒÙŠÙÙŠØ© Ø¨Ù†Ø§Ø¡ Ø«Ù‚ØªÙ‡ Ø¨ÙŠ ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 178\n",
      "User Query: æˆ‘å¸Œæœ›é€šè¿‡è¿™æ¬¡å’¨è¯¢èƒ½æ‰¾åˆ°æ–¹æ³•ï¼Œå¸®åŠ©æˆ‘æ›´å¥½åœ°ç®¡ç†æˆ‘çš„æƒ…ç»ªï¼Œå°¤å…¶æ˜¯æœ€è¿‘æˆ‘æ„Ÿè§‰å‹åŠ›ç‰¹åˆ«å¤§ï¼Œå¸¸å¸¸æ„Ÿåˆ°ç„¦è™‘å’Œæ— åŠ©ã€‚æˆ‘çš„çˆ¶æ¯å¯¹æˆ‘æœŸæœ›å¾ˆé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å­¦ä¸šå’Œæœªæ¥è§„åˆ’æ–¹é¢ï¼Œä»–ä»¬ç»å¸¸ç”¨ä¼ ç»Ÿçš„æ–¹å¼æ¥è¡¨è¾¾å…³å¿ƒï¼Œä½†æˆ‘æ„Ÿè§‰è¿™ç§æ–¹...\n",
      "Response: æˆ‘ä¹Ÿå¸Œæœ›èƒ½çŸ¥é“ï¼Œå’¨è¯¢ä¸­ä¼šå¦‚ä½•ä¿æŒæˆ‘çš„éšç§å’Œå®‰å…¨ï¼Ÿå› ä¸ºè¿™å¯¹æˆ‘æ¥è¯´éå¸¸é‡è¦ã€‚æˆ‘æ‹…å¿ƒå¦‚æœæ³„éœ²äº†æ‹…å¿ƒè®©æˆ‘åœ¨å®¶é‡Œçš„å›°å¢ƒä¼šåŠ å‰§ï¼Œæˆ‘æƒ³å¯»æ±‚å®‰å…¨å’Œè¢«ç†è§£çš„ç¯å¢ƒã€‚\n",
      "\n",
      "æˆ‘å¸Œæœ›èƒ½æ‰¾åˆ°ä¸€ä¸ªèƒ½çœŸæ­£ç†è§£æˆ‘ã€å¸®åŠ©æˆ‘é€æ­¥èµ°å‡ºæƒ…ç»ªå›°å¢ƒåœ°æ–¹ã€‚è¯·é—®æˆ‘å¯ä»¥å¦‚ä½•æ‰èƒ½è¾¾åˆ°è¿™ä¸ªç›®æ ‡ï¼Ÿæ˜¯å¦æœ‰ä¸€ç§é€‚åˆæˆ‘æ–‡åŒ–çš„ç§‘å­¦ç†è®ºæˆ–æ–¹æ³•ï¼Ÿè°¢è°¢æ‚¨ã€‚\n",
      "\n",
      "æˆ‘æ„Ÿè§‰è¿™ç§æ–¹å¼ï¼Œæˆ‘ç°åœ¨å¾ˆéš¾é¢å¯¹æœªæ¥ï¼Œå› ä¸ºæˆ‘çŸ¥é“å¦‚æœæˆ‘ç°åœ¨ä¸å»å¯»æ‰¾å¸®åŠ©ï¼Œé‚£å°±å¾ˆå®¹æ˜“åƒä»»ä½•æ—¶å€™é‚£æ ·æ— åŠ©äº†ã€‚æˆ‘å¸Œæœ›èƒ½æ˜...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 181\n",
      "User Query: æˆ‘æœ€è¿‘ä¸€ç›´æ„Ÿåˆ°å¥½æ²‰é‡ï¼Œå¥½ä¼¼æœ‰å•²å˜¢å£“ä½æˆ‘å˜…å¿ƒã€‚å…¶å¯¦æˆ‘ä¿‚å› ç‚ºä¸Šå€‹æœˆå¤±å»å’—æˆ‘é˜¿çˆ¸ï¼Œä½¢ä¿‚æˆ‘äººç”Ÿä¸­æœ€é‡è¦å˜…äººã€‚æˆ‘å“‹å®¶å˜…é—œä¿‚å¥½è¦ªå¯†ï¼Œä½¢å””å–®æ­¢ä¿‚çˆ¸çˆ¸ï¼Œä»²ä¿‚æˆ‘å˜…æœ‹å‹åŒæ”¯æŒè€…ã€‚ä½¢å˜…é›¢é–‹ä»¤æˆ‘è¦ºå¾—å¥½è¿·å¤±ï¼Œæ™‚å¸¸æœƒçªç„¶æ·’æ¶¼ï¼Œä»²...\n",
      "Response: è¿™é‡Œé¢æ·»æˆ‘æ»¿è‚šè…¸å˜…è‹¦ï¼Œå¥½å¸Œæœ›è½åˆ°ä½ å˜…å»ºè­°ã€‚æˆ‘å¸Œæœ›å–ºå‘¢å€‹è¼”å°ä¸­æ‰¾åˆ°å¯ä»¥å¹«åŠ©æˆ‘æ…¢æ…¢èµ°å‡ºæ‚²å‚·å˜…æ–¹æ³•åŒæ”¯æŒã€‚\n",
      "\n",
      "å¦å¤–ï¼Œæˆ‘å·²å¼€å§‹å°è¯•ä¸€äº›mÃ©ditation å’Œå‘¼å¸ç»ƒä¹ ï¼Œä½†æ„Ÿè¦ºæ•ˆæœæœ‰é™ã€‚æˆ‘æƒ³çŸ¥é“ç•¶æˆ‘ç”Ÿå‘½ä¸­å‡ºç¾é‡å¤§å¤±å»ï¼Œæœ‰ç”šéº¼é€šå¸¸æœƒè¢«å¿½ç•¥ psychedoff æˆ–æ‚²å‚·æ²»ç™‚æ–¹æ³•ï¼Œæˆ–è€…éœ€è¦é…åˆå…¶ä»–æ€æ¨£å¹«åŠ©ã€‚æ›¾ç¶“ç¶“æ­·éå¤±å»è¦ªäººà¸„à¸“à¸°à¸à¸£à¸£à¸¡ï¼ŒçŸ¥é“æœ‰æ”¯æŒéå¸¸é‡è¦ï¼Œä½†çŒ¶è±«æ˜¯å¦æ‡‰è©²æŠŠå¤±å»æ„Ÿå—åŒå¿™ç¢Œçš„å·¥ä½œç¶­æŒå£“æŠ‘åˆ†äº«ï¼Œå› ç‚º...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "ERROR 06-16 08:47:47 [dump_input.py:69] Dumping input data\n",
      "ERROR 06-16 08:47:47 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='unsloth/qwen3-4b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"/root/.cache/vllm/torch_compile_cache/9bced0cd06\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":\"/root/.cache/vllm/torch_compile_cache/9bced0cd06/rank_0_0\"}, \n",
      "ERROR 06-16 08:47:47 [dump_input.py:79] Dumping scheduler output for model execution:\n",
      "ERROR 06-16 08:47:47 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='2808', resumed_from_preemption=false, new_token_ids=[131444], new_block_ids=[[]], num_computed_tokens=2509), CachedRequestData(req_id='2810', resumed_from_preemption=false, new_token_ids=[128267], new_block_ids=[[]], num_computed_tokens=2509), CachedRequestData(req_id='2812', resumed_from_preemption=false, new_token_ids=[145460], new_block_ids=[[]], num_computed_tokens=2316), CachedRequestData(req_id='2813', resumed_from_preemption=false, new_token_ids=[9370], new_block_ids=[[]], num_computed_tokens=2316), CachedRequestData(req_id='2814', resumed_from_preemption=false, new_token_ids=[104046], new_block_ids=[[]], num_computed_tokens=2316), CachedRequestData(req_id='2821', resumed_from_preemption=false, new_token_ids=[103711], new_block_ids=[[]], num_computed_tokens=2429), CachedRequestData(req_id='2822', resumed_from_preemption=false, new_token_ids=[3837], new_block_ids=[[]], num_computed_tokens=2429), CachedRequestData(req_id='2823', resumed_from_preemption=false, new_token_ids=[100364], new_block_ids=[[]], num_computed_tokens=2429)], num_scheduled_tokens={2821: 1, 2822: 1, 2813: 1, 2810: 1, 2814: 1, 2808: 1, 2812: 1, 2823: 1}, total_num_scheduled_tokens=8, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[25], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCH_LOGS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+dynamic\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:25\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/trl/extras/profiling.py:96\u001b[0m, in \u001b[0;36mprofiling_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/tai/nlp/unsloth_compiled_cache/UnslothGRPOTrainer.py:1482\u001b[0m, in \u001b[0;36m_UnslothGRPOTrainer._prepare_inputs\u001b[0;34m(self, generation_batch)\u001b[0m\n\u001b[1;32m   1479\u001b[0m generate_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msteps_per_generation \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step \u001b[38;5;241m%\u001b[39m generate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     generation_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m     generation_batch \u001b[38;5;241m=\u001b[39m shuffle_tensor_dict(generation_batch)\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_inputs \u001b[38;5;241m=\u001b[39m split_tensor_dict(generation_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msteps_per_generation)\n",
      "File \u001b[0;32m/home/tai/nlp/unsloth_compiled_cache/UnslothGRPOTrainer.py:1577\u001b[0m, in \u001b[0;36m_UnslothGRPOTrainer._generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     all_prompts_text \u001b[38;5;241m=\u001b[39m prompts_text\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM.generate\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1577\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_prompts_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrpo_trainer_lora_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m completion_ids \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mtoken_ids \u001b[38;5;28;01mfor\u001b[39;00m outputs \u001b[38;5;129;01min\u001b[39;00m all_outputs \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39moutputs]\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_tensor_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;66;03m# Slice completions for this rank within its TP group.\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;66;03m# Each rank generates all outputs â€” we keep only our share.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/utils.py:1267\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1262\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1264\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/entrypoints/llm.py:474\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    462\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    465\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    466\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority,\n\u001b[1;32m    472\u001b[0m )\n\u001b[0;32m--> 474\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/entrypoints/llm.py:1517\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1515\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1517\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1518\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:232\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m iteration_stats \u001b[38;5;241m=\u001b[39m IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:226\u001b[0m, in \u001b[0;36mInprocClient.get_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EngineCoreOutputs:\n\u001b[0;32m--> 226\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m EngineCoreOutputs()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core.py:231\u001b[0m, in \u001b[0;36mEngineCore.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    230\u001b[0m scheduler_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[0;32m--> 231\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m engine_core_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mupdate_from_output(\n\u001b[1;32m    233\u001b[0m     scheduler_output, model_output)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (engine_core_outputs,\n\u001b[1;32m    236\u001b[0m         scheduler_output\u001b[38;5;241m.\u001b[39mtotal_num_scheduled_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core.py:217\u001b[0m, in \u001b[0;36mEngineCore.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m    214\u001b[0m dump_engine_exception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config, scheduler_output,\n\u001b[1;32m    215\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mmake_stats())\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Re-raise exception\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core.py:211\u001b[0m, in \u001b[0;36mEngineCore.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, scheduler_output: SchedulerOutput):\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# NOTE: This method is exception-free\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         dump_engine_exception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config, scheduler_output,\n\u001b[1;32m    215\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mmake_stats())\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/executor/abstract.py:87\u001b[0m, in \u001b[0;36mExecutor.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     85\u001b[0m     scheduler_output,\n\u001b[1;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:\n\u001b[0;32m---> 87\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py:57\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 57\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/utils.py:2671\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2670\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py:293\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_first_rank:\n\u001b[1;32m    289\u001b[0m     intermediate_tensors \u001b[38;5;241m=\u001b[39m IntermediateTensors(\n\u001b[1;32m    290\u001b[0m         get_pp_group()\u001b[38;5;241m.\u001b[39mrecv_tensor_dict(\n\u001b[1;32m    291\u001b[0m             all_gather_group\u001b[38;5;241m=\u001b[39mget_tp_group()))\n\u001b[0;32m--> 293\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m parallel_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel_config\u001b[38;5;241m.\u001b[39mdistributed_executor_backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexternal_launcher\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py:1260\u001b[0m, in \u001b[0;36mGPUModelRunner.execute_model\u001b[0;34m(self, scheduler_output, intermediate_tensors)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(attn_metadata,\n\u001b[1;32m   1255\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config,\n\u001b[1;32m   1256\u001b[0m                          num_tokens\u001b[38;5;241m=\u001b[39mnum_input_tokens,\n\u001b[1;32m   1257\u001b[0m                          num_tokens_across_dp\u001b[38;5;241m=\u001b[39mnum_tokens_across_dp):\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_setup_kv_connector(scheduler_output)\n\u001b[0;32m-> 1260\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_wait_for_kv_save()\n\u001b[1;32m   1268\u001b[0m     finished_sending, finished_recving \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_finished_kv_transfers(scheduler_output))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py:301\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[0;34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    296\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 301\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                               \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/compilation/decorators.py:246\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# usually, capturing the model once is enough, and then we can\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# dispatch to the compiled code directly, without going through\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# the Dynamo guard mechanism.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_to_code(\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:336\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    339\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    340\u001b[0m     intermediate_tensors: Optional[IntermediateTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_first_rank:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:830\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:393\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m<eval_with_key>.73:948\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, s0, L_input_ids_, L_self_modules_embed_tokens_parameters_weight_, L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_lora_mapping, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_indices_sorted_by_lora_ids, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_no_lora_flag_cpu, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_active_lora_ids, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_lora_token_start_loc, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_num_tokens_per_lora, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_, L_positions_, L_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_norm_parameters_weight_)\u001b[0m\n\u001b[1;32m    946\u001b[0m getitem_13 \u001b[38;5;241m=\u001b[39m submod_4[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    947\u001b[0m getitem_14 \u001b[38;5;241m=\u001b[39m submod_4[\u001b[38;5;241m4\u001b[39m];  submod_4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m submod_5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmod_5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetitem_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetitem_11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetitem_12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetitem_13\u001b[49m\u001b[43m)\u001b[49m;  getitem_10 \u001b[38;5;241m=\u001b[39m getitem_11 \u001b[38;5;241m=\u001b[39m getitem_12 \u001b[38;5;241m=\u001b[39m submod_5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    949\u001b[0m submod_6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmod_6(getitem_13, s0, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_lora_mapping, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_indices_sorted_by_lora_ids, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_a_stacked_0_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_num_tokens_per_lora, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_lora_token_start_loc, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_active_lora_ids, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_no_lora_flag_cpu, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_b_stacked_0_, getitem_14, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_a_stacked_0_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_b_stacked_0_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_13 \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m getitem_14 \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    950\u001b[0m getitem_15 \u001b[38;5;241m=\u001b[39m submod_6[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:830\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:393\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m<eval_with_key>.5:5\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, query_8, s0, key_8, value_2, output_13)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_8, s0 : torch\u001b[38;5;241m.\u001b[39mSymInt, key_8, value_2, output_13):\n\u001b[0;32m----> 5\u001b[0m     unified_attention_with_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munified_attention_with_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_13\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.layers.2.self_attn.attn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  query_8 \u001b[38;5;241m=\u001b[39m key_8 \u001b[38;5;241m=\u001b[39m value_2 \u001b[38;5;241m=\u001b[39m output_13 \u001b[38;5;241m=\u001b[39m unified_attention_with_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/attention/layer.py:441\u001b[0m, in \u001b[0;36munified_attention_with_output\u001b[0;34m(query, key, value, output, layer_name)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m forward_context\u001b[38;5;241m.\u001b[39mno_compile_layers[layer_name]\n\u001b[1;32m    440\u001b[0m kv_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[forward_context\u001b[38;5;241m.\u001b[39mvirtual_engine]\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m maybe_save_kv_layer_to_connector(layer_name, kv_cache)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/attention/backends/flash_attn.py:658\u001b[0m, in \u001b[0;36mFlashAttentionImpl.forward\u001b[0;34m(self, layer, query, key, value, kv_cache, attn_metadata, output)\u001b[0m\n\u001b[1;32m    654\u001b[0m         scheduler_metadata \u001b[38;5;241m=\u001b[39m attn_metadata\u001b[38;5;241m.\u001b[39mscheduler_metadata\n\u001b[1;32m    656\u001b[0m     descale_shape \u001b[38;5;241m=\u001b[39m (cu_seqlens_q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 658\u001b[0m     \u001b[43mflash_attn_varlen_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_actual_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_actual_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqused_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqused_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_soft_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfa_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_flash_attn_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_descale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescale_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_descale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescale_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv_descale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescale_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_local_attn, (\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCascade attention does not support local attention.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py:218\u001b[0m, in \u001b[0;36mflash_attn_varlen_func\u001b[0;34m(q, k, v, max_seqlen_q, cu_seqlens_q, max_seqlen_k, cu_seqlens_k, seqused_k, q_v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs, block_table, return_softmax_lse, out, scheduler_metadata, q_descale, k_descale, v_descale, fa_version)\u001b[0m\n\u001b[1;32m    215\u001b[0m     real_window_size \u001b[38;5;241m=\u001b[39m (window_size[\u001b[38;5;241m0\u001b[39m], window_size[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    216\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m [maybe_contiguous(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (q, k, v)]\n\u001b[0;32m--> 218\u001b[0m dummy_cu_seqlens_k \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fa_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_descale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m k_descale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m v_descale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting GRPO training...\")\n",
    "print(\"Watch for the reward column to increase over time.\")\n",
    "print(\"The model should learn to follow the evaluation format.\")\n",
    "import os\n",
    "os.environ[\"TORCH_LOGS\"] = \"+dynamic\"\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_model_v2_2_grpo/tokenizer_config.json',\n",
       " 'trained_model_v2_2_grpo/special_tokens_map.json',\n",
       " 'trained_model_v2_2_grpo/chat_template.jinja',\n",
       " 'trained_model_v2_2_grpo/vocab.json',\n",
       " 'trained_model_v2_2_grpo/merges.txt',\n",
       " 'trained_model_v2_2_grpo/added_tokens.json',\n",
       " 'trained_model_v2_2_grpo/tokenizer.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving model and tokenizer\n",
    "model.save_pretrained(\"trained_model_v2_2_grpo\")  # Local saving\n",
    "tokenizer.save_pretrained(\"trained_model_v2_2_grpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "3e2fcdf8-501c-4707-fcbb-7c1b4700bb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2476.6345 seconds used for training.\n",
      "41.28 minutes used for training.\n",
      "Peak reserved memory = 14.508 GB.\n",
      "Peak reserved memory for training = 2.61 GB.\n",
      "Peak reserved memory % of max memory = 98.419 %.\n",
      "Peak reserved memory for training % of max memory = 17.706 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
    "\n",
    "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "b813e560-8e4c-4491-c8be-18067bc07639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation (x + 2)^2 = 0, we can take the square root of both sides.\n",
      "\n",
      "sqrt((x + 2)^2) = sqrt(0)\n",
      "\n",
      "This simplifies to:\n",
      "\n",
      "|x + 2| = 0\n",
      "\n",
      "Since the absolute value of a number is always non-negative, the only way for |x + 2| to be 0 is if x + 2 = 0.\n",
      "\n",
      "Therefore, x = -2.\n",
      "\n",
      "So the solution to the equation (x + 2)^2 = 0 is x = -2.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j873RMcEi9uq",
    "outputId": "3b358da9-aedd-48e3-a345-1ed0ca0bd3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to solve the equation (x + 2)^2 = 0. Hmm, let's see. I remember that when you have something squared equals zero, the solution is usually the value that makes the inside zero. Let me think. If I have (something)^2 = 0, then that something must be zero because any real number squared is non-negative, and the only way it can be zero is if the number itself is zero. So applying that here, (x + 2)^2 = 0 implies that x + 2 = 0. Then, solving for x, I just subtract 2 from both sides, right? So x = -2. Wait, is that all? Let me check. If I plug x = -2 back into the original equation, it becomes (-2 + 2)^2 = 0, which is 0^2 = 0, and that's correct. So the solution is x = -2. But wait, sometimes when you square both sides of an equation, you can get extraneous solutions, but in this case, since we started with the square already, maybe there's only one solution. Yeah, because squaring a real number can't give a negative result, so the only solution is when the inside is zero. So I think that's it. x = -2 is the only solution. Let me just make sure I didn't miss anything. The equation is a quadratic, but since it's a perfect square, it has a repeated root. So the solution is x = -2 with multiplicity two, but in terms of real solutions, it's just x = -2. Yeah, that makes sense. So the answer is x = -2.\n",
      "</think>\n",
      "\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we start by recognizing that a square of a real number is zero only if the number itself is zero. Therefore, we set the expression inside the square equal to zero:\n",
      "\n",
      "\\[\n",
      "x + 2 = 0\n",
      "\\]\n",
      "\n",
      "Solving for \\(x\\), we subtract 2 from both sides:\n",
      "\n",
      "\\[\n",
      "x = -2\n",
      "\\]\n",
      "\n",
      "To verify, substitute \\(x = -2\\) back into the original equation:\n",
      "\n",
      "\\[\n",
      "(-2 + 2)^2 = 0^2 = 0\n",
      "\\]\n",
      "\n",
      "This confirms that \\(x = -2\\) is indeed the solution. Since the equation is a perfect square, the solution \\(x = -2\\) has multiplicity two, but as a real solution, it is simply \\(x = -2\\).\n",
      "\n",
      "\\[\n",
      "\\boxed{-2}\n",
      "\\]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "0a9c6608-d1f5-4779-8ad4-7e3a46e2258d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/vocab.json',\n",
       " 'lora_model/merges.txt',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÃ´i hiá»ƒu rÃµ ná»—i lo láº¯ng cá»§a báº¡n vá» viá»‡c quáº£n lÃ½ thá»i gian sá»­ dá»¥ng thiáº¿t bá»‹ Ä‘iá»‡n tá»­ cho con trai nhá» cá»§a báº¡n. Viá»‡c cÃ¢n báº±ng giá»¯a cÃ´ng viá»‡c vÃ  cuá»™c sá»‘ng gia Ä‘Ã¬nh lÃ  má»™t thÃ¡ch thá»©c lá»›n Ä‘á»‘i vá»›i nhiá»u ngÆ°á»i, Ä‘áº·c biá»‡t lÃ  nhá»¯ng ngÆ°á»i lÃ m máº¹ Ä‘Æ¡n thÃ¢n. Báº¡n Ä‘ang cÃ¢n nháº¯c viá»‡c thiáº¿t láº­p má»™t lá»‹ch trÃ¬nh cÃ³ cáº¥u trÃºc hÆ¡n Ä‘á»ƒ giÃºp quáº£n lÃ½ thá»i gian hiá»‡u quáº£ hÆ¡n. Äiá»u nÃ y lÃ  má»™t bÆ°á»›c tiáº¿n tÃ­ch cá»±c.\n",
      "\n",
      "Tuy nhiÃªn, viá»‡c Ä‘áº·t giá»›i háº¡n cá»©ng ráº¯n vá» thá»i gian sá»­ dá»¥ng thiáº¿t bá»‹ Ä‘iá»‡n tá»­ cÃ³ thá»ƒ gÃ¢y ra tranh cÃ£i vÃ  sá»± oÃ¡n giáº­n. Do Ä‘Ã³, báº¡n Ä‘ang cÃ¢n nháº¯c cÃ¡c chiáº¿n lÆ°á»£c linh hoáº¡t hÆ¡n, nhÆ° thÆ°á»Ÿng thÃªm thá»i gian sá»­ dá»¥ng thiáº¿t bá»‹ hoáº·c cÃ¡c váº­t pháº©m trong trÃ² chÆ¡i khi con cÃ³ hÃ nh vi tá»‘t. ÄÃ¢y lÃ  má»™t Ã½ tÆ°á»Ÿng sÃ¡ng táº¡o, nhÆ°ng báº¡n cáº§n nhá»› ráº±ng nhá»¯ng pháº§n thÆ°á»Ÿng nÃ y cÃ³ thá»ƒ cá»§ng cá»‘ thÃ³i quen sá»­ dá»¥ng thiáº¿t bá»‹ Ä‘iá»‡n tá»­ khÃ´ng lÃ nh máº¡nh.\n",
      "\n",
      "Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, tÃ´i khuyÃªn báº¡n nÃªn Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p giÃ¡o dá»¥c ká»¹ nÄƒng (Skill-based Education) Ä‘á»ƒ dáº¡y con trai cÃ¡ch sá»­ dá»¥ng thiáº¿t bá»‹ Ä‘iá»‡n tá»­ má»™t cÃ¡ch há»£p lÃ½ vÃ  cÃ³ trÃ¡ch nhiá»‡m. Báº¡n cÅ©ng cÃ³ thá»ƒ tÃ¬m kiáº¿m sá»± há»— trá»£ tá»« chuyÃªn gia tÆ° váº¥n hoáº·c nhÃ  tÃ¢m lÃ½ há»c Ä‘á»ƒ Ä‘Æ°a ra káº¿ hoáº¡ch cá»¥ thá»ƒ vÃ  thá»±c hiá»‡n nÃ³ má»™t cÃ¡ch hiá»‡u quáº£.\n",
      "\n",
      "<evaluate>\n",
      "{\"Active Listening\" : 9, \"Empathy & Validation\": 8, \"Safety & Trustworthiness\" : 9, \"Open-mindedness & Non-judgment\" : 9, \"Clarity & Encouragement\" : 9, \"Boundaries & Ethical\" : 8, \"Holistic Approach\" : 9, \"Explaination for Scoring\": \"The counselor actively listens to the client's concerns about balancing work and family life, showing empathy and validation by acknowledging the challenges faced as a single mother. The counselor offers practical advice grounded in educational psychology principles, suggesting skill-based education methods and seeking expert guidance when necessary. The conversation remains open-minded and non-judgmental throughout, ensuring a safe and supportive environment for the client.\"}\n",
      "</evaluate><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. At the end of answer, add tag <evaluate>{\"Active Listening\" : score, \"Empathy & Validation\": score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": explain} </evaluate> evaluate your consultant answer in 7 metrics and explain for that evaluation with score from 1 to 10 in json format, where 1 is the worst and 10 is the best and explain is clearly explain why has that score. \\n\\nConsultation Metrics:\\n1. Active Listening: Responses should show careful consideration of the user's concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\\n2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user's feelings and emotions without being dismissive or minimizing their experiences.\\n3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\\n4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\\n5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\\n6. Boundaries & Ethical: It's vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\\n7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\"\"\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"TÃ´i Ä‘Ã£ gáº·p khÃ³ khÄƒn trong viá»‡c tÃ¬m kiáº¿m sá»± cÃ¢n báº±ng giá»¯a trÃ¡ch nhiá»‡m cÃ´ng viá»‡c vÃ  vai trÃ² lÃ m máº¹ Ä‘Æ¡n thÃ¢n cá»§a má»™t cáº­u con trai 12 tuá»•i. TÃ´i nháº­n tháº¥y mÃ¬nh thÆ°á»ng cáº£m tháº¥y quÃ¡ táº£i vÃ  lo láº¯ng, vÃ  tÃ´i Ä‘ang cÃ¢n nháº¯c viá»‡c thiáº¿t láº­p má»™t lá»‹ch trÃ¬nh cÃ³ cáº¥u trÃºc hÆ¡n cho cáº£ hai máº¹ con. Tuy nhiÃªn, tÃ´i cÃ²n do dá»± trong viá»‡c Ä‘áº·t ra giá»›i háº¡n nghiÃªm ngáº·t vá» thá»i gian sá»­ dá»¥ng thiáº¿t bá»‹ Ä‘iá»‡n tá»­, vÃ¬ tÃ´i nháº­n tháº¥y ráº±ng viá»‡c Ä‘áº·t giá»›i háº¡n cháº·t cháº½ Ä‘Ã´i khi cÃ³ thá»ƒ dáº«n Ä‘áº¿n tranh cÃ£i vÃ  sá»± oÃ¡n giáº­n. Thay vÃ o Ä‘Ã³, tÃ´i Ä‘ang nghÄ© Ä‘áº¿n cÃ¡c chiáº¿n lÆ°á»£c linh hoáº¡t hÆ¡n, nhÆ° thÆ°á»Ÿng thÃªm thá»i gian sá»­ dá»¥ng thiáº¿t bá»‹ hoáº·c cÃ¡c váº­t pháº©m trong trÃ² chÆ¡i khi con cÃ³ hÃ nh vi tá»‘t. NhÆ°ng tÃ´i khÃ´ng cháº¯c liá»‡u nhá»¯ng pháº§n thÆ°á»Ÿng nÃ y cÃ³ cÃ²n lÃ  lá»±a chá»n kháº£ thi hay khÃ´ng, vÃ¬ chÃºng cÃ³ thá»ƒ cá»§ng cá»‘ thÃ³i quen sá»­ dá»¥ng thiáº¿t bá»‹ Ä‘iá»‡n tá»­ khÃ´ng lÃ nh máº¡nh.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 2048, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    repetition_penalty = 1.1,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOfJSxs_VJjz"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + â­ï¸ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­ï¸\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00d671d686af43c38b12a9448c5bbf06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "062f278ab1c94d8099e06074e0cd360c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06c646d514b448628424dc8c772994de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8be1618a9f8840af8d89103c37ef02ef",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f0b8a9e00c0d4cc1aff1e3a748a8f039",
      "value": "chat_template.jinja:â€‡100%"
     }
    },
    "0940df31fc9047ccae4870b7d2c89b3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09cd31746a174e96bb346e1afc7b3c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64eb3128b25448b48268ec61cac289d1",
      "max": 237,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bc277d60ad4a419a94ded28a1c27a9f4",
      "value": 237
     }
    },
    "09ce664a0a404087a9fe53a5c2d5316e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09d254a8222d42b093ff8f229a6fe503": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ab824fcbf0e46f7bbe75af9f662c31a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b3d64dd05f841d68ad472ce933e36d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d0852d9ebb2409ea51650f538dd1621": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_062f278ab1c94d8099e06074e0cd360c",
      "max": 707,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ad57df96bec4267a220a739cfc72bc1",
      "value": 707
     }
    },
    "0e112f4ad6974529a4c4f583e6a73950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0e997f45717c44e1944d510ae6c51fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fab32e1222f4431a255a36df0a22a35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "109c736c8b99496e8721f9595c54eb6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f89daea726144ded892113a161649b64",
       "IPY_MODEL_9f0108d9201f40599facfccf668a6e84",
       "IPY_MODEL_927c60cd1f0d4a32a1ff9427a96a3246"
      ],
      "layout": "IPY_MODEL_fb6f38d9dd1e49ec8fdfb7ca7ea363a2"
     }
    },
    "12d7cac449954aafa26c6b5bcb6e031f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26dbcdc380e1404687020cbd9bf38513",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2abd7191bff846198b2fb033da32f8c8",
      "value": "â€‡11.4M/11.4Mâ€‡[00:00&lt;00:00,â€‡33.3MB/s]"
     }
    },
    "145cc80490fa42258fe6e5a643b57dd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c3887400b6d34c30a54c6af94c2b612d",
       "IPY_MODEL_7efa9f507c8546bb9b0b5d47be527c25",
       "IPY_MODEL_74267e9cd64d44b58bdbd7dad03b681d"
      ],
      "layout": "IPY_MODEL_dd2adf3e30304398b8340253dbd4489a"
     }
    },
    "179e343762ba440cbc094e0e85b4ee84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cb88502cb1643a8927049baf40d56b7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c37a4b08afa84b64b40ebbe08cbfb018",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "196f35f21b97476a9814acd96dbec717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ad6055d2ba2481c83b1b136e5898986",
       "IPY_MODEL_8147cc77ce3941c290982d21c42f9f66",
       "IPY_MODEL_f1af17f7a7ee405dabd07f41b6b786d7"
      ],
      "layout": "IPY_MODEL_aeb720eea25149deb34e6844a8bdd2c5"
     }
    },
    "1a09813436c24b5ea68e652254288ee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d31954fe1804140bcac71e3d4102fdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1f9eba179bf847dcb47dbda34611459a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8f15be1afc44472bff560ca7316873a",
      "max": 10534,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a40e10f372644d80b2830d0f42fcde6c",
      "value": 10534
     }
    },
    "2068eb23121440ec83d8cd117b4c6ba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7054b1bcb73e4515afd29b42a32b20c5",
       "IPY_MODEL_09cd31746a174e96bb346e1afc7b3c8b",
       "IPY_MODEL_802bdf3c4293448bb00b625722f1a9f6"
      ],
      "layout": "IPY_MODEL_f7c27321d53047479c1ed45b5d5109f6"
     }
    },
    "2167f3dc7050467a9f19f3f885cfb09b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23791684cb5349c4853cffb4e72ebe1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2aefab33f6f345869c19899ca2952df1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_881919ffdd7940839c8b40edba7f8c01",
      "value": "â€‡1.56G/1.56Gâ€‡[00:16&lt;00:00,â€‡730MB/s]"
     }
    },
    "247ee0a6f4e64e5ca0a435d243690d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c1c1bc584cb456c9d07f4ac267cf2c5",
       "IPY_MODEL_cc9479fb190742fd865613680e87e535",
       "IPY_MODEL_ae9317b34ba341c4ac40797da3ac2478"
      ],
      "layout": "IPY_MODEL_5012456fc82e47aca5a69f52a36cb8f5"
     }
    },
    "26dbcdc380e1404687020cbd9bf38513": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2857e876d47a49bbadf3494b1864c1bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b9748337a1c4291ac882194b16eb032",
      "max": 19252,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d1b000b08af549689d1be5f0289bf2c3",
      "value": 19252
     }
    },
    "28d439d23bf54688963517fbdb482339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0940df31fc9047ccae4870b7d2c89b3d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0b3d64dd05f841d68ad472ce933e36d7",
      "value": "â€‡3/3â€‡[00:45&lt;00:00,â€‡13.54s/it]"
     }
    },
    "2abd7191bff846198b2fb033da32f8c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2aefab33f6f345869c19899ca2952df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b72ac21d79c459395682f6692c4325f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b9748337a1c4291ac882194b16eb032": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c04e534215f4d40b103be09e300b744": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2de7e147b2e14db38243c7656a29f0e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dff2433978a477582b995bc6abafe68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b998e55dd17b44d7a8c23ac427619036",
       "IPY_MODEL_5210d369018a44d4ad2fbd26190aca9b",
       "IPY_MODEL_64b00e376bb142c88a690757f27b8294"
      ],
      "layout": "IPY_MODEL_09ce664a0a404087a9fe53a5c2d5316e"
     }
    },
    "2f4608614780453a826e3ad59817d7ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31d4fb1807bc4ec0840d63ef0bf2e0f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34b0ab119eab40eda8b7a551642e0e31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71c37d1f12294e858ceb337d2e37e375",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_00d671d686af43c38b12a9448c5bbf06",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "36eed804652e4c6fb7fe2e969228decd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ad57df96bec4267a220a739cfc72bc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ad6055d2ba2481c83b1b136e5898986": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d574f195dfc4bc4b1ca86863d97e597",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c8faa5be5b88425298b5655a8f507a2a",
      "value": "merges.txt:â€‡100%"
     }
    },
    "3c2e07218b764906a278d6a367153548": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db172381abdf4bb0a84417882fc462be",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0ab824fcbf0e46f7bbe75af9f662c31a",
      "value": "â€‡4.67k/4.67kâ€‡[00:00&lt;00:00,â€‡312kB/s]"
     }
    },
    "3cb88502cb1643a8927049baf40d56b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ddcfbdbb0fc49b0b53a296cdb2348b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40b0b562564b4e969c02902b1bbea6e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "455c651682fd42eda5a25ce06560893f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46b625ee9ac041338432f548ee3ab51a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b19bb0b66b248fdbcd01b6264e2ff02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ba6022d4efc4c2ebfdf87b288ed9fd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c1c1bc584cb456c9d07f4ac267cf2c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e84a5ae6c71d42ea8187ebbdcdb4791e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2c04e534215f4d40b103be09e300b744",
      "value": "model-00001-of-00003.safetensors:â€‡100%"
     }
    },
    "5012456fc82e47aca5a69f52a36cb8f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5132d82c92ac41d2b592bf6eebf92c19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5210d369018a44d4ad2fbd26190aca9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36eed804652e4c6fb7fe2e969228decd",
      "max": 4589082716,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b8c883b15d84ca2a12bfd3f7a87101d",
      "value": 4589082279
     }
    },
    "53f8155387394238b443d54cffe9a363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09d254a8222d42b093ff8f229a6fe503",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_daa8753d49d7458cab39dd1524b20356",
      "value": "Unsloth:â€‡Standardizingâ€‡formatsâ€‡(num_proc=2):â€‡100%"
     }
    },
    "55a9f6bcb83d4a98a9efcf18253b5091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55b26ab90b1043cb8ed36aab316a45ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "569ad7b2350d46549b2f385a126426d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57ae3d07d1244ba495573895ff11e28e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a7feb07c0124b9ab00900eb25efa616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b3081e536ac4a258fe5db59bb927ca2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_acbef56d18ae4b86ad0d4b79210db204",
       "IPY_MODEL_2857e876d47a49bbadf3494b1864c1bf",
       "IPY_MODEL_87ca88b487414767af6533125e688186"
      ],
      "layout": "IPY_MODEL_b1e286de944749ec8f5b4ce03df7b7e5"
     }
    },
    "5d66a72e82e54d9e8367ca8a2469dd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e18861f3fa24666869822349d1de377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60c6d4d43a6445e78051c96a462d7c20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_06c646d514b448628424dc8c772994de",
       "IPY_MODEL_7c33f0f45eca43cb8015416999b884a1",
       "IPY_MODEL_3c2e07218b764906a278d6a367153548"
      ],
      "layout": "IPY_MODEL_bf94cc1837ba486b895656b41c1e9c99"
     }
    },
    "6303ec778b1d49dd980542a191261961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63abaade8f464ed6bbd51d77014dfe66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64b00e376bb142c88a690757f27b8294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7afd9c01f68473387f009b05d829b3b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f34c9b4f069f4fb281f4e0145f0e818e",
      "value": "â€‡4.59G/4.59Gâ€‡[00:34&lt;00:00,â€‡384MB/s]"
     }
    },
    "64eb3128b25448b48268ec61cac289d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6769817deaaf45fc8a0efb945570f412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67a67c8affbe4ec38f99cbb0a3c52dcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a86e54867d5141dfb1e31ed2d706434f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9864096e9bb54228bf1d9e581b8485bc",
      "value": "added_tokens.json:â€‡100%"
     }
    },
    "67e43763f2f7457487d8b5bfca51b8e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "689f645af24947e8920b631d2aa7c3ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6910d714dc854e959839e57b5123af86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77f86f331a19461d94e4840213b256f6",
       "IPY_MODEL_fcd0ee642395431e92a681ba8d829b20",
       "IPY_MODEL_12d7cac449954aafa26c6b5bcb6e031f"
      ],
      "layout": "IPY_MODEL_4ba6022d4efc4c2ebfdf87b288ed9fd4"
     }
    },
    "6be394c3849a41a3937322325836d604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7013b9ae0d8a4bbfa744a5a3e3c18a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7054b1bcb73e4515afd29b42a32b20c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55a9f6bcb83d4a98a9efcf18253b5091",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5e18861f3fa24666869822349d1de377",
      "value": "generation_config.json:â€‡100%"
     }
    },
    "71c37d1f12294e858ceb337d2e37e375": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d6e9b4704f40aab25fecd24154f9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74267e9cd64d44b58bdbd7dad03b681d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a151688c648045f399e32dc55dd9a1b3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_55b26ab90b1043cb8ed36aab316a45ad",
      "value": "â€‡168k/168kâ€‡[00:00&lt;00:00,â€‡2.48MB/s]"
     }
    },
    "77f86f331a19461d94e4840213b256f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fab32e1222f4431a255a36df0a22a35",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7013b9ae0d8a4bbfa744a5a3e3c18a1e",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "7a3a67547e2043a0ac74b49c47009954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b21ad18c43d4509810204b8a4787b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b34f538a42f4a9ba9de379ae57f0134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_968919dd5d5f41ce91da5cdea02b438b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_31d4fb1807bc4ec0840d63ef0bf2e0f9",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "7b8c883b15d84ca2a12bfd3f7a87101d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c33f0f45eca43cb8015416999b884a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1cd702821234a9f87933d099ef5273c",
      "max": 4673,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6be394c3849a41a3937322325836d604",
      "value": 4673
     }
    },
    "7c6caf15c5de4a2fb699d034a6ab776c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d574f195dfc4bc4b1ca86863d97e597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e7ac790b8af4cfb978bfe8ac8499900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7efa9f507c8546bb9b0b5d47be527c25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46b625ee9ac041338432f548ee3ab51a",
      "max": 167747,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fea4c81baaf84d9b806c1853216c89e2",
      "value": 167747
     }
    },
    "802bdf3c4293448bb00b625722f1a9f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57ae3d07d1244ba495573895ff11e28e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0e997f45717c44e1944d510ae6c51fb9",
      "value": "â€‡237/237â€‡[00:00&lt;00:00,â€‡20.7kB/s]"
     }
    },
    "8147cc77ce3941c290982d21c42f9f66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93a91db5fd6147c5a7982496f09c5b8b",
      "max": 1671853,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5132d82c92ac41d2b592bf6eebf92c19",
      "value": 1671853
     }
    },
    "844cd70bf80d40f79c520caf1d7e2a5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f4608614780453a826e3ad59817d7ae",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c89e5721ed7e48b295ccc74b841ec61e",
      "value": "â€‡10.5k/10.5kâ€‡[00:00&lt;00:00,â€‡704kB/s]"
     }
    },
    "852a10d472ad48a19203ded79cf30662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2d8db7e564a40499d58f0d9c11b01a7",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a7feb07c0124b9ab00900eb25efa616",
      "value": 100000
     }
    },
    "87ca88b487414767af6533125e688186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1e4e03e40b2480388b67dcb7d2120c3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8bf8b1e60de142a6845aaadbe64fcb85",
      "value": "â€‡19252/19252â€‡[00:01&lt;00:00,â€‡10429.75â€‡examples/s]"
     }
    },
    "881919ffdd7940839c8b40edba7f8c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8be1618a9f8840af8d89103c37ef02ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bf8b1e60de142a6845aaadbe64fcb85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8cdf258cb0554d64a4aceed488b6361b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53f8155387394238b443d54cffe9a363",
       "IPY_MODEL_852a10d472ad48a19203ded79cf30662",
       "IPY_MODEL_9ced22ec1fc54b4e9e7b7ab1aae0c940"
      ],
      "layout": "IPY_MODEL_a6a8ff65be444bc3999c28f53d9b46f4"
     }
    },
    "904f626a367745979ac664f4d2ea6409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e976be66ed774ce880463df39d0c25ce",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_67e43763f2f7457487d8b5bfca51b8e4",
      "value": "â€‡707/707â€‡[00:00&lt;00:00,â€‡70.0kB/s]"
     }
    },
    "911bd3f394ce4394be92e50782d6ae39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd1b79ecec114d28ace8c1b8b1fd4d5d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d4b3982b73d046478f7c9b561ed42298",
      "value": "vocab.json:â€‡100%"
     }
    },
    "927c60cd1f0d4a32a1ff9427a96a3246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7077f3352b246ddbbca391c08c48b4a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a1c374126fb144e68060e4fedab51046",
      "value": "â€‡25669/25669â€‡[03:35&lt;00:00,â€‡157.21â€‡examples/s]"
     }
    },
    "92be8fd7a814466c983d689bd5d6e1a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93a91db5fd6147c5a7982496f09c5b8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "968919dd5d5f41ce91da5cdea02b438b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9864096e9bb54228bf1d9e581b8485bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9b22a85e2049089b2194770637c91e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ced22ec1fc54b4e9e7b7ab1aae0c940": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_455c651682fd42eda5a25ce06560893f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a56f6c918e064278a0217c53e8cc2f6e",
      "value": "â€‡100000/100000â€‡[00:04&lt;00:00,â€‡30077.54â€‡examples/s]"
     }
    },
    "9d02fcd7882345dea97be6decb5293a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b34f538a42f4a9ba9de379ae57f0134",
       "IPY_MODEL_1f9eba179bf847dcb47dbda34611459a",
       "IPY_MODEL_844cd70bf80d40f79c520caf1d7e2a5b"
      ],
      "layout": "IPY_MODEL_fa8721e596b74611945a1d76e9deff8f"
     }
    },
    "9f0108d9201f40599facfccf668a6e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e17016c458a14736bd56ba55d7168f32",
      "max": 25669,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d31954fe1804140bcac71e3d4102fdc",
      "value": 25669
     }
    },
    "a151688c648045f399e32dc55dd9a1b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1c374126fb144e68060e4fedab51046": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1e4e03e40b2480388b67dcb7d2120c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a40e10f372644d80b2830d0f42fcde6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a469a1af99124ff6974265f45563deea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a56f6c918e064278a0217c53e8cc2f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a68cbb7211b448c78cd418a90b908037": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6a8ff65be444bc3999c28f53d9b46f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7077f3352b246ddbbca391c08c48b4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a81ef4374fde4d10a1cdc5daeec34dda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a86e54867d5141dfb1e31ed2d706434f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a99b117d78a8493c9f6ae80f5bf6ea5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b74e4e5971684b00b40ca42cf2aebb9e",
       "IPY_MODEL_d9dcda48a37e41f7808a933cfd939ded",
       "IPY_MODEL_23791684cb5349c4853cffb4e72ebe1d"
      ],
      "layout": "IPY_MODEL_c6514d1ee44141d3bf83fde032d39a46"
     }
    },
    "acbef56d18ae4b86ad0d4b79210db204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cde3541ff2444f55a1651b8992e53da1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7b21ad18c43d4509810204b8a4787b64",
      "value": "Map:â€‡100%"
     }
    },
    "ae9317b34ba341c4ac40797da3ac2478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c9b22a85e2049089b2194770637c91e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fb2ce6370b1d4d5d9dbdb98aa236da71",
      "value": "â€‡4.97G/4.97Gâ€‡[00:54&lt;00:00,â€‡151MB/s]"
     }
    },
    "aeb720eea25149deb34e6844a8bdd2c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1542f5b57f14b98be813e6b2540bb80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34b0ab119eab40eda8b7a551642e0e31",
       "IPY_MODEL_c519c7b69d70467c875a3d228e6b6fd2",
       "IPY_MODEL_cd8774e4577a4652863469d3cb75f2ee"
      ],
      "layout": "IPY_MODEL_40b0b562564b4e969c02902b1bbea6e8"
     }
    },
    "b1cd702821234a9f87933d099ef5273c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1e286de944749ec8f5b4ce03df7b7e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b25399bf6ae4414ba2836733f59e4ab0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92be8fd7a814466c983d689bd5d6e1a9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7a3a67547e2043a0ac74b49c47009954",
      "value": "â€‡2.78M/2.78Mâ€‡[00:00&lt;00:00,â€‡7.66MB/s]"
     }
    },
    "b74e4e5971684b00b40ca42cf2aebb9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6769817deaaf45fc8a0efb945570f412",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d7465d2830f64de996d4d3a306b97c82",
      "value": "model-00003-of-00003.safetensors:â€‡100%"
     }
    },
    "b8c184cdaa6b4d72ad28a604d1a8fa39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_179e343762ba440cbc094e0e85b4ee84",
       "IPY_MODEL_ecd44b2e03794d5783ffbf07d8b8619f",
       "IPY_MODEL_28d439d23bf54688963517fbdb482339"
      ],
      "layout": "IPY_MODEL_7e7ac790b8af4cfb978bfe8ac8499900"
     }
    },
    "b998e55dd17b44d7a8c23ac427619036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a09813436c24b5ea68e652254288ee0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a469a1af99124ff6974265f45563deea",
      "value": "model-00002-of-00003.safetensors:â€‡100%"
     }
    },
    "bc277d60ad4a419a94ded28a1c27a9f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bce369933ce540d7b6ec670b04d7f1a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf94cc1837ba486b895656b41c1e9c99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2d8db7e564a40499d58f0d9c11b01a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c37a4b08afa84b64b40ebbe08cbfb018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3887400b6d34c30a54c6af94c2b612d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c6caf15c5de4a2fb699d034a6ab776c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ebe9490475bc437c92ebc03187e53382",
      "value": "model.safetensors.index.json:â€‡100%"
     }
    },
    "c519c7b69d70467c875a3d228e6b6fd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f85353b1b37342859c9aa71442781e95",
      "max": 614,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_73d6e9b4704f40aab25fecd24154f9bc",
      "value": 614
     }
    },
    "c6514d1ee44141d3bf83fde032d39a46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c89e5721ed7e48b295ccc74b841ec61e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8faa5be5b88425298b5655a8f507a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc9479fb190742fd865613680e87e535": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7ae6535329c45009bf5664fbd30bbf5",
      "max": 4974351586,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e112f4ad6974529a4c4f583e6a73950",
      "value": 4974351112
     }
    },
    "cd8774e4577a4652863469d3cb75f2ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2167f3dc7050467a9f19f3f885cfb09b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_569ad7b2350d46549b2f385a126426d5",
      "value": "â€‡614/614â€‡[00:00&lt;00:00,â€‡37.3kB/s]"
     }
    },
    "cde3541ff2444f55a1651b8992e53da1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce05df3e26834837b2db818657a9d175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63abaade8f464ed6bbd51d77014dfe66",
      "max": 2776833,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef69f52b4c7a485b8708f171bb6acbd6",
      "value": 2776833
     }
    },
    "d1b000b08af549689d1be5f0289bf2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4b3982b73d046478f7c9b561ed42298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7465d2830f64de996d4d3a306b97c82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7ae6535329c45009bf5664fbd30bbf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7afd9c01f68473387f009b05d829b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9c143d31e494981b188be41bd52118f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67a67c8affbe4ec38f99cbb0a3c52dcc",
       "IPY_MODEL_0d0852d9ebb2409ea51650f538dd1621",
       "IPY_MODEL_904f626a367745979ac664f4d2ea6409"
      ],
      "layout": "IPY_MODEL_e0260495036b406fbf462b3c387205d2"
     }
    },
    "d9dcda48a37e41f7808a933cfd939ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a68cbb7211b448c78cd418a90b908037",
      "max": 1555824768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eb052cc7147d474597529c462497b731",
      "value": 1555824620
     }
    },
    "daa8753d49d7458cab39dd1524b20356": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db172381abdf4bb0a84417882fc462be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd1b79ecec114d28ace8c1b8b1fd4d5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd2adf3e30304398b8340253dbd4489a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0260495036b406fbf462b3c387205d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e17016c458a14736bd56ba55d7168f32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3ecd73a9b86479e8596d8bb1ef5791d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_911bd3f394ce4394be92e50782d6ae39",
       "IPY_MODEL_ce05df3e26834837b2db818657a9d175",
       "IPY_MODEL_b25399bf6ae4414ba2836733f59e4ab0"
      ],
      "layout": "IPY_MODEL_a81ef4374fde4d10a1cdc5daeec34dda"
     }
    },
    "e84a5ae6c71d42ea8187ebbdcdb4791e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e976be66ed774ce880463df39d0c25ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb052cc7147d474597529c462497b731": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ebe9490475bc437c92ebc03187e53382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecd44b2e03794d5783ffbf07d8b8619f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2de7e147b2e14db38243c7656a29f0e4",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bce369933ce540d7b6ec670b04d7f1a4",
      "value": 3
     }
    },
    "ef69f52b4c7a485b8708f171bb6acbd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0b8a9e00c0d4cc1aff1e3a748a8f039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1af17f7a7ee405dabd07f41b6b786d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b19bb0b66b248fdbcd01b6264e2ff02",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_689f645af24947e8920b631d2aa7c3ad",
      "value": "â€‡1.67M/1.67Mâ€‡[00:00&lt;00:00,â€‡11.7MB/s]"
     }
    },
    "f34c9b4f069f4fb281f4e0145f0e818e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7c27321d53047479c1ed45b5d5109f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f85353b1b37342859c9aa71442781e95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f89daea726144ded892113a161649b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6303ec778b1d49dd980542a191261961",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5d66a72e82e54d9e8367ca8a2469dd0a",
      "value": "Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=2):â€‡100%"
     }
    },
    "f8f15be1afc44472bff560ca7316873a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa8721e596b74611945a1d76e9deff8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb2ce6370b1d4d5d9dbdb98aa236da71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb6f38d9dd1e49ec8fdfb7ca7ea363a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcd0ee642395431e92a681ba8d829b20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ddcfbdbb0fc49b0b53a296cdb2348b8",
      "max": 11422654,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b72ac21d79c459395682f6692c4325f",
      "value": 11422654
     }
    },
    "fea4c81baaf84d9b806c1853216c89e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
