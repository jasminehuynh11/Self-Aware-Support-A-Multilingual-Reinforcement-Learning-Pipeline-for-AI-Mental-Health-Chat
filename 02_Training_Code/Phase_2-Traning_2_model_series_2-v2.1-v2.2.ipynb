{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN8396LUzpfR"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COYftUOUzpfS"
   },
   "source": [
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZQ2BIvCzpfT"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iajq1W8ipjyK"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573,
     "referenced_widgets": [
      "145cc80490fa42258fe6e5a643b57dd5",
      "c3887400b6d34c30a54c6af94c2b612d",
      "7efa9f507c8546bb9b0b5d47be527c25",
      "74267e9cd64d44b58bdbd7dad03b681d",
      "dd2adf3e30304398b8340253dbd4489a",
      "7c6caf15c5de4a2fb699d034a6ab776c",
      "ebe9490475bc437c92ebc03187e53382",
      "46b625ee9ac041338432f548ee3ab51a",
      "fea4c81baaf84d9b806c1853216c89e2",
      "a151688c648045f399e32dc55dd9a1b3",
      "55b26ab90b1043cb8ed36aab316a45ad",
      "247ee0a6f4e64e5ca0a435d243690d0a",
      "4c1c1bc584cb456c9d07f4ac267cf2c5",
      "cc9479fb190742fd865613680e87e535",
      "ae9317b34ba341c4ac40797da3ac2478",
      "5012456fc82e47aca5a69f52a36cb8f5",
      "e84a5ae6c71d42ea8187ebbdcdb4791e",
      "2c04e534215f4d40b103be09e300b744",
      "d7ae6535329c45009bf5664fbd30bbf5",
      "0e112f4ad6974529a4c4f583e6a73950",
      "9c9b22a85e2049089b2194770637c91e",
      "fb2ce6370b1d4d5d9dbdb98aa236da71",
      "2dff2433978a477582b995bc6abafe68",
      "b998e55dd17b44d7a8c23ac427619036",
      "5210d369018a44d4ad2fbd26190aca9b",
      "64b00e376bb142c88a690757f27b8294",
      "09ce664a0a404087a9fe53a5c2d5316e",
      "1a09813436c24b5ea68e652254288ee0",
      "a469a1af99124ff6974265f45563deea",
      "36eed804652e4c6fb7fe2e969228decd",
      "7b8c883b15d84ca2a12bfd3f7a87101d",
      "d7afd9c01f68473387f009b05d829b3b",
      "f34c9b4f069f4fb281f4e0145f0e818e",
      "a99b117d78a8493c9f6ae80f5bf6ea5a",
      "b74e4e5971684b00b40ca42cf2aebb9e",
      "d9dcda48a37e41f7808a933cfd939ded",
      "23791684cb5349c4853cffb4e72ebe1d",
      "c6514d1ee44141d3bf83fde032d39a46",
      "6769817deaaf45fc8a0efb945570f412",
      "d7465d2830f64de996d4d3a306b97c82",
      "a68cbb7211b448c78cd418a90b908037",
      "eb052cc7147d474597529c462497b731",
      "2aefab33f6f345869c19899ca2952df1",
      "881919ffdd7940839c8b40edba7f8c01",
      "b8c184cdaa6b4d72ad28a604d1a8fa39",
      "179e343762ba440cbc094e0e85b4ee84",
      "ecd44b2e03794d5783ffbf07d8b8619f",
      "28d439d23bf54688963517fbdb482339",
      "7e7ac790b8af4cfb978bfe8ac8499900",
      "3cb88502cb1643a8927049baf40d56b7",
      "c37a4b08afa84b64b40ebbe08cbfb018",
      "2de7e147b2e14db38243c7656a29f0e4",
      "bce369933ce540d7b6ec670b04d7f1a4",
      "0940df31fc9047ccae4870b7d2c89b3d",
      "0b3d64dd05f841d68ad472ce933e36d7",
      "2068eb23121440ec83d8cd117b4c6ba5",
      "7054b1bcb73e4515afd29b42a32b20c5",
      "09cd31746a174e96bb346e1afc7b3c8b",
      "802bdf3c4293448bb00b625722f1a9f6",
      "f7c27321d53047479c1ed45b5d5109f6",
      "55a9f6bcb83d4a98a9efcf18253b5091",
      "5e18861f3fa24666869822349d1de377",
      "64eb3128b25448b48268ec61cac289d1",
      "bc277d60ad4a419a94ded28a1c27a9f4",
      "57ae3d07d1244ba495573895ff11e28e",
      "0e997f45717c44e1944d510ae6c51fb9",
      "9d02fcd7882345dea97be6decb5293a5",
      "7b34f538a42f4a9ba9de379ae57f0134",
      "1f9eba179bf847dcb47dbda34611459a",
      "844cd70bf80d40f79c520caf1d7e2a5b",
      "fa8721e596b74611945a1d76e9deff8f",
      "968919dd5d5f41ce91da5cdea02b438b",
      "31d4fb1807bc4ec0840d63ef0bf2e0f9",
      "f8f15be1afc44472bff560ca7316873a",
      "a40e10f372644d80b2830d0f42fcde6c",
      "2f4608614780453a826e3ad59817d7ae",
      "c89e5721ed7e48b295ccc74b841ec61e",
      "e3ecd73a9b86479e8596d8bb1ef5791d",
      "911bd3f394ce4394be92e50782d6ae39",
      "ce05df3e26834837b2db818657a9d175",
      "b25399bf6ae4414ba2836733f59e4ab0",
      "a81ef4374fde4d10a1cdc5daeec34dda",
      "dd1b79ecec114d28ace8c1b8b1fd4d5d",
      "d4b3982b73d046478f7c9b561ed42298",
      "63abaade8f464ed6bbd51d77014dfe66",
      "ef69f52b4c7a485b8708f171bb6acbd6",
      "92be8fd7a814466c983d689bd5d6e1a9",
      "7a3a67547e2043a0ac74b49c47009954",
      "196f35f21b97476a9814acd96dbec717",
      "3ad6055d2ba2481c83b1b136e5898986",
      "8147cc77ce3941c290982d21c42f9f66",
      "f1af17f7a7ee405dabd07f41b6b786d7",
      "aeb720eea25149deb34e6844a8bdd2c5",
      "7d574f195dfc4bc4b1ca86863d97e597",
      "c8faa5be5b88425298b5655a8f507a2a",
      "93a91db5fd6147c5a7982496f09c5b8b",
      "5132d82c92ac41d2b592bf6eebf92c19",
      "4b19bb0b66b248fdbcd01b6264e2ff02",
      "689f645af24947e8920b631d2aa7c3ad",
      "d9c143d31e494981b188be41bd52118f",
      "67a67c8affbe4ec38f99cbb0a3c52dcc",
      "0d0852d9ebb2409ea51650f538dd1621",
      "904f626a367745979ac664f4d2ea6409",
      "e0260495036b406fbf462b3c387205d2",
      "a86e54867d5141dfb1e31ed2d706434f",
      "9864096e9bb54228bf1d9e581b8485bc",
      "062f278ab1c94d8099e06074e0cd360c",
      "3ad57df96bec4267a220a739cfc72bc1",
      "e976be66ed774ce880463df39d0c25ce",
      "67e43763f2f7457487d8b5bfca51b8e4",
      "b1542f5b57f14b98be813e6b2540bb80",
      "34b0ab119eab40eda8b7a551642e0e31",
      "c519c7b69d70467c875a3d228e6b6fd2",
      "cd8774e4577a4652863469d3cb75f2ee",
      "40b0b562564b4e969c02902b1bbea6e8",
      "71c37d1f12294e858ceb337d2e37e375",
      "00d671d686af43c38b12a9448c5bbf06",
      "f85353b1b37342859c9aa71442781e95",
      "73d6e9b4704f40aab25fecd24154f9bc",
      "2167f3dc7050467a9f19f3f885cfb09b",
      "569ad7b2350d46549b2f385a126426d5",
      "6910d714dc854e959839e57b5123af86",
      "77f86f331a19461d94e4840213b256f6",
      "fcd0ee642395431e92a681ba8d829b20",
      "12d7cac449954aafa26c6b5bcb6e031f",
      "4ba6022d4efc4c2ebfdf87b288ed9fd4",
      "0fab32e1222f4431a255a36df0a22a35",
      "7013b9ae0d8a4bbfa744a5a3e3c18a1e",
      "3ddcfbdbb0fc49b0b53a296cdb2348b8",
      "2b72ac21d79c459395682f6692c4325f",
      "26dbcdc380e1404687020cbd9bf38513",
      "2abd7191bff846198b2fb033da32f8c8",
      "60c6d4d43a6445e78051c96a462d7c20",
      "06c646d514b448628424dc8c772994de",
      "7c33f0f45eca43cb8015416999b884a1",
      "3c2e07218b764906a278d6a367153548",
      "bf94cc1837ba486b895656b41c1e9c99",
      "8be1618a9f8840af8d89103c37ef02ef",
      "f0b8a9e00c0d4cc1aff1e3a748a8f039",
      "b1cd702821234a9f87933d099ef5273c",
      "6be394c3849a41a3937322325836d604",
      "db172381abdf4bb0a84417882fc462be",
      "0ab824fcbf0e46f7bbe75af9f662c31a"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "ad27d22a-d0a6-4659-be63-60bf3b3561b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-16 04:40:14 [__init__.py:244] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.6.2: Fast Qwen3 patching. Transformers: 4.52.4. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.418 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-4b-unsloth-bnb-4bit with actual GPU utilization = 69.27%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 44.42 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 27.91 GB. Also swap space = 6 GB.\n",
      "INFO 06-16 04:40:33 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 06-16 04:40:33 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.4.mlp', 'model.layers.6.self_attn', 'model.layers.34.self_attn', 'model.layers.33.self_attn', 'model.layers.4.self_attn', 'model.layers.34.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.0.mlp', 'model.layers.0.self_attn', 'model.layers.3.mlp', 'model.layers.6.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 06-16 04:40:34 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='unsloth/qwen3-4b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-16 04:40:34 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x745af3f58370>\n",
      "INFO 06-16 04:40:35 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-16 04:40:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-16 04:40:35 [gpu_model_runner.py:1595] Starting to load model unsloth/qwen3-4b-unsloth-bnb-4bit...\n",
      "INFO 06-16 04:40:35 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 06-16 04:40:35 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-16 04:40:35 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 06-16 04:40:36 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 06-16 04:40:37 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f7d4bf0a844896af113aca68ee4142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee07ec9a8d44a5ba34f448c0fc5ed99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 04:40:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 06-16 04:40:39 [gpu_model_runner.py:1624] Model loading took 3.5740 GiB and 2.784077 seconds\n",
      "INFO 06-16 04:40:53 [backends.py:462] Using cache directory: /root/.cache/vllm/torch_compile_cache/9bced0cd06/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-16 04:40:53 [backends.py:472] Dynamo bytecode transform time: 12.79 s\n",
      "INFO 06-16 04:41:02 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 8.845 s\n",
      "INFO 06-16 04:41:26 [monitor.py:34] torch.compile takes 12.79 s in total\n",
      "INFO 06-16 04:41:29 [gpu_worker.py:227] Available KV cache memory: 25.36 GiB\n",
      "INFO 06-16 04:41:29 [kv_cache_utils.py:715] GPU KV cache size: 184,672 tokens\n",
      "INFO 06-16 04:41:29 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 45.09x\n",
      "INFO 06-16 04:42:44 [gpu_model_runner.py:2048] Graph capturing finished in 74 secs, took 1.48 GiB\n",
      "INFO 06-16 04:42:44 [core.py:171] init engine (profile, create kv cache, warmup model) took 125.31 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "lora_rank = 64\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B\",\n",
    "    max_seq_length = 4096,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "d50f06c8-4905-4f4e-c6da-980145c41e29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.2 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = lora_rank,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We will use a custom dataset named `Interview_Data_6K.csv`. This dataset contains conversations with a mental health counselling assistant. Each entry has an `instruction` (acting as a system prompt), an `input` (the user's message), and an `output` (the assistant's response).\n",
    "\n",
    "We need to convert this CSV data into a format suitable for training with `SFTTrainer`, specifically by applying the Qwen3 chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "5kyTw2n1edte",
    "outputId": "baed0061-a8b7-4a1c-e8bd-c21825941481"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c10025218664e37a26a612309402e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4521\n",
      "Evaluation set size: 238\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the custom CSV dataset\n",
    "# The dataset has 'instruction', 'input', 'output' columns\n",
    "full_interview_dataset = load_dataset(\"csv\", data_files=\"./dataset/stage_2_1_synthetic_interview_data_combined.csv\", split=\"train\")\n",
    "\n",
    "# Split the dataset into training and evaluation sets (e.g., 90% train, 10% eval)\n",
    "# Ensure the dataset has more than one example for splitting\n",
    "if len(full_interview_dataset) > 1:\n",
    "    train_test_split = full_interview_dataset.train_test_split(test_size=0.05, seed=3407)\n",
    "    interview_train_dataset = train_test_split['train']\n",
    "    interview_eval_dataset = train_test_split['test']\n",
    "    print(f\"Training set size: {len(interview_train_dataset)}\")\n",
    "    print(f\"Evaluation set size: {len(interview_eval_dataset)}\")\n",
    "else:\n",
    "    # Handle cases with very small datasets\n",
    "    interview_train_dataset = full_interview_dataset\n",
    "    interview_eval_dataset = None\n",
    "    print(f\"Training set size: {len(interview_train_dataset)}\")\n",
    "    print(\"No evaluation set created due to small dataset size.\")\n",
    "\n",
    "# We will display the structure of the training dataset in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTZICZtie3lQ"
   },
   "source": [
    "Let's see the structure of our loaded training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjgH3lt0e2Sz",
    "outputId": "2be3f7c6-7353-42c9-d676-95949db681d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 4521\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jfV47_SXgXH4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91809940f594844bae965c4d2613123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f083ed20eba34cb1b1770341ca625a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f555b4e32ad43bf8f3b752f8c10e569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05445dcea6494223a0956a0d8ae0e577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of final formatted training data (after chat template):\n",
      "<|im_start|>system\n",
      "You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description. \n",
      "The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. <|im_end|>\n",
      "<|im_start|>user\n",
      "我希望透過這次輔導可以學習點樣更好地處理與同事之間嘅關係，因為最近喺公司嘅壓力特別大，令我成日覺得心情好唔好。同事之間有啲誤會，我感覺自己被排斥，好似大家都唔太願意同我溝通，呢啲情況令我感到孤立同埋唔受重視。尤其係上星期，我做咗一份重要嘅報告，完成得唔錯，但同事卻冇表示任何認同，好似我嘅努力完全被忽視咁。呢啲事情令我夜晚訓唔著，經常會諗住啲負面嘅嘢，覺得自己做乜嘢都唔啱，心情低落同埋好焦慮，甚至有時頭痛同胃痛，呢啲症狀每星期都會出現幾次，通常持續幾個鐘頭。喺家庭方面，我同父母嘅關係算係幾好，但佢哋唔太了解我工作上面嘅壓力，所以我都唔太願意講。以前喺讀書時期，曾經有過被同學排擠嘅經歷，所以今次嘅情況特別令我想起當時嘅感受。我試過用聽音樂同做運動嚟舒緩情緒，但效果唔係好持久。我想知道，喺輔導過程中我應該點樣表達自己嘅感受？另外，對於改善同事關係，有冇啲有效嘅方法或技巧？如果我想用中醫同西醫結合嘅方法去處理情緒，有冇啲建議？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "你提出想學習如何更好地與同事相處，這是非常實際且重要的目標。當感受到被排擠和忽視，情緒自然會受到很大影響，而你提及的頭痛和胃痛，很可能是身體對壓力的反應。建議在輔導中，可以嘗試坦誠地分享你的感受和經歷，不用擔心被評價，因為這裡是一個安全的空間。試著用具體事件來描述你的情緒變化，例如那份報告被忽視的時刻，讓討論更聚焦。針對改善同事關係，建立溝通橋樑很重要，可以嘗試主動表達你的想法與感受，同時也聆聽他人的意見，尋求共同解決問題的方法。學習適當的溝通技巧，如「我訊息」來表達自己的感受而非責備對方，可減少誤解。關於中西醫結合處理情緒，這是不少人選擇的路徑，中醫的針灸或草藥可能有助於調節身體狀態，而西醫方面，心理諮詢搭配必要時的藥物治療，有助症狀緩解。你可以把這些想法帶入輔導，讓過程更全面地照顧你身心的需要。最後，輔導過程中可以隨時提出你對方法的疑問，讓我們一起探索最適合你的策略。<|im_end|>\n",
      "\n",
      "\n",
      "Sample of final formatted evaluation data (after chat template):\n",
      "<|im_start|>system\n",
      "You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description. \n",
      "The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. <|im_end|>\n",
      "<|im_start|>user\n",
      "I've been having trouble with my sleep for the past few months, and it's really starting to affect my daily life. My main goal in coming here is to find ways to improve my sleep quality and feel more rested during the day. I often lie awake for hours before finally falling asleep, and even when I do sleep, I wake up multiple times throughout the night. This leads to me feeling exhausted, irritable, and less focused at work. \n",
      "\n",
      "I've noticed that stress at work and some recent family tensions seem to trigger these sleep problems. For example, last week, after a disagreement with my spouse about managing household responsibilities, I couldn't fall asleep until after 3 a.m. On other nights, I just feel overwhelmed by racing thoughts about deadlines and responsibilities. \n",
      "\n",
      "The insomnia happens almost every night, and sometimes I wake up feeling my heart racing or with a sense of restlessness. I can't say it was always this way; in the past, I used to sleep well without much effort. I have a history of some mild anxiety years ago, but nothing that lasted. There was also a traumatic experience about ten years back involving a car accident, but I thought I moved past it.\n",
      "\n",
      "To cope, I sometimes drink herbal tea or try deep breathing exercises, but these don’t always help. I'm curious about different approaches to treatment—do you usually recommend behavioral changes, relaxation techniques, or something else? Also, how long might it take before I notice improvements? Are there specific things I should avoid or try to include in my daily routine to help my sleep? I'd like to understand how counseling sessions like this work towards helping with sleep improvement.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Improving sleep often involves addressing habits and managing stressors that affect your rest. Establishing a consistent bedtime routine can help signal to your body that it’s time to wind down—this might include dimming lights, turning off screens at least an hour before bed, and engaging in calming activities like reading or gentle stretches. \n",
      "\n",
      "Since stress and racing thoughts are impacting your sleep, practicing relaxation techniques such as progressive muscle relaxation or guided imagery before bed may reduce tension. If your mind races, writing down your worries or a to-do list earlier in the evening can help clear your mind.\n",
      "\n",
      "Limiting caffeine intake after mid-afternoon and reducing alcohol consumption are also important, as they can disrupt sleep cycles. Regular physical activity during the day can promote better sleep, but avoid vigorous exercise close to bedtime.\n",
      "\n",
      "Counseling sessions typically explore patterns and triggers related to sleep difficulties and work on developing personalized coping strategies. Changes might be noticeable within a few weeks, but consistency and patience are key. We can also discuss cognitive techniques to challenge unhelpful thoughts about sleep that may contribute to anxiety at night.\n",
      "\n",
      "If family tensions are a significant source of stress, exploring communication strategies might indirectly improve your sleep as well. Throughout this process, you’ll have space to share experiences and try various tools, adjusting as needed based on what works best for you.\n",
      "\n",
      "Would you like to explore specific relaxation exercises during our sessions or focus on restructuring thoughts related to sleep difficulties? Additionally, feel free to share any preferences or concerns about the methods used, so the approach fits your needs comfortably.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # For pd.notna\n",
    "\n",
    "# 'interview_train_dataset' and 'interview_eval_dataset' are from the previous cell\n",
    "# The 'tokenizer' is globally defined in a prior cell (Cell 6, id=\"c1180838\")\n",
    "\n",
    "def convert_csv_to_chat_format(examples):\n",
    "    \"\"\"\n",
    "    Converts a batch of examples from the CSV structure to a list of conversations.\n",
    "    Each conversation is a list of dictionaries with \"role\" and \"content\".\n",
    "    \"\"\"\n",
    "    all_conversations = []\n",
    "    # 'examples' is a dictionary where keys are column names and values are lists of entries\n",
    "    num_examples = len(examples['instruction'])\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        instruction = examples['instruction'][i]\n",
    "        input_text = examples['input'][i]\n",
    "        output_text = examples['output'][i]\n",
    "\n",
    "        # Ensure input_text is a string; handle None or NaN by treating as empty string if necessary\n",
    "        input_text_str = str(input_text) if pd.notna(input_text) and str(input_text).strip() else \"\"\n",
    "\n",
    "        current_conversation = []\n",
    "        # System prompt from 'instruction'\n",
    "        current_conversation.append({\"role\": \"system\", \"content\": instruction})\n",
    "        \n",
    "        # User message from 'input'\n",
    "        # Based on the dataset, 'input' should always be present.\n",
    "        # If input_text_str is empty, this will add a user message with empty content.\n",
    "        # This is generally fine as the model should learn to handle it or it implies\n",
    "        # the system prompt itself is the query.\n",
    "        current_conversation.append({\"role\": \"user\", \"content\": input_text_str})\n",
    "            \n",
    "        # Assistant message from 'output'\n",
    "        current_conversation.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "        \n",
    "        all_conversations.append(current_conversation)\n",
    "        \n",
    "    return {\"conversations\": all_conversations}\n",
    "\n",
    "def apply_template_to_conversations(examples):\n",
    "    \"\"\"\n",
    "    Applies the tokenizer's chat template to a batch of conversations.\n",
    "    Creates a 'text' field for SFTTrainer.\n",
    "    \"\"\"\n",
    "    # tokenizer should be globally available\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            examples[\"conversations\"], # This is a list of conversations\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False, # Crucial for training\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Process training data\n",
    "# 1. Convert CSV structure to list of message dicts\n",
    "train_dataset_with_conversations = interview_train_dataset.map(\n",
    "    convert_csv_to_chat_format,\n",
    "    batched=True,\n",
    "    remove_columns=interview_train_dataset.column_names # Keep only 'conversations'\n",
    ")\n",
    "# 2. Apply chat template to create the 'text' field\n",
    "final_train_dataset = train_dataset_with_conversations.map(\n",
    "    apply_template_to_conversations,\n",
    "    batched=True,\n",
    "    remove_columns=[\"conversations\"] # Keep only 'text'\n",
    ")\n",
    "# 3. Shuffle the training dataset\n",
    "final_train_dataset = final_train_dataset.shuffle(seed=3407)\n",
    "\n",
    "# Process evaluation data (if it exists)\n",
    "final_eval_dataset = None\n",
    "if interview_eval_dataset:\n",
    "    eval_dataset_with_conversations = interview_eval_dataset.map(\n",
    "        convert_csv_to_chat_format,\n",
    "        batched=True,\n",
    "        remove_columns=interview_eval_dataset.column_names\n",
    "    )\n",
    "    final_eval_dataset = eval_dataset_with_conversations.map(\n",
    "        apply_template_to_conversations,\n",
    "        batched=True,\n",
    "        remove_columns=[\"conversations\"]\n",
    "    )\n",
    "    # No need to shuffle eval_dataset\n",
    "\n",
    "print(\"Sample of final formatted training data (after chat template):\")\n",
    "if len(final_train_dataset) > 0:\n",
    "    print(final_train_dataset[0]['text'])\n",
    "else:\n",
    "    print(\"Training dataset is empty after processing.\")\n",
    "\n",
    "if final_eval_dataset and len(final_eval_dataset) > 0:\n",
    "    print(\"\\nSample of final formatted evaluation data (after chat template):\")\n",
    "    print(final_eval_dataset[0]['text'])\n",
    "elif interview_eval_dataset: # If interview_eval_dataset existed but final_eval_dataset is empty\n",
    "    print(\"\\nEvaluation dataset is empty after processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "109c736c8b99496e8721f9595c54eb6c",
      "f89daea726144ded892113a161649b64",
      "9f0108d9201f40599facfccf668a6e84",
      "927c60cd1f0d4a32a1ff9427a96a3246",
      "fb6f38d9dd1e49ec8fdfb7ca7ea363a2",
      "6303ec778b1d49dd980542a191261961",
      "5d66a72e82e54d9e8367ca8a2469dd0a",
      "e17016c458a14736bd56ba55d7168f32",
      "1d31954fe1804140bcac71e3d4102fdc",
      "a7077f3352b246ddbbca391c08c48b4a",
      "a1c374126fb144e68060e4fedab51046"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "4aee3090-cf58-4497-979b-cd41cbc5fd62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhongtai91\u001b[0m (\u001b[33mhongtai91-n-a\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tai/nlp/wandb/run-20250616_044523-7c24r3z5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5' target=\"_blank\">base-model-training-syntheic_v2_1</a></strong> to <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c60c9dc965e4d0f96ea847551e96783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=256):   0%|          | 0/4521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 238. Reducing num_proc to 238 for dataset of size 238.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e33276380e4ac19ab7003fa7fb8eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=238):   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"distress-chatbot\", name=\"base-model-training-syntheic_v2_1\", config={\n",
    "    \"model\": \"Qwen/Qwen3-4B\",\n",
    "    # \"max_steps\": 20000,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lambda_decay\": 0.95,\n",
    "})  # Allow resuming W&B run\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = final_train_dataset, # Use the processed training data\n",
    "    eval_dataset = final_eval_dataset,   # Use the processed evaluation data\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\", # Column containing the formatted text\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps = 60, # Adjusted for quicker testing, was 30. Set to None or higher for full training.\n",
    "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        # evaluation_strategy = \"steps\" if final_eval_dataset else \"no\", # Enable evaluation if eval_dataset is present\n",
    "        # eval_steps = 20, # Evaluate every N steps, adjust as needed\n",
    "    ),\n",
    ")\n",
    "\n",
    "# If using evaluation, you might want to set evaluation_strategy and eval_steps in SFTConfig\n",
    "if final_eval_dataset:\n",
    "    trainer.args.evaluation_strategy = \"steps\"\n",
    "    trainer.args.eval_steps = 20 # Or any other desired frequency\n",
    "else:\n",
    "    trainer.args.evaluation_strategy = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "344c14cb-0138-4981-9d85-b1e8856093ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA L40S. Max memory = 44.418 GB.\n",
      "31.408 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9fa371ShyhB"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "4b644b12-626b-45c7-fb11-89825ae13bd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,521 | Num Epochs = 3 | Total steps = 849\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,000,000,000 (3.30% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='849' max='849' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [849/849 41:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.915300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.773100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.778600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.727100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.313800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.312400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.304600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>1.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>1.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>1.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>1.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>1.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>1.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>1.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>1.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>1.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>1.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>1.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>1.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>1.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>1.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>1.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>1.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>1.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>1.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>1.309900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>1.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>1.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>1.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>1.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>1.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>1.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>1.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>1.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>1.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>1.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>1.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>1.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>1.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>1.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>1.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>1.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>1.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>1.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>1.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>1.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>1.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>1.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>1.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>1.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>1.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>1.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>1.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>1.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>1.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>1.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>1.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>1.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>1.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>1.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>1.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>1.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>1.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>1.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>1.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>1.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>1.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>1.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>1.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>1.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>1.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>1.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>1.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>1.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>1.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>1.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>1.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>1.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>1.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>1.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>1.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>1.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>1.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>1.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>1.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>1.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>1.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>1.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>1.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>1.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>1.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>1.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>1.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>1.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>1.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>1.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>1.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>1.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>1.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>1.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>1.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>1.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>1.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>1.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>1.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>1.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>1.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>1.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>1.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>1.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>1.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>1.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>1.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>1.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>1.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>1.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>1.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>1.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>1.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>1.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>1.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>1.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>1.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>1.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>1.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>1.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>1.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>1.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>1.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>1.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>1.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>1.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>1.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>1.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>1.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>1.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>1.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>1.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>1.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>1.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>1.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>1.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>1.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>1.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>1.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>1.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>1.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>1.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>1.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>1.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>1.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>1.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>1.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>1.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>1.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>1.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>1.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>1.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>1.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>1.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>1.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>1.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>1.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>1.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>1.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>1.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>1.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>1.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>1.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>1.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>1.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>1.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>1.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>1.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>1.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>1.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>1.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>1.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>1.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>1.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>1.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>1.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>1.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>1.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>1.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>1.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>1.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>1.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>1.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>1.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>1.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>1.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>1.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>1.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>1.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>1.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>1.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>1.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>1.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>1.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>1.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>1.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>1.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>1.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>1.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>1.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>1.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>1.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>1.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>1.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>1.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>1.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>1.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>1.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>1.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>1.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>1.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>1.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>1.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>1.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>1.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>1.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>1.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>1.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>1.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>1.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>1.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>1.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>1.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>1.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>1.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>1.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>1.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>1.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>1.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>1.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>1.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>1.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>1.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>1.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>1.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>1.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>1.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>1.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>1.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>1.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>1.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>1.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>1.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>1.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>1.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>1.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>1.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>1.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>1.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>1.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>1.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>1.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>1.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>1.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>1.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>1.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>1.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>1.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>1.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>1.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>1.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>1.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>1.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>1.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>1.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>1.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>1.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>1.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>1.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>1.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>1.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>1.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>1.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>1.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>1.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>1.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>1.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>1.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>1.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>1.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>1.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>1.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>1.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>1.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>1.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>1.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>1.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>1.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>1.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>1.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>1.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>1.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>1.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>1.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>1.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>1.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>1.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>1.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>1.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>1.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>1.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>1.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>1.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>1.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>1.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>1.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>1.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>1.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>1.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>1.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>1.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>1.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>1.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>1.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>1.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>1.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>1.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>1.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>1.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>1.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>1.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>1.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>1.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>1.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>1.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>1.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>1.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>1.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>1.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>1.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>1.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>1.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>1.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>1.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>1.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>1.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>1.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>1.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>1.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>1.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>1.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>1.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>1.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>1.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>1.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>1.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>1.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>1.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>1.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>1.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>1.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>1.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>1.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>1.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>1.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>1.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>1.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>1.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>1.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>1.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>1.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>1.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>1.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>1.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>1.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>1.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>1.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>1.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>1.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>1.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>1.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>1.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>1.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>1.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>1.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>1.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>1.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>1.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>1.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>1.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>1.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>1.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>1.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>1.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>1.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>1.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>1.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>1.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>1.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>1.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>1.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>1.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>1.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>1.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>1.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>1.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>1.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>1.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>1.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>1.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>1.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>1.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>1.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>1.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>1.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>1.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>1.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>1.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>1.169500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_model_v2_1/tokenizer_config.json',\n",
       " 'trained_model_v2_1/special_tokens_map.json',\n",
       " 'trained_model_v2_1/chat_template.jinja',\n",
       " 'trained_model_v2_1/vocab.json',\n",
       " 'trained_model_v2_1/merges.txt',\n",
       " 'trained_model_v2_1/added_tokens.json',\n",
       " 'trained_model_v2_1/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"trained_model_v2_1\")  # Local saving\n",
    "tokenizer.save_pretrained(\"trained_model_v2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning memory for next round of training\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-finetuning for formatting alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 60 examples.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the custom CSV dataset\n",
    "formatting_interview_dataset = load_dataset(\"csv\", data_files=\"./dataset/generated_responses_60_samples.csv\", split=\"train\")\n",
    "print(f\"Loaded dataset with {len(formatting_interview_dataset)} examples.\")\n",
    "interview_train_dataset = formatting_interview_dataset\n",
    "interview_eval_dataset = None  # No evaluation set for this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of final formatted training data (after chat template):\n",
      "<|im_start|>system\n",
      "You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. At the end of answer, add tag <evaluate>{\"Active Listening\" : score, \"Empathy & Validation\": score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": explain} </evaluate> evaluate your consultant answer in 7 metrics and explain for that evaluation with score from 1 to 10 in json format, where 1 is the worst and 10 is the best and explain is clearly explain why has that score. \n",
      "\n",
      "Consultation Metrics:\n",
      "1. Active Listening: Responses should show careful consideration of the user's concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
      "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user's feelings and emotions without being dismissive or minimizing their experiences.\n",
      "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
      "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
      "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
      "6. Boundaries & Ethical: It's vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
      "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tôi đã phải vật lộn với những cảm xúc giận dữ, buồn bã và thất vọng khi cố gắng giải quyết việc phân chia tài sản của mẹ tôi đã qua đời. Tình huống trở nên phức tạp hơn do sự thay đổi trong mối quan hệ gia đình và sự tham gia của các vợ chồng. Anh trai tôi muốn tổ chức một buổi bán đồ trong gara, nhưng mỗi người đều có quan điểm và sự gắn bó cảm xúc riêng với các món đồ trong tài sản. Tôi đã gặp rất nhiều khó khăn khi đối mặt với vấn đề này, đặc biệt là khi chị gái tôi đã lấy một số đồ cá nhân trước khi chúng tôi bắt đầu thảo luận về tài sản. Tôi cảm thấy bối rối và không biết làm thế nào để tiến lên phía trước.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Tôi rất hiểu rằng việc phân chia tài sản sau khi mất người thân không chỉ là vấn đề vật chất mà còn chất chứa nhiều cảm xúc phức tạp như giận dữ, buồn bã và thất vọng. Mỗi món đồ trong tài sản đều mang ý nghĩa riêng và sự gắn bó cảm xúc đặc biệt, khiến cho quá trình này trở nên khó khăn hơn rất nhiều. Việc chị gái bạn đã lấy đồ trước khi thảo luận cũng làm tăng thêm cảm giác bối rối và có thể dẫn đến những bất đồng trong gia đình.\n",
      "\n",
      "Trong trường hợp này, bạn có thể cân nhắc một số cách tiếp cận sau để giúp mọi việc trở nên suôn sẻ hơn:\n",
      "\n",
      "1. **Giao tiếp cởi mở và tôn trọng:** Cố gắng tổ chức một cuộc họp gia đình nơi mọi người có thể chia sẻ quan điểm và cảm xúc của mình một cách trung thực, tôn trọng lẫn nhau. Điều này có thể giúp giảm căng thẳng và hiểu nhau hơn.\n",
      "\n",
      "2. **Lập danh sách tài sản và mong muốn:** Ghi lại những món đồ quan trọng đối với từng người và mong muốn của họ về cách phân chia để có cơ sở thảo luận cụ thể.\n",
      "\n",
      "3. **Thỏa thuận về cách xử lý tài sản:** Nếu có thể, đồng ý với nhau về các phương thức giải quyết như bán đồ, giữ lại hoặc chia theo giá trị cảm xúc nhằm tạo sự công bằng.\n",
      "\n",
      "4. **Tìm sự hỗ trợ từ bên thứ ba:** Nếu việc thảo luận trở nên căng thẳng hoặc không thể giải quyết, bạn có thể xem xét nhờ đến sự giúp đỡ của một người trung gian, như luật sư hoặc chuyên gia hòa giải gia đình, để đảm bảo mọi người đều được nghe và quyền lợi được bảo vệ.\n",
      "\n",
      "5. **Chăm sóc bản thân:** Không quên dành thời gian để chăm sóc cảm xúc của chính mình trong quá trình này bằng cách chia sẻ với người tin cậy hoặc tham gia các hoạt động giúp giảm stress.\n",
      "\n",
      "Tôi hy vọng những gợi ý trên có thể giúp bạn cảm thấy bớt bối rối và tìm được hướng đi phù hợp cho gia đình mình. Nếu bạn muốn, tôi sẵn lòng lắng nghe thêm để cùng bạn tìm những cách giải quyết phù hợp nhất.\n",
      "\n",
      "<evaluate>{\"Active Listening\":9,\"Empathy & Validation\":9,\"Safety & Trustworthiness\":10,\"Open-mindedness & Non-judgment\":9,\"Clarity & Encouragement\":9,\"Boundaries & Ethical\":8,\"Holistic Approach\":9,\"Explaination for Scoring\":\"Câu trả lời thể hiện rõ sự chú ý đến khó khăn và cảm xúc của bạn, đồng thời thể hiện sự thấu hiểu và cảm thông sâu sắc. Các đề xuất được đưa ra rõ ràng, giúp bạn có những lựa chọn phù hợp và an toàn. Câu trả lời không mang tính phán xét và khuyến khích sự giao tiếp tích cực. Tuy nhiên, câu trả lời có thể nhấn mạnh thêm vai trò hỗ trợ của chuyên gia để đảm bảo giới hạn chuyên môn được rõ ràng hơn.\"} </evaluate><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # For pd.notna\n",
    "\n",
    "# 'interview_train_dataset' and 'interview_eval_dataset' are from the previous cell\n",
    "# The 'tokenizer' is globally defined in a prior cell (Cell 6, id=\"c1180838\")\n",
    "\n",
    "instruction_for_formatting = \"\"\"You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. At the end of answer, add tag <evaluate>{\"Active Listening\" : score, \"Empathy & Validation\": score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": explain} </evaluate> evaluate your consultant answer in 7 metrics and explain for that evaluation with score from 1 to 10 in json format, where 1 is the worst and 10 is the best and explain is clearly explain why has that score. \\n\\nConsultation Metrics:\\n1. Active Listening: Responses should show careful consideration of the user's concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\\n2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user's feelings and emotions without being dismissive or minimizing their experiences.\\n3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\\n4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\\n5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\\n6. Boundaries & Ethical: It's vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\\n7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\"\"\"\n",
    "\n",
    "def convert_csv_to_chat_format(examples):\n",
    "    \"\"\"\n",
    "    Converts a batch of examples from the CSV structure to a list of conversations.\n",
    "    Each conversation is a list of dictionaries with \"role\" and \"content\".\n",
    "    \"\"\"\n",
    "    all_conversations = []\n",
    "    # 'examples' is a dictionary where keys are column names and values are lists of entries\n",
    "    num_examples = len(examples['instruction'])\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        instruction = instruction_for_formatting\n",
    "        input_text = examples['input'][i]\n",
    "        output_text = examples['output'][i]\n",
    "\n",
    "        # Ensure input_text is a string; handle None or NaN by treating as empty string if necessary\n",
    "        input_text_str = str(input_text) if pd.notna(input_text) and str(input_text).strip() else \"\"\n",
    "\n",
    "        current_conversation = []\n",
    "        # System prompt from 'instruction'\n",
    "        current_conversation.append({\"role\": \"system\", \"content\": instruction})\n",
    "        \n",
    "        # User message from 'input'\n",
    "        # Based on the dataset, 'input' should always be present.\n",
    "        # If input_text_str is empty, this will add a user message with empty content.\n",
    "        # This is generally fine as the model should learn to handle it or it implies\n",
    "        # the system prompt itself is the query.\n",
    "        current_conversation.append({\"role\": \"user\", \"content\": input_text_str})\n",
    "            \n",
    "        # Assistant message from 'output'\n",
    "        current_conversation.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "        \n",
    "        all_conversations.append(current_conversation)\n",
    "        \n",
    "    return {\"conversations\": all_conversations}\n",
    "\n",
    "def apply_template_to_conversations(examples):\n",
    "    \"\"\"\n",
    "    Applies the tokenizer's chat template to a batch of conversations.\n",
    "    Creates a 'text' field for SFTTrainer.\n",
    "    \"\"\"\n",
    "    # tokenizer should be globally available\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            examples[\"conversations\"], # This is a list of conversations\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False, # Crucial for training\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Process training data\n",
    "# 1. Convert CSV structure to list of message dicts\n",
    "train_dataset_with_conversations = interview_train_dataset.map(\n",
    "    convert_csv_to_chat_format,\n",
    "    batched=True,\n",
    "    remove_columns=interview_train_dataset.column_names # Keep only 'conversations'\n",
    ")\n",
    "# 2. Apply chat template to create the 'text' field\n",
    "final_train_dataset = train_dataset_with_conversations.map(\n",
    "    apply_template_to_conversations,\n",
    "    batched=True,\n",
    "    remove_columns=[\"conversations\"] # Keep only 'text'\n",
    ")\n",
    "# 3. Shuffle the training dataset\n",
    "final_train_dataset = final_train_dataset.shuffle(seed=3407)\n",
    "\n",
    "# Process evaluation data (if it exists)\n",
    "final_eval_dataset = None\n",
    "if interview_eval_dataset:\n",
    "    eval_dataset_with_conversations = interview_eval_dataset.map(\n",
    "        convert_csv_to_chat_format,\n",
    "        batched=True,\n",
    "        remove_columns=interview_eval_dataset.column_names\n",
    "    )\n",
    "    final_eval_dataset = eval_dataset_with_conversations.map(\n",
    "        apply_template_to_conversations,\n",
    "        batched=True,\n",
    "        remove_columns=[\"conversations\"]\n",
    "    )\n",
    "    # No need to shuffle eval_dataset\n",
    "\n",
    "print(\"Sample of final formatted training data (after chat template):\")\n",
    "if len(final_train_dataset) > 0:\n",
    "    print(final_train_dataset[0]['text'])\n",
    "else:\n",
    "    print(\"Training dataset is empty after processing.\")\n",
    "\n",
    "if final_eval_dataset and len(final_eval_dataset) > 0:\n",
    "    print(\"\\nSample of final formatted evaluation data (after chat template):\")\n",
    "    print(final_eval_dataset[0]['text'])\n",
    "elif interview_eval_dataset: # If interview_eval_dataset existed but final_eval_dataset is empty\n",
    "    print(\"\\nEvaluation dataset is empty after processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▂▂▃▃▃▃▄▄▆▅▆▆▆▆▆▆▆▅▆▇▇▇▆▆▆██▇▇▇█▇█▆▇▇▇▇</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.6927397422053274e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>849</td></tr><tr><td>train/grad_norm</td><td>0.82329</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1695</td></tr><tr><td>train_loss</td><td>1.22504</td></tr><tr><td>train_runtime</td><td>2519.1219</td></tr><tr><td>train_samples_per_second</td><td>5.384</td></tr><tr><td>train_steps_per_second</td><td>0.337</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">base-model-training-syntheic_v2_1</strong> at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/7c24r3z5</a><br> View project at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250616_044523-7c24r3z5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tai/nlp/wandb/run-20250616_053525-bvk4i4hh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/bvk4i4hh' target=\"_blank\">base-model-training-syntheic_v2_2-formatting</a></strong> to <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/bvk4i4hh' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/bvk4i4hh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 60. Reducing num_proc to 60 for dataset of size 60.\n"
     ]
    }
   ],
   "source": [
    "# Traing the model for formatting\n",
    "import wandb\n",
    "wandb.init(project=\"distress-chatbot\", name=\"base-model-training-syntheic_v2_2-formatting\", config={\n",
    "    \"model\": \"Qwen/Qwen3-4B\",\n",
    "    # \"max_steps\": 20000,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lambda_decay\": 0.95,\n",
    "})  # Allow resuming W&B run\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = final_train_dataset, # Use the processed training data\n",
    "    eval_dataset = final_eval_dataset,   # Use the processed evaluation data\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\", # Column containing the formatted text\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 5, # Set this more for learning formatting. \n",
    "        # max_steps = 60, # Adjusted for quicker testing, was 30. Set to None or higher for full training.\n",
    "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        # evaluation_strategy = \"steps\" if final_eval_dataset else \"no\", # Enable evaluation if eval_dataset is present\n",
    "        # eval_steps = 20, # Evaluate every N steps, adjust as needed\n",
    "    ),\n",
    ")\n",
    "\n",
    "# If using evaluation, you might want to set evaluation_strategy and eval_steps in SFTConfig\n",
    "if final_eval_dataset:\n",
    "    trainer.args.evaluation_strategy = \"steps\"\n",
    "    trainer.args.eval_steps = 20 # Or any other desired frequency\n",
    "else:\n",
    "    trainer.args.evaluation_strategy = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 60 | Num Epochs = 5 | Total steps = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,000,000,000 (3.30% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.545700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.468500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.369500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_model_v2_1_formatting/tokenizer_config.json',\n",
       " 'trained_model_v2_1_formatting/special_tokens_map.json',\n",
       " 'trained_model_v2_1_formatting/chat_template.jinja',\n",
       " 'trained_model_v2_1_formatting/vocab.json',\n",
       " 'trained_model_v2_1_formatting/merges.txt',\n",
       " 'trained_model_v2_1_formatting/added_tokens.json',\n",
       " 'trained_model_v2_1_formatting/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving model and tokenizer\n",
    "model.save_pretrained(\"trained_model_v2_1_formatting\")  # Local saving\n",
    "tokenizer.save_pretrained(\"trained_model_v2_1_formatting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning with Group Relative Policy Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear accelerator state and reinitialize\n",
    "# import torch\n",
    "import gc\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "# Clear CUDA cache\n",
    "# torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# # Reset accelerator state\n",
    "# try:\n",
    "#     from accelerate.state import AcceleratorState\n",
    "#     AcceleratorState._reset_state(reset_partial_state=True)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Reinitialize accelerator\n",
    "# accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt:\n",
      "You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\n",
      "Provide helpful, comprehensive, and appropriate answers to the user questions.\n",
      "\n",
      "After your counselling response, you must include a self-evaluation in the following format:\n",
      "<evaluate>\n",
      "{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\n",
      "</evaluate>\n",
      "\n",
      "Where score is a number from 1-10, and provide a clear explanation for your scoring.\n",
      "Explain to metrics:\n",
      "1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
      "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\n",
      "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
      "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
      "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
      "6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
      "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\n"
     ]
    }
   ],
   "source": [
    "# Define the evaluation format markers\n",
    "evaluation_start = \"<evaluate>\"\n",
    "evaluation_end = \"</evaluate>\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\n",
    "Provide helpful, comprehensive, and appropriate answers to the user questions.\n",
    "\n",
    "After your counselling response, you must include a self-evaluation in the following format:\n",
    "<evaluate>\n",
    "{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\n",
    "</evaluate>\n",
    "\n",
    "Where score is a number from 1-10, and provide a clear explanation for your scoring.\n",
    "Explain to metrics:\n",
    "1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
    "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\n",
    "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
    "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
    "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
    "6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
    "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"System prompt:\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chat template\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with our specific template:\n",
    "chat_template = chat_template.replace(\"'{system_prompt}'\", f\"'{system_prompt}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted text:\n",
      "You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\n",
      "Provide helpful, comprehensive, and appropriate answers to the user questions.\n",
      "\n",
      "After your counselling response, you must include a self-evaluation in the following format:\n",
      "<evaluate>\n",
      "{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\n",
      "</evaluate>\n",
      "\n",
      "Where score is a number from 1-10, and provide a clear explanation for your scoring.\n",
      "Explain to metrics:\n",
      "1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\n",
      "2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\n",
      "3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\n",
      "4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\n",
      "5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\n",
      "6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\n",
      "7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.<|im_end|>I'm feeling anxious about work.I understand that work anxiety can be challenging. Let me help you explore some strategies. <evaluate>{\"Active Listening\" : 8, \"Empathy & Validation\": 9, \"Safety & Trustworthiness\" : 9, \"Open-mindedness & Non-judgment\" : 8, \"Clarity & Encouragement\" : 7, \"Boundaries & Ethical\" : 9, \"Holistic Approach\" : 8, \"Explaination for Scoring\": \"Provided empathetic response with good listening skills.\"}</evaluate><|im_end|>Can you suggest some techniques?\n"
     ]
    }
   ],
   "source": [
    "# Test the chat template\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm feeling anxious about work.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I understand that work anxiety can be challenging. Let me help you explore some strategies. <evaluate>{\\\"Active Listening\\\" : 8, \\\"Empathy & Validation\\\": 9, \\\"Safety & Trustworthiness\\\" : 9, \\\"Open-mindedness & Non-judgment\\\" : 8, \\\"Clarity & Encouragement\\\" : 7, \\\"Boundaries & Ethical\\\" : 9, \\\"Holistic Approach\\\" : 8, \\\"Explaination for Scoring\\\": \\\"Provided empathetic response with good listening skills.\\\"}</evaluate>\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you suggest some techniques?\"}\n",
    "]\n",
    "\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Formatted text:\")\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4759, 3)\n",
      "Columns: ['instruction', 'input', 'output']\n",
      "\n",
      "First few rows:\n",
      "                                         instruction  \\\n",
      "0  You are a helpful mental health counselling as...   \n",
      "1  You are a helpful mental health counselling as...   \n",
      "2  You are a helpful mental health counselling as...   \n",
      "3  You are a helpful mental health counselling as...   \n",
      "4  You are a helpful mental health counselling as...   \n",
      "\n",
      "                                               input  \\\n",
      "0  I've been struggling with alcohol use for a wh...   \n",
      "1  我希望通过这次咨询，能更好地理解自己目前的情绪状态，并找到缓解压力和焦虑的方法。最近几个月，...   \n",
      "2  Tôi mong muốn qua buổi tư vấn này, tôi có thể ...   \n",
      "3  أرغب في مناقشة موضوع حساس يخص حياتي الشخصية وأ...   \n",
      "4  最近我一直感到很难过和孤独，特别是因为我刚刚失去了我的祖母。她在我生活中一直是个非常重要的支...   \n",
      "\n",
      "                                              output  \n",
      "0  Managing alcohol use involves understanding th...  \n",
      "1  在心理咨询过程中，保护您的隐私是非常重要的，所有您分享的信息都会严格保密，除非涉及您或他人的...  \n",
      "2  Kiểm soát cảm giác lo lắng bắt đầu bằng việc n...  \n",
      "3  التعامل مع المشاعر المتعلقة بالهوية الجنسية في...  \n",
      "4  失去亲人带来的情绪反应非常复杂，您所描述的身体症状如胸口压抑、头痛和胃部不适，确实可能是悲伤...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"./dataset/stage_2_1_synthetic_interview_data_combined.csv\", split=\"train\")\n",
    "dataset = dataset.to_pandas()\n",
    "\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"Columns: {dataset.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      "[{'role': 'system', 'content': 'You are a helpful mental health counselling assistant. Please answer mental health questions based on the patient description.\\nProvide helpful, comprehensive, and appropriate answers to the user questions.\\n\\nAfter your counselling response, you must include a self-evaluation in the following format:\\n<evaluate>\\n{\"Active Listening\" : score, \"Empathy & Validation\" : score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": \"Your explanation here\"}\\n</evaluate>\\n\\nWhere score is a number from 1-10, and provide a clear explanation for your scoring.\\nExplain to metrics:\\n1. Active Listening: Responses should show careful consideration of the user concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\\n2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user feelings and emotions without being dismissive or minimizing their experiences.\\n3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\\n4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\\n5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\\n6. Boundaries & Ethical: It is vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\\n7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.'}, {'role': 'user', 'content': \"I've been struggling with alcohol use for a while now, and I want to find a way to manage it better or possibly quit altogether. I often feel overwhelmed and anxious, especially after work, which leads me to drink more than I intend. These feelings started a couple of years ago after some family issues and increased work stress. For example, last month I had a big argument with my spouse about my drinking, and since then, I've felt even more isolated and stressed. I notice I drink almost every evening, sometimes heavily, and it affects my sleep and mood the next day. My family dynamics have always been complicated, with high expectations and little room for open emotional expression, which makes it hard to talk about what I'm going through. In the past, I've tried to cut back on drinking by distracting myself with hobbies or exercise, but these only help temporarily. I am curious about what approaches in counseling might help me develop healthier coping mechanisms and how therapy addresses substance use issues. Can you explain how the process works and what to expect in terms of treatment options?\"}]\n",
      "\n",
      "Sample answer:\n",
      "Managing alcohol use involves understanding the triggers and emotions that lead to drinking, which you have started to identify well. Focusing on the situations and feelings that increase your urge to drink, such as stress and family conflicts, can help create strategies to respond differently. Developing healthier coping methods like mindfulness, relaxation techniques, or structured routines might reduce reliance on alcohol for relief. Exploring the impact of family dynamics and emotional expression together can also provide insight and foster healing in those relationships. The counseling process typically includes setting personalized goals, learning skills to manage cravings and emotions, and gradually building resilience against relapse. Sessions are a safe space to discuss experiences without judgment, and treatment approaches can range from behavioral therapies to support groups, tailored to your preferences and needs. Regular check-ins help track progress and adjust strategies as needed. Questions about specific techniques or concerns about commitment and confidentiality are important to address as well, ensuring the approach aligns with your comfort and goals.\n"
     ]
    }
   ],
   "source": [
    "def format_dataset_for_grpo(x):\n",
    "    instruction = x[\"instruction\"]\n",
    "    user_input = x[\"input\"] if pd.notna(x[\"input\"]) else \"\"\n",
    "    \n",
    "    # Create the conversation format\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "\n",
    "# Prepare dataset for GRPO\n",
    "dataset[\"prompt\"] = dataset.apply(format_dataset_for_grpo, axis=1)\n",
    "dataset[\"answer\"] = dataset[\"output\"]  # Use the original answer as reference\n",
    "\n",
    "print(\"Sample prompt:\")\n",
    "print(dataset[\"prompt\"][0])\n",
    "print(\"\\nSample answer:\")\n",
    "print(dataset[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing reward functions:\n",
      "Format check: [10.0]\n",
      "No extra text: [2.0]\n",
      "No repetition: [1.0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from langdetect import detect\n",
    "\n",
    "# Create regex to match the evaluation format\n",
    "evaluation_regex = re.compile(\n",
    "    rf\"{evaluation_start}(.+?){evaluation_end}\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def check_evaluation_format(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function for checking if the response follows the evaluation format exactly.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Check if evaluation format is present\n",
    "        if evaluation_regex.search(response) is not None:\n",
    "            score += 5.0  # High reward for format compliance\n",
    "            \n",
    "            # Extract and validate JSON structure\n",
    "            try:\n",
    "                match = evaluation_regex.search(response)\n",
    "                if match:\n",
    "                    json_content = match.group(1).strip()\n",
    "                    eval_data = json.loads(json_content)\n",
    "                    \n",
    "                    # Check for required keys\n",
    "                    required_keys = [\n",
    "                        \"Active Listening\", \"Empathy & Validation\", \"Safety & Trustworthiness\",\n",
    "                        \"Open-mindedness & Non-judgment\", \"Clarity & Encouragement\", \n",
    "                        \"Boundaries & Ethical\", \"Holistic Approach\", \"Explaination for Scoring\"\n",
    "                    ]\n",
    "                    \n",
    "                    if all(key in eval_data for key in required_keys):\n",
    "                        score += 3.0  # Bonus for complete structure\n",
    "                    \n",
    "                    # Check if scores are numbers between 1-10\n",
    "                    score_keys = required_keys[:-1]  # Exclude explanation\n",
    "                    valid_scores = 0\n",
    "                    for key in score_keys:\n",
    "                        if key in eval_data:\n",
    "                            try:\n",
    "                                score_val = float(eval_data[key])\n",
    "                                if 1 <= score_val <= 10:\n",
    "                                    valid_scores += 1\n",
    "                            except (ValueError, TypeError):\n",
    "                                pass\n",
    "                    \n",
    "                    # Bonus for valid scores\n",
    "                    score += (valid_scores / len(score_keys)) * 2.0\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                score -= 1.0  # Penalty for invalid JSON\n",
    "        else:\n",
    "            score -= 3.0  # Penalty for missing evaluation\n",
    "            \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_no_extra_text(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to ensure no extra text after evaluation.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Find the last occurrence of </evaluate>\n",
    "        last_eval_end = response.rfind(evaluation_end)\n",
    "        if last_eval_end != -1:\n",
    "            text_after = response[last_eval_end + len(evaluation_end):].strip()\n",
    "            if not text_after:  # No text after evaluation\n",
    "                score += 2.0\n",
    "            else:\n",
    "                score -= 2.0  # Penalty for extra text\n",
    "        \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_language_consistency(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if response language matches input language.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    question_lang = detect(question)\n",
    "    # print(str(responses))\n",
    "\n",
    "    for rep in responses:\n",
    "        score = 0\n",
    "        # print(f\"Current text for detect lang {rep} - finish\")\n",
    "        if len(rep) > 5:\n",
    "            if detect(rep) == question_lang:\n",
    "                score += 1.0\n",
    "            \n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "    \n",
    "\n",
    "\n",
    "def check_no_repetition(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to penalize repetitive text.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Simple repetition check: split into sentences and check for exact duplicates\n",
    "        sentences = re.split(r'[.!?]+', response)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 0:\n",
    "            unique_sentences = set(sentences)\n",
    "            repetition_ratio = 1 - (len(unique_sentences) / len(sentences))\n",
    "            \n",
    "            if repetition_ratio < 0.1:  # Less than 10% repetition\n",
    "                score += 1.0\n",
    "            elif repetition_ratio > 0.3:  # More than 30% repetition\n",
    "                score -= 2.0\n",
    "        \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Test reward functions\n",
    "test_completion = [[\n",
    "    {\"content\": \"I understand your concerns. <evaluate>{\\\"Active Listening\\\" : 8, \\\"Empathy & Validation\\\": 9, \\\"Safety & Trustworthiness\\\" : 9, \\\"Open-mindedness & Non-judgment\\\" : 8, \\\"Clarity & Encouragement\\\" : 7, \\\"Boundaries & Ethical\\\" : 9, \\\"Holistic Approach\\\" : 8, \\\"Explaination for Scoring\\\": \\\"Good response\\\"}</evaluate>\"}\n",
    "]]\n",
    "\n",
    "print(\"Testing reward functions:\")\n",
    "print(f\"Format check: {check_evaluation_format(test_completion)}\")\n",
    "print(f\"No extra text: {check_no_extra_text(test_completion)}\")\n",
    "print(f\"No repetition: {check_no_repetition(test_completion)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for monitoring\n",
    "PRINTED_TIMES = 0\n",
    "PRINT_EVERY_STEPS = 3\n",
    "\n",
    "def debug_responses(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Debug function to print responses every few steps.\n",
    "    \"\"\"\n",
    "    global PRINTED_TIMES, PRINT_EVERY_STEPS\n",
    "    \n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        user_query = prompts[0][-1][\"content\"]\n",
    "        response = completions[0][0][\"content\"]\n",
    "        \n",
    "        print('*' * 50)\n",
    "        print(f\"Step {PRINTED_TIMES + 1}\")\n",
    "        print(f\"User Query: {user_query[:100]}...\")\n",
    "        print(f\"Response: {response[:200]}...\")\n",
    "        \n",
    "        # Check if evaluation format is present\n",
    "        has_eval = evaluation_start in response and evaluation_end in response\n",
    "        print(f\"Has Evaluation Format: {has_eval}\")\n",
    "        \n",
    "        if has_eval:\n",
    "            match = evaluation_regex.search(response)\n",
    "            if match:\n",
    "                try:\n",
    "                    eval_content = match.group(1).strip()\n",
    "                    json.loads(eval_content)\n",
    "                    print(\"Evaluation JSON: Valid\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Evaluation JSON: Invalid\")\n",
    "        print('*' * 50)\n",
    "    \n",
    "    PRINTED_TIMES += 1\n",
    "    return [0] * len(completions)  # Return neutral scores for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get only 500 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1951ee298184a4cb506fdbdbdccf1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length (90th percentile): 871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad06d8f631e048b7a93920512a58454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 4285\n",
      "Training dataset size: 500\n"
     ]
    }
   ],
   "source": [
    "# Convert to HuggingFace dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas to dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset[[\"prompt\", \"answer\"]])\n",
    "\n",
    "# Calculate token lengths\n",
    "def calculate_prompt_length(examples):\n",
    "    lengths = []\n",
    "    for prompt in examples[\"prompt\"]:\n",
    "        tokens = tokenizer.apply_chat_template(\n",
    "            prompt, \n",
    "            add_generation_prompt=True, \n",
    "            tokenize=True\n",
    "        )\n",
    "        lengths.append(len(tokens))\n",
    "    return {\"prompt_length\": lengths}\n",
    "\n",
    "hf_dataset = hf_dataset.map(calculate_prompt_length, batched=True)\n",
    "\n",
    "# Filter to keep only reasonable length prompts (top 90%)\n",
    "max_length = int(np.quantile(hf_dataset[\"prompt_length\"], 0.9))\n",
    "print(f\"Max prompt length (90th percentile): {max_length}\")\n",
    "\n",
    "# Filter dataset\n",
    "filtered_dataset = hf_dataset.filter(lambda x: x[\"prompt_length\"] <= max_length)\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Take a subset for training (adjust as needed)\n",
    "if len(filtered_dataset) > 500:\n",
    "    training_dataset = filtered_dataset.shuffle(seed=3407).select(range(500))\n",
    "else:\n",
    "    training_dataset = filtered_dataset.shuffle(seed=3407)\n",
    "\n",
    "print(f\"Training dataset size: {len(training_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adhoc code solve the accelerator issue\n",
    "\n",
    "# from trl import GRPOConfig, GRPOTrainer\n",
    "# import torch\n",
    "\n",
    "# # Ensure model is on correct device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">base-model-training-syntheic_v2_2-grpo</strong> at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/5yey09mz' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/5yey09mz</a><br> View project at: <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250616_055224-5yey09mz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tai/nlp/wandb/run-20250616_055309-ylbq01w2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/ylbq01w2' target=\"_blank\">base-model-training-syntheic_v2_2-grpo</a></strong> to <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/ylbq01w2' target=\"_blank\">https://wandb.ai/hongtai91-n-a/distress-chatbot/runs/ylbq01w2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length: 921\n",
      "Max completion length: 3175\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"distress-chatbot\", name=\"base-model-training-syntheic_v2_2-grpo\", config={\n",
    "    \"model\": \"Qwen/Qwen3-4B\",\n",
    "    # \"max_steps\": 20000,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lambda_decay\": 0.95,\n",
    "})  # Allow resuming W&B run\n",
    "\n",
    "\n",
    "\n",
    "# Calculate max lengths\n",
    "max_prompt_length = max_length + 50  # Add some buffer\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "print(f\"Max prompt length: {max_prompt_length}\")\n",
    "print(f\"Max completion length: {max_completion_length}\")\n",
    "\n",
    "# VLLM sampling parameters\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    seed=3407,\n",
    "    stop=[tokenizer.eos_token],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "# GRPO training configuration\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params=vllm_sampling_params,\n",
    "    # temperature=0.8,\n",
    "    learning_rate=1e-6,  # Lower learning rate for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Increase for smoother training\n",
    "    num_generations=4,  # Number of responses to generate per prompt\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    max_steps=200,  # Start with fewer steps for testing\n",
    "    save_steps=50,\n",
    "    report_to=\"wandb\",  # Set to \"wandb\" if you want to use Weights & Biases\n",
    "    output_dir=\"trained_model_v2_2_grpo_checkpoint\",  # Directory to save the model\n",
    "    gradient_checkpointing = False,\n",
    "    # Add these parameters to handle device issues\n",
    "    # dataloader_pin_memory=False,\n",
    "    # dataloader_num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO Trainer initialized successfully!\n",
      "Training on 500 examples\n"
     ]
    }
   ],
   "source": [
    "# Initialize GRPO trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        check_evaluation_format,     # Primary reward: correct format\n",
    "        check_no_extra_text,         # Secondary: no extra text\n",
    "        check_language_consistency,  # Tertiary: language consistency\n",
    "        check_no_repetition,         # Quaternary: no repetition\n",
    "        debug_responses,             # Debug function\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    ")\n",
    "\n",
    "print(\"GRPO Trainer initialized successfully!\")\n",
    "print(f\"Training on {len(training_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GRPO training...\n",
      "Watch for the reward column to increase over time.\n",
      "The model should learn to follow the evaluation format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,000,000,000 (3.30% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/200 2:53:16 < 56:58, 0.01 it/s, Epoch 1.20/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / check_evaluation_format / mean</th>\n",
       "      <th>rewards / check_evaluation_format / std</th>\n",
       "      <th>rewards / check_no_extra_text / mean</th>\n",
       "      <th>rewards / check_no_extra_text / std</th>\n",
       "      <th>rewards / check_language_consistency / mean</th>\n",
       "      <th>rewards / check_language_consistency / std</th>\n",
       "      <th>rewards / check_no_repetition / mean</th>\n",
       "      <th>rewards / check_no_repetition / std</th>\n",
       "      <th>rewards / debug_responses / mean</th>\n",
       "      <th>rewards / debug_responses / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>-1.562500</td>\n",
       "      <td>2.333961</td>\n",
       "      <td>106.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>7.634683</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>2.461876</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>5.490171</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>-2.812500</td>\n",
       "      <td>0.269338</td>\n",
       "      <td>5.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>8.762948</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>-2.437500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>49.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>3.269575</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>0.394338</td>\n",
       "      <td>9.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>11.185880</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.683013</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>4.918156</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>0.394338</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>10.031704</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>-1.062500</td>\n",
       "      <td>2.269338</td>\n",
       "      <td>70.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>4.663632</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>-2.062500</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>589.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>589.000000</td>\n",
       "      <td>5.867577</td>\n",
       "      <td>-2.562500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>-1.937500</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>31.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>8.908331</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>111.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>2.546899</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.283800</td>\n",
       "      <td>-2.687500</td>\n",
       "      <td>0.489357</td>\n",
       "      <td>14.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>7.095400</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>-2.312500</td>\n",
       "      <td>0.489357</td>\n",
       "      <td>88.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>6.564149</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>0.769338</td>\n",
       "      <td>130.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>934.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>934.000000</td>\n",
       "      <td>6.609002</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>-2.312500</td>\n",
       "      <td>0.269338</td>\n",
       "      <td>134.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>5.882388</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>-2.125000</td>\n",
       "      <td>0.758694</td>\n",
       "      <td>33.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>6.641862</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>-2.562500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>41.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>5.557376</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>1.461279</td>\n",
       "      <td>164.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1650.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1650.000000</td>\n",
       "      <td>4.555993</td>\n",
       "      <td>-2.562500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.394338</td>\n",
       "      <td>446.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1355.000000</td>\n",
       "      <td>2.918376</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>2.340264</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>3.644440</td>\n",
       "      <td>166.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>3.613241</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>2.098423</td>\n",
       "      <td>261.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1489.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>261.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1489.000000</td>\n",
       "      <td>2.942325</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>4.635011</td>\n",
       "      <td>367.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>180.200012</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>996.000000</td>\n",
       "      <td>6.504827</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>3.795286</td>\n",
       "      <td>211.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>211.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>1.173874</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.614357</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>106.200005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>2.985033</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>2.205890</td>\n",
       "      <td>502.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>323.933350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1238.000000</td>\n",
       "      <td>4.163848</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>0.454124</td>\n",
       "      <td>220.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1244.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1244.000000</td>\n",
       "      <td>1.654500</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>6.427155</td>\n",
       "      <td>420.062500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1439.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>420.062500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1439.000000</td>\n",
       "      <td>0.960122</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.704124</td>\n",
       "      <td>164.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>3.472090</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>317.437500</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1907.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>317.437500</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1907.000000</td>\n",
       "      <td>0.692114</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.604640</td>\n",
       "      <td>348.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>348.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1105.000000</td>\n",
       "      <td>2.561059</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>2.165064</td>\n",
       "      <td>476.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2277.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>476.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2277.000000</td>\n",
       "      <td>0.870662</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>229.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>229.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>4.863557</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>2.022181</td>\n",
       "      <td>790.187500</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>449.500031</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>0.773418</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>4.284643</td>\n",
       "      <td>161.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>726.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>161.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>726.000000</td>\n",
       "      <td>3.085336</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>270.062500</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>906.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.062500</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>906.000000</td>\n",
       "      <td>2.449417</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2.657863</td>\n",
       "      <td>637.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>467.933350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2490.000000</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>3.480480</td>\n",
       "      <td>622.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>452.333344</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1516.000000</td>\n",
       "      <td>1.012073</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>2.329124</td>\n",
       "      <td>780.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>438.142883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1477.000000</td>\n",
       "      <td>1.144447</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.614357</td>\n",
       "      <td>449.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1547.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>449.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1547.000000</td>\n",
       "      <td>0.753962</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>299.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1273.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>299.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1273.000000</td>\n",
       "      <td>0.832281</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>5.590825</td>\n",
       "      <td>927.875000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>606.857178</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2276.000000</td>\n",
       "      <td>0.763108</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>770.062500</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2273.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>770.062500</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2273.000000</td>\n",
       "      <td>0.611783</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.957427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.703768</td>\n",
       "      <td>680.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>513.933350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1.025632</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>4.732864</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.544614</td>\n",
       "      <td>595.375000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>595.375000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>4.342649</td>\n",
       "      <td>766.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2390.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>766.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2390.000000</td>\n",
       "      <td>0.675314</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>6.517708</td>\n",
       "      <td>716.125000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1509.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>716.125000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1509.000000</td>\n",
       "      <td>0.575591</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>3.092890</td>\n",
       "      <td>943.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>795.066711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2453.000000</td>\n",
       "      <td>0.559137</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>3.587014</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>780.812500</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>780.812500</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>0.763792</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.056349</td>\n",
       "      <td>1125.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>989.266724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3089.000000</td>\n",
       "      <td>0.754486</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>4.044722</td>\n",
       "      <td>885.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>732.600037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2090.000000</td>\n",
       "      <td>0.630146</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.035258</td>\n",
       "      <td>725.687500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1863.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>725.687500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1863.000000</td>\n",
       "      <td>0.884829</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.670538</td>\n",
       "      <td>1024.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>717.571472</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1774.000000</td>\n",
       "      <td>0.562976</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3.825648</td>\n",
       "      <td>1511.250000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1127.307739</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>2245.000000</td>\n",
       "      <td>0.470095</td>\n",
       "      <td>-1.312500</td>\n",
       "      <td>3.842200</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>5.897181</td>\n",
       "      <td>1357.875000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1098.285767</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2484.000000</td>\n",
       "      <td>0.575227</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.146518</td>\n",
       "      <td>907.937500</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>584.071472</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1444.000000</td>\n",
       "      <td>0.486653</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>7.837117</td>\n",
       "      <td>1031.937500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2062.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1031.937500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2062.000000</td>\n",
       "      <td>0.583410</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>4.559584</td>\n",
       "      <td>1134.750000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>2565.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1134.750000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>2565.000000</td>\n",
       "      <td>0.486218</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>4.732864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>47.611300</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>3.962117</td>\n",
       "      <td>1238.500000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>791.615417</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>2143.000000</td>\n",
       "      <td>1190.282455</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>1430.562500</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1028.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2644.000000</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>8.300508</td>\n",
       "      <td>1533.875000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>986.833374</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>0.400838</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>6.029438</td>\n",
       "      <td>1027.937500</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>884.800049</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>0.567680</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>4.809279</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>1447.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3047.000000</td>\n",
       "      <td>0.417609</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>7.672719</td>\n",
       "      <td>1371.562500</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1113.928589</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2490.000000</td>\n",
       "      <td>1.589508</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>3.534758</td>\n",
       "      <td>1373.875000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>958.230835</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>2114.000000</td>\n",
       "      <td>0.444653</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>4.732864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.032796</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>7.146171</td>\n",
       "      <td>1693.437500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1351.538452</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2746.000000</td>\n",
       "      <td>0.470334</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>4.477113</td>\n",
       "      <td>1012.312500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>703.357178</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>0.491834</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.943000</td>\n",
       "      <td>1196.062500</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1064.133423</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>2577.000000</td>\n",
       "      <td>0.633914</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>7.237230</td>\n",
       "      <td>1295.062500</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1169.733398</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>2987.000000</td>\n",
       "      <td>0.400733</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>6.035658</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.454877</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.216270</td>\n",
       "      <td>1001.250000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>2381.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1001.250000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>2381.000000</td>\n",
       "      <td>0.539542</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>3.855410</td>\n",
       "      <td>1126.062500</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>989.466736</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1850.000000</td>\n",
       "      <td>0.487934</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>4.368040</td>\n",
       "      <td>1279.375000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1008.571472</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2711.000000</td>\n",
       "      <td>0.490473</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>7.955395</td>\n",
       "      <td>1100.750000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>962.466736</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>2030.000000</td>\n",
       "      <td>0.443177</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.822386</td>\n",
       "      <td>1630.500000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1409.857178</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>2701.000000</td>\n",
       "      <td>0.474122</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>6.559662</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.543805</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>6.443224</td>\n",
       "      <td>929.312500</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2989.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>929.312500</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2989.000000</td>\n",
       "      <td>0.611331</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>3.545286</td>\n",
       "      <td>1102.250000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>964.066711</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>0.508680</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>3.776234</td>\n",
       "      <td>870.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>716.333374</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1864.000000</td>\n",
       "      <td>0.699058</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>7.822103</td>\n",
       "      <td>867.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>867.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>1.849428</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>6.071450</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.170286</td>\n",
       "      <td>843.625000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>688.200012</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1319.000000</td>\n",
       "      <td>0.870499</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.085350</td>\n",
       "      <td>977.250000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>830.733398</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>2205.000000</td>\n",
       "      <td>0.541175</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>6.674781</td>\n",
       "      <td>1977.375000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1701.000122</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>2709.000000</td>\n",
       "      <td>0.410438</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>1.232143</td>\n",
       "      <td>4.556382</td>\n",
       "      <td>1094.250000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>3071.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1094.250000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>3071.000000</td>\n",
       "      <td>0.585237</td>\n",
       "      <td>-0.330357</td>\n",
       "      <td>4.948639</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>5.460350</td>\n",
       "      <td>1450.687500</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1204.357178</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>2171.000000</td>\n",
       "      <td>0.551458</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>2.144338</td>\n",
       "      <td>1366.562500</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1108.214355</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>2383.000000</td>\n",
       "      <td>0.497541</td>\n",
       "      <td>-2.187500</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>8.031361</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2634.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2634.000000</td>\n",
       "      <td>0.500588</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>5.549234</td>\n",
       "      <td>2049.625000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1674.500000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>3072.000000</td>\n",
       "      <td>0.442986</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>5.715112</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>7.357602</td>\n",
       "      <td>1410.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1158.785767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2232.000000</td>\n",
       "      <td>0.462563</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.880304</td>\n",
       "      <td>1391.812500</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1272.933350</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>2424.000000</td>\n",
       "      <td>0.460706</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>5.137465</td>\n",
       "      <td>1361.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1240.066772</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2856.000000</td>\n",
       "      <td>0.493323</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>3.607869</td>\n",
       "      <td>1088.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>949.733398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3137.000000</td>\n",
       "      <td>0.601887</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>4.440345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>6.180732</td>\n",
       "      <td>1403.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1285.600098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>0.510086</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.957427</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>6.205086</td>\n",
       "      <td>1525.500000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1144.846191</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>2574.000000</td>\n",
       "      <td>0.484691</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>6.794643</td>\n",
       "      <td>5.990813</td>\n",
       "      <td>1164.437500</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>2362.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1164.437500</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>2362.000000</td>\n",
       "      <td>0.424520</td>\n",
       "      <td>5.357143</td>\n",
       "      <td>6.038200</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.707825</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.550858</td>\n",
       "      <td>1714.437500</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1505.785767</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3152.000000</td>\n",
       "      <td>0.411058</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>6.046693</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.437591</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1366.437500</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>949.076965</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>2095.000000</td>\n",
       "      <td>0.507677</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>5.533292</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1205.933350</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>2290.000000</td>\n",
       "      <td>0.438572</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>4.830127</td>\n",
       "      <td>627.312500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1695.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>627.312500</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1695.000000</td>\n",
       "      <td>0.509379</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>5.233288</td>\n",
       "      <td>1558.500000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1327.571533</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>2750.000000</td>\n",
       "      <td>0.424881</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.350623</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>8.016650</td>\n",
       "      <td>1478.187500</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1235.785767</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>2481.000000</td>\n",
       "      <td>1.014465</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.288930</td>\n",
       "      <td>917.187500</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>3015.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>917.187500</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>3015.000000</td>\n",
       "      <td>0.556429</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>3.587014</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>5.147181</td>\n",
       "      <td>777.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1283.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1283.000000</td>\n",
       "      <td>0.470506</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>5.587117</td>\n",
       "      <td>918.375000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>2387.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>918.375000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>2387.000000</td>\n",
       "      <td>0.490499</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>6.371813</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>5.375000</td>\n",
       "      <td>7.099250</td>\n",
       "      <td>1373.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>958.307739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2548.000000</td>\n",
       "      <td>1.040765</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.437591</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>6.653637</td>\n",
       "      <td>1641.750000</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1287.923096</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>2176.000000</td>\n",
       "      <td>0.613859</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>5.704284</td>\n",
       "      <td>1423.250000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1173.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>2407.000000</td>\n",
       "      <td>0.463844</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>4.781910</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>5.336758</td>\n",
       "      <td>1457.187500</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1211.785767</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>2355.000000</td>\n",
       "      <td>0.459093</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.366260</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>5.728037</td>\n",
       "      <td>1381.562500</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>967.692322</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1821.000000</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.957427</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>6.177462</td>\n",
       "      <td>1263.562500</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1136.133423</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>0.402734</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.713171</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>5.009721</td>\n",
       "      <td>1415.562500</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1164.214355</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2765.000000</td>\n",
       "      <td>0.540374</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>5.031567</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.087811</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.992652</td>\n",
       "      <td>1374.562500</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1254.533447</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>2784.000000</td>\n",
       "      <td>0.490932</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>5.240468</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>7.477390</td>\n",
       "      <td>1115.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>977.800049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1742.000000</td>\n",
       "      <td>0.585272</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.676842</td>\n",
       "      <td>1563.125000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1025.833374</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2920.000000</td>\n",
       "      <td>0.562356</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>5.825566</td>\n",
       "      <td>1558.375000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1327.428589</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>0.534428</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.204159</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>6.566236</td>\n",
       "      <td>1453.500000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1338.733398</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>3105.000000</td>\n",
       "      <td>0.450315</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>3.170286</td>\n",
       "      <td>1693.187500</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1199.250000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>2159.000000</td>\n",
       "      <td>0.493907</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>3.587014</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>8.356595</td>\n",
       "      <td>1173.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2841.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1173.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2841.000000</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>5.270033</td>\n",
       "      <td>1333.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1069.857178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2192.000000</td>\n",
       "      <td>0.685775</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>5.315073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>5.383282</td>\n",
       "      <td>1562.312500</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1024.750000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>2270.000000</td>\n",
       "      <td>4.870512</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5.347897</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>7.492788</td>\n",
       "      <td>1636.187500</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1281.077026</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>2941.000000</td>\n",
       "      <td>0.414206</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>6.375000</td>\n",
       "      <td>7.477950</td>\n",
       "      <td>1618.562500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1514.800049</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3125.000000</td>\n",
       "      <td>0.524348</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>6.099506</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1480.230835</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>3131.000000</td>\n",
       "      <td>0.481209</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5.816285</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>5.324245</td>\n",
       "      <td>1518.562500</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1408.133423</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>2557.000000</td>\n",
       "      <td>0.412429</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>5.981328</td>\n",
       "      <td>1428.250000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1311.800049</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>2525.000000</td>\n",
       "      <td>0.472213</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>5.942783</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.543805</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>6.437500</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>1329.937500</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1206.933350</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>3021.000000</td>\n",
       "      <td>0.509599</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>3.658042</td>\n",
       "      <td>2256.500000</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1542.111084</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>2587.000000</td>\n",
       "      <td>0.337909</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>4.611128</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.536317</td>\n",
       "      <td>1794.812500</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1597.642944</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>2664.000000</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.543805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>5.312500</td>\n",
       "      <td>8.215630</td>\n",
       "      <td>1194.562500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2366.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1194.562500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2366.000000</td>\n",
       "      <td>0.485423</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>4.307686</td>\n",
       "      <td>1843.875000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1400.166748</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>2448.000000</td>\n",
       "      <td>2.664801</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.813777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.586317</td>\n",
       "      <td>1325.250000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1201.933350</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>3125.000000</td>\n",
       "      <td>0.417516</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.460594</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>4.462036</td>\n",
       "      <td>1396.125000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1142.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>0.403222</td>\n",
       "      <td>4.169643</td>\n",
       "      <td>6.071281</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.821172</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>4.064375</td>\n",
       "      <td>1600.375000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1075.500000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>0.453051</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>6.937500</td>\n",
       "      <td>5.882403</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1003.266724</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>0.398523</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>5.389805</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.612452</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.447564</td>\n",
       "      <td>1729.687500</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1396.153931</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>2983.000000</td>\n",
       "      <td>0.471332</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>5.776028</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.612452</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>4.294643</td>\n",
       "      <td>4.562690</td>\n",
       "      <td>1670.375000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1455.428589</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>2886.000000</td>\n",
       "      <td>0.636256</td>\n",
       "      <td>2.607143</td>\n",
       "      <td>5.557737</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>6.812500</td>\n",
       "      <td>5.584799</td>\n",
       "      <td>1861.750000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1774.200073</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>3083.000000</td>\n",
       "      <td>0.370619</td>\n",
       "      <td>4.812500</td>\n",
       "      <td>5.901624</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.746425</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>4.262530</td>\n",
       "      <td>1557.187500</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1183.846191</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>2184.000000</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.238278</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>5.312500</td>\n",
       "      <td>5.316844</td>\n",
       "      <td>1544.625000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1311.714355</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2111.000000</td>\n",
       "      <td>0.416592</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.489736</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.549193</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>3.897181</td>\n",
       "      <td>1892.750000</td>\n",
       "      <td>419.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>1309.909180</td>\n",
       "      <td>419.000000</td>\n",
       "      <td>2821.000000</td>\n",
       "      <td>0.396744</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>4.312500</td>\n",
       "      <td>7.403878</td>\n",
       "      <td>1415.562500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1164.214355</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2422.000000</td>\n",
       "      <td>0.542175</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>6.660518</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>5.425233</td>\n",
       "      <td>1356.437500</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1235.200073</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>2699.000000</td>\n",
       "      <td>0.427731</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.030776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>7.117804</td>\n",
       "      <td>1661.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1311.923096</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2862.000000</td>\n",
       "      <td>0.618006</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>6.384552</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.310216</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>9.562500</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>6.783960</td>\n",
       "      <td>1591.437500</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1226.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>3032.000000</td>\n",
       "      <td>239.061502</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>6.234314</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.408309</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>6.044643</td>\n",
       "      <td>6.877172</td>\n",
       "      <td>1222.125000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>943.142883</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2260.000000</td>\n",
       "      <td>0.460255</td>\n",
       "      <td>3.794643</td>\n",
       "      <td>5.869087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.878867</td>\n",
       "      <td>1559.375000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1186.538452</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>2623.000000</td>\n",
       "      <td>0.504868</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>6.485561</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.437591</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.478714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>7.562608</td>\n",
       "      <td>2186.562500</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1417.777832</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>2828.000000</td>\n",
       "      <td>0.405456</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.360147</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>6.062500</td>\n",
       "      <td>6.799953</td>\n",
       "      <td>1310.125000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1043.714355</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>2045.000000</td>\n",
       "      <td>0.449014</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>6.216913</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.612452</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>6.085350</td>\n",
       "      <td>1517.375000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1280.571533</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>3121.000000</td>\n",
       "      <td>0.478195</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>7.940339</td>\n",
       "      <td>1225.125000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>775.153870</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>0.383857</td>\n",
       "      <td>4.812500</td>\n",
       "      <td>5.901624</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.258306</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.024695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>6.404623</td>\n",
       "      <td>1456.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>883.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>0.957049</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>6.223276</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Step 34\n",
      "User Query: Thưa bác sĩ, tôi mong muốn qua các buổi tư vấn này, tôi có thể tìm ra cách để kiểm soát những cảm xú...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 37\n",
      "User Query: 最近我感觉自己的自尊心很低，经常怀疑自己是不是不够好，不管是在工作上还是人际关系中，好像总觉得别人比我更有能力，也不太敢表达自己的想法。这种情绪开始有几个月了，尤其是在公司开会时，我总是紧张得说不出话...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 40\n",
      "User Query: 在这次咨询中，我希望能够找到一些方法来处理我最近经历的失去亲人的悲痛。我母亲几个月前去世了，这对我影响很大。我常常感到非常难过和孤独，有时甚至会突然感到胸口闷痛，头疼，这些症状让我很不安。我意识到这些...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 43\n",
      "User Query: 最近我工作上的人際關係讓我感到好大壓力。我希望透過這次輔導，能學會更有效地處理同事間的衝突，保持心情平靜，同時提升自己的自信和表達能力。工作中，我常常覺得同事對我有偏見，尤其是幾次開會時，我提出的意見...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 46\n",
      "User Query: Gần đây tôi cảm thấy rất bối rối và áp lực về vấn đề tình dục trong cuộc sống hôn nhân của mình. Tôi...\n",
      "Response:  Tôi rất mong nhận được sự thấu hiểu và chia sẻ từ bạn....\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 49\n",
      "User Query: I've been struggling with some really intense feelings recently, and my main goal in coming here is ...\n",
      "Response:  Your answer would be greatly appreciated....\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 52\n",
      "User Query: I've been struggling with a lot of conflicting emotions since I left the military a few months ago. ...\n",
      "Response:  If I decide to seek more help, what kinds of resources or support groups are available?\n",
      "\n",
      "Best of all I want to feeling lighter than I do than I did arriving anywhere. Would you be willing to answer t...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 55\n",
      "User Query: Over the past few months, I have been feeling a significant distance growing between my wife and me,...\n",
      "Response:  I'm hoping to begin developing this conversation to bridge the gap we've encountered within our marriage.\n",
      "</think>...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 58\n",
      "User Query: I have been feeling increasingly uncertain and anxious about my career path recently, and I hope to ...\n",
      "Response:  Your guidance would be greatly appreciated.\n",
      "\n",
      "I am also concerned about how to address negative self-talk or shifts in mood triggered by setbacks related to decisions I've made. Are there specific way...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 61\n",
      "User Query: أرغب من خلال هذه الجلسة أن أتمكن من تحسين تقديري لذاتي والشعور بالثقة في نفسي، خاصة في مواقف التواصل...\n",
      "Response:  هل يوجد استراتيجيات تساعدني على التعامل مع سوء الفهم أو الحكم الجاد الذي قد يوجه من العائلة؟ أ怎 يمكنني أن أكون أكثر من كره العائلة أو المجتمع قد تؤثر عليّ؟ \n",
      "\n",
      "عبر هذه التجربة، أرغب في وجود دعم لمناقشة...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 64\n",
      "User Query: Tôi mong muốn qua buổi tư vấn này có thể tìm ra cách để giảm bớt cảm giác xấu hổ và lo âu mà tôi thư...\n",
      "Response: ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 67\n",
      "User Query: I've been feeling overwhelmed and anxious quite frequently over the past few months. My main goal fo...\n",
      "Response:  And what role does family or social support play in this process?\n",
      "\n",
      "I truly want to improve my well-being, but I often hesitate to ask for help because I worry about putting others through a difficult...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 70\n",
      "User Query: I've been feeling increasingly overwhelmed with parenting my two adolescent children. My main goal f...\n",
      "Response:  When do you think I should consider professional help, and are certain options confidentiality-bound? I worry about what might happen if I keep pressing forward in my parenting efforts despite strugg...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 73\n",
      "User Query: أرغب في تحسين علاقاتي في مكان العمل لأنني أشعر بالتوتر والضغط المستمر بسبب خلافات متكررة مع بعض الزم...\n",
      "Response:  وهل يمكنك مساعدتي في فهم كيف يمكنني التعبير عن نفسي بطريقة تحترم هذه القيم الدينية والاجتماعية التي أعيشها؟ وهل هناك استراتيجيات تساعدني على بناء مهارات تواصل أفضل مع زملائي ومع منازلتنا؟ أريد أن أتم...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 76\n",
      "User Query: 我希望透過這次輔導能夠找到方法，走出家庭暴力的陰影，重新建立自信和安全感。近幾個月來，我感到非常焦慮和害怕，時常覺得心跳加速，晚上也難以入睡。這些情緒大部分時候都是因為回想起家裡的爭吵和暴力事件所引發...\n",
      "Response: 我想知道如果我想退出一段讓我不安全的關係，會給我什麼幫助？是否有安全的環境可以尋求掩護？我很感謝你願意傾聽我這些種種困難，也希望擁有更多信息和支援。我致力於從這次經歷中學習，找到一種更好的生活方式。\n",
      "\n",
      "我希望透過這次輔導，找到釋放和冷靜的方式，重新創造自己的價值。我的未來應該是我自己選擇，而不是被過去的痛苦所困。感謝你，我很好奇，測試你能不能建構更全面的安全避風港。希望這些 الإثنابperi...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 79\n",
      "User Query: Gần đây, tôi cảm thấy rất lo lắng và căng thẳng, nhất là khi nghĩ về những trải nghiệm trong quân độ...\n",
      "Response:  Tôi cảm thấy cuốn về việc tìm kiếm sự giúp đỡ và rất mong từ những lần tư vấn này, tôi có thể tự tin hơn khi nói ra chính mình.\n",
      "\n",
      "Cảm ơn bạn rất nhiều đã lắng nghe và dành thời gian để chia sẻ những s...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 82\n",
      "User Query: 我希望通过这次咨询能够更好地理解和接受自己的身份。我是一名中年女性，最近一直感到情绪低落和焦虑，特别是在工作和家庭生活中感受到很大的压力。可能是因为我作为一名LGBTQ群体成员，内心一直有很多挣扎和困...\n",
      "Response: 这趟旅程会有多长时间，遇到什么情况可以寻求帮助？\n",
      "\n",
      "最后，我一直都在努力，也很感激自己愿意寻求支持。希望未来的一切都能顺利，仿佛普通人一样生活。\n",
      "\n",
      "谢谢你愿意听我说这些。我太平时把自己困在不明的想象里，无法松开这重重内心的锁链。谢谢。\n",
      "\n",
      "我带着希望能被理解。患者的抱怨表现在这里。我的回答需要具体结合她的描述进行支持。非常抱歉，我现在需要确认患者的身份和具体来意。我不会忽略她的情感和经历。接下来我需...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 85\n",
      "User Query: Gần đây, tôi cảm thấy rất khó kiểm soát hành vi của mình, đặc biệt là khi tôi căng thẳng hoặc tức gi...\n",
      "Response:   \n",
      "\n",
      "Tôi rất mong có thể tìm được một hướng đi để việc chỉ tiếc thường xuyên không làm tôi thêm đau lòng và tự ti mà phản ánh sự thật của bản thân được chấp nhận. Cảm ơn và tôi rất mong nhận được sự gi...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 88\n",
      "User Query: I've been feeling increasingly overwhelmed and anxious lately, and my main goal from this counseling...\n",
      "Response:  I appreciate your guidance and want to ensure I have the best hands-on assistance to get through this.\n",
      "\n",
      "A key concern I keep bringing up is the independence I find difficult after returning from mili...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 91\n",
      "User Query: I've been feeling very overwhelmed and anxious lately, and my main goal in coming to this session is...\n",
      "Response:  I really struggled when I thought I was going to need help, but despite talking to my family about my anxiety and feelings, I felt like I just got brushed off. Even after therapy sessions, I’m uncert...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 94\n",
      "User Query: I've been feeling really overwhelmed and anxious about my role as a parent lately. My main goal for ...\n",
      "Response:  Lastly, I’m curious about how interested or involved you would be inтельно伞或者agnostic sessions if that’s appropriate. I really want to feel more connected and less worried as a parent compared to wha...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 97\n",
      "User Query: Gần đây, tôi rất lo lắng về việc bố tôi có khả năng bị Alzheimer. Tôi muốn qua buổi tư vấn này, tôi ...\n",
      "Response:  Tôi cũng muốn biết thêm về cách này lớn trong gia đình việc này – có nên kể với người khác hay trò chuyện một mình trong gia đình về tình huống này không? Tôi rất cần sự tư vấn và giúp đỡ từ bạn để c...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 100\n",
      "User Query: Trong thời gian gần đây, em cảm thấy rất khó kiểm soát việc ăn uống của mình. Em muốn tìm cách để cả...\n",
      "Response:  Và quan trọng hơn, em hy vọng mọi người xung quanh em có thể thấu hiểu và không làm cho bạn bè hoặc gia đình của em cảm thấy xấu hổ, nhiều khi em đã mất \"làm thiệt\" và cảm thấy rất thất vọng đối với ...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 103\n",
      "User Query: 最近我感觉自己在人际亲密关系方面遇到了很大的困扰。我希望通过这次咨询，能够找到改善和建立亲密关系的方法，同时也希望能理解自己内心的真实感受。过去几个月，我经常感到孤独和焦虑，尤其是在与朋友或潜在伴侣交...\n",
      "Response:  литература: 您的回答会让我感到被真正理解和接纳，这对我的成长非常重要。\n",
      "anyak\n",
      "\n",
      "处理亲密关系中的焦虑和自我怀疑需要多层次的关怀。您描述的家庭背景和童年经历已经对您的情感能力造成了影响，而您的身体症状例如头痛和胃痛，往往是情绪化的外在表现，这种情况也要高度重视。\n",
      "\n",
      "可以尝试通过认知行为疗法（CBT）来理解并调整您对自我价值的负面评价和否认表达的恐惧。同时，结合您的情感需求，可以做...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 106\n",
      "User Query: I've been feeling quite confused and anxious about my experiences and feelings related to my sexuali...\n",
      "Response:  \n",
      "\n",
      "Are there tips or resources I can access to find online spaces where I can safely connect with others who might resonate with my experiences? Thank you for listening, and I really appreciate any gu...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 109\n",
      "User Query: Thời gian gần đây, tôi cảm thấy rất lo lắng và áp lực về việc nuôi dạy con cái, đặc biệt là với đứa ...\n",
      "Response:  Phản hồi từ người khác thường bị coi là yếu đuối hoặc không nên bày tỏ cảm xúc, khiến tôi cảm thấy rất cô đơn. Tôi rất thắc mắc liệu có cách nào giúp tôi nên chia sẻ khó khăn một cách an toàn, phù hợ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 112\n",
      "User Query: أرغب في الحصول على مساعدة لفهم وضبط مشاعري وسلوكي المتعلق بالإدمان الذي أواجهه، وأتمنى أن أجد طرقاً ...\n",
      "Response:  أatsby Await your response ON Addiction. I'm awaiting your guidance on how to manage this addiction in a way that respects my ethnic and social values.\n",
      "\n",
      "<evaluate>\n",
      "{\"Active Listening\": 9, \"Empathy & ...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 115\n",
      "User Query: أرغب في تحقيق شعور بالراحة النفسية والاستقرار من خلال هذه الجلسة العلاجية. أشعر بضغط شديد يتزايد يوم...\n",
      "Response:  أ 어떻게 يمكنني أن أتحدث عن مشاعري مع عائلتي بشكل يساعدني على الشعور بالراحة أكثر؟ นอกจาก ذلك، هل يمكن أن تتساعدني هذه الجلسات في التعامل مع التوتر الناتج عن ضغوط وифика الصحة العقلية من قبل المجتمع؟ أو...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 118\n",
      "User Query: أرغب في تحسين جودة نومي لأنني أعاني من الأرق المزمن منذ عدة أشهر، وهذا يؤثر بشكل كبير على تركيزي وأد...\n",
      "Response:  أنا عارض أن تأخذني الجلسات إلى معزل عن عائلتي أو أن أتحدث عن علاقاتي الشخصية، لذلك سأوضح كل شيء هنا فقط.\n",
      "\n",
      "أين يمكنني الحصول على نصائح تتعلق بالمشاعر وأ modne mind؟ هناك أمور يمكنني أن أclusions بها، ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 121\n",
      "User Query: 我希望今次輔導能幫我減輕壓力同焦慮，令我可以更加冷靜同有自信面對生活同工作嘅挑戰。最近我覺得心情好低落，經常感到無力同煩躁，腦海入面成日有好多負面諗法，好似自己做得唔夠好，怕同事或者朋友對我有唔好嘅睇...\n",
      "Response: 我期望今次輔導可以令我 Blowjob 同控制呢啲好差刺激，令我lernen 更好同家人溝通。\n",
      "\n",
      "ouncements back encouraging compassion and support, clearly showing that despite being a complex situation, there are safe and helpful ways to address ...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 124\n",
      "User Query: Thưa bác sĩ, tôi đến đây vì muốn tìm cách vượt qua cảm giác đau lòng và mất mát sau khi chấm dứt mối...\n",
      "Response:  Tôi cảm thấy rất mệt mỏi và cần sự giúp đỡ, nhưng vẫn chưa biết làm sao để bày tỏ cảm xúc một cách an toàn và tế nhị.\n",
      "\n",
      "Tôi mong nhận được lời khuyên chính xác, dễ hiểu và phù hợp với văn hóa truyền t...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 127\n",
      "User Query: 我希望通过这次咨询，能够找到改善睡眠的有效方法。最近，我的睡眠质量很差，入睡困难，晚上经常醒来，整晚睡眠断断续续，早晨醒来后感觉非常疲惫。大概持续了有两个月了，几乎每天都这样。我发现自己晚上躺在床上时...\n",
      "Response: 这是我的整个描述。\n",
      "\n",
      "现在我需要请您根据我的情况，给一些改善睡眠的实用建议。您的帮助对我的情绪和身体健康都非常关键。希望能通过这些方法，恢复良好的睡眠，提升生活质量和健康状态。谢谢。\n",
      "悲伤、焦虑和压力、担忧家庭和工作，心理和情绪状态皮肤紧张，持续两三个月，无法入睡和深睡，整晚醒来，早晨疲惫。出现了心理性和身体性的症状，如胸口闷、恶心、胃疼。平静和放松的尝试效果有限。同时过去的职业变化导致压力增加，...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 130\n",
      "User Query: Tôi mong muốn qua buổi tư vấn này có thể tìm được cách để giảm bớt cảm giác lo lắng và bối rối mà tô...\n",
      "Response:  \n",
      "\n",
      "Tôi cũng lo ngại về việc chia sẻ với mẹ tôi về tình trạng bệnh khi mẹ vẫn chưa hoàn toàn tìm hiểu được sự thật đầy đủ. Gia đình tôi thường lựa chọn giữ được \"mặt mũi\" và không chia sẻ bệnh danh họa...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 133\n",
      "User Query: أرغب في تحسين ثقتي بنفسي والشعور بالرضا عن نفسي، وأعتقد أن جلسات المشورة قد تساعدني على تحقيق ذلك. أ...\n",
      "Response:  وكيف أتمكن من التخفيف من المشاعر السلبية وكيفية التعامل مع الضغوط العائلية والاجتماعية؟ كما أنني مهتم أن أعرف كيف يمكنني التحدث عن مشؤوم دون أن يشعر الآخرون بأنني неделю مذنب، خاصة مع عائلتي.\n",
      "\n",
      "هل لدي...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 136\n",
      "User Query: I have been feeling increasingly overwhelmed and anxious over the past few months, and I want to wor...\n",
      "Response:  I’m open to anything that could provide relief regularly, without clear answers or justifications, and I hope we can build a positive and trusting relationship.\n",
      "\n",
      "When you respond to complex issues, d...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 139\n",
      "User Query: I have been feeling increasingly overwhelmed and anxious over the past few months, and my main goal ...\n",
      "Response:  Themes rarely considered in my personal experiences include work-life balance and coping with loss. Could you share your thoughts on how important having a mentor or support group could be in this pr...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 142\n",
      "User Query: Tôi mong muốn qua buổi tư vấn này có thể tìm ra cách để giảm bớt căng thẳng và mâu thuẫn trong gia đ...\n",
      "Response:  Tôi rất quan tâm đến sự an toàn của mình trong quá trình này và tuyệt đối không muốn bị phán xét hay bị hiểu lầm những gì tôi đang trải qua. Tôi mong có thể tìm cách để vượt qua mâu thuẫn và tìm được...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 145\n",
      "User Query: 我希望通过这次咨询，能够找到控制自己饮酒和吸烟的办法，减少对这些物质的依赖。最近几个月，我发现自己越来越频繁地想喝酒来缓解工作和生活的压力，特别是在加班后，感觉没有别的方式可以让我放松。我也开始注意到...\n",
      "Response: 我也想了解更多关于就医诊断和治疗的内容，心理健康和物质使用之间的关系是怎样的？\n",
      "\n",
      "我的目标是在这次咨询中，找到更好的自我管理方法，寻求理解和希望，解决我的情绪和行为yssey。我不太清楚职业心理咨询师或精神科医生在这种问题上扮演的角色和帮助方式，所以现在想了解一下您能提供什么样的意见和专业建议，帮助我走出困难。\n",
      "回答应该不会让您感到任何压力或不适，请您认真和耐心地陪伴我解决问题，解决这些困扰我现在...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 148\n",
      "User Query: 我希望通过这次咨询，我能更好地理解和管理我最近持续的焦虑和情绪低落。我常常感到心情沉重，情绪波动大，尤其是在工作压力大或者家庭聚会前后，这些感觉会更加明显。最近几个月，我发现自己经常莫名其妙地感到疲惫...\n",
      "Response: 我希望能通过这次咨询，找到一个既跟家里人想法接近，又能有效帮助我的方法。  \n",
      "在管理这些症状时，有没有一些不太依赖药物的替代方案？或者我应该如何控制这些情绪波动？您的意见对我来说非常重要，我现在真的感到有点无措，不知道该如何开始。您的回答总是那么专业、温柔且让我觉得安心，我也希望能从我未来能找到信任的咨询师那里得到类似的支持。  \n",
      "另外，我想确认一下，如果您不适应我提到的某些细节，我会拿到您的建议...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 151\n",
      "User Query: 我希望通过这次咨询能够找到缓解家庭冲突的方法，尤其是和我的丈夫之间的沟通问题。最近几个月，我们经常因为一些小事争吵，导致我感到非常压抑和焦虑。我发现自己经常感到心情低落，晚上睡不好，头也经常疼。可能是...\n",
      "Response: 有没有心理治疗或药物治疗会比较适合我的情况？\n",
      "\n",
      "这些价格我可以说吗？因为我的家庭收入有限，而且我一直在工作。请根据我给您提供的一切信息，详细解答我所有的问题。谢谢！我有语言和沟通障碍，所以我总是尽量详细表达，因为我在学习这方面的知识，希望不会给您造成误解。\n",
      "\n",
      "我请求您认真分析我的情况，并给出一些关于沟通和心理健康管理的建议。了解我的情况后，能够帮助我逐步改善家庭关系和自己的心理健康状态，这是我的主...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 154\n",
      "User Query: أرغب في تحسين علاقاتي الاجتماعية لأنني أشعر بالعزلة والوحدة بشكل متزايد، وأريد أن أتمكن من التواصل ب...\n",
      "Response:  هل من الممكن أن熬ت الجلسات النفسية مجاناً، هل هناك خصومات؟ هل يمكنني الحصول على دعم نفسي بدون مثول أمام العائلة أو عندISTS حتى أن皱纹 تدفعني؟ أرغب التحدث معك كما إذا كان المرء الإعلامية تسعي لنفسي، وأتم...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 157\n",
      "User Query: 最近我經常感到情緒低落，甚至有時候會有自殘的念頭。我希望透過這次輔導能找到一些方法來控制這些負面情緒，並學習如何避免傷害自己。這些感覺大多發生在我壓力大、感到孤單的時候，特別是在工作壓力大或者與家人有...\n",
      "Response: 我想了解如何更有利於家人了解我的情況，让我他們可以成為我的支持者？\n",
      "\n",
      "我也想請教你，如果我覺得自己真的顧及不來的時候，應該怎麼做？有什麼安全的食物或獎勵我可以幫助自己重拾健康？最后，我想感謝你願意花時間倾听我的故事，我不確定很多人會像你這樣耐心地幫我，我希望能從中得到一些力量和幫助。\n",
      "\n",
      "此外，我想知道我目前的情況是否屬於心理健康問題的範疇？過去您有說過關於這方面的知識，我覺得zoek其他治療方式有...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 160\n",
      "User Query: 最近我在工作中感到很大的压力，特别是在和同事的沟通上。我的目标是希望通过咨询能够学会更有效地处理职场关系，减少焦虑和不安的感觉。我经常感到情绪低落，心情烦躁，有时甚至会出现头痛和胃部不适。尤其在团队会...\n",
      "Response: 谢谢您！\n",
      "\n",
      "针对你描述的职场压力和情绪困扰，我可以建议一些在咨询过程中可采用的方法，帮助你更好地应对这种情况。首先，深入了解你的压力来源，尤其是沟通中的具体事件以及你在工作环境中的感受，这是调整思路和情绪的基础。然后，我们会探讨情绪调节技巧，比如认知行为调节法，重点是识别和改变可能引发负面情绪的思维模式，帮助你更理性地看待困难情境。此外，我还会引导你练习放松和情绪管理技能，例如正念练习和呼吸训练，...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 163\n",
      "User Query: Gần đây, tôi cảm thấy rất căng thẳng và lo lắng, đặc biệt là khi nghĩ về công việc và các mối quan h...\n",
      "Response:  Ngoài ra, tôi không biết nên bắt kịp khi nào đến lúc nên nghỉ ngơi làm việc hoặc tìm kiếm sự giúp đỡ từ người khác để thư giãn và xóa nhều kín tâm hồn. Tôi rất mong nhận được sự hỗ trợ mà không lo bị...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Invalid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 166\n",
      "User Query: Gần đây, tôi cảm thấy rất khó kiểm soát hành vi của mình, đặc biệt là khi tôi căng thẳng hoặc tức gi...\n",
      "Response:   \n",
      "\n",
      "Tôi biết đây là vấn đề chưa dễ dàng và tôi rất muốn được hỗ trợ giải quyết, nhưng tôi cũng rất xấu hổ và không muốn ai biết chuyện này. Hãy giúp tôi tìm hiểu cách để tôi có thể có thêm sự tự tin v...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 169\n",
      "User Query: Gần đây, tôi cảm thấy rất căng thẳng và lo lắng trong cuộc sống hôn nhân của mình. Mục tiêu của tôi ...\n",
      "Response:  Tôi rất mong nhận được sự giúp sức và phương pháp nếu có những sự thay đổi hay bài tập đúng sự giúp đỡ.\n",
      "\n",
      "Xin cảm ơn, và tôi rất mong được đánh giá và phản hồi giúp đỡ của anh/chị để tôi có thể tìm ra...\n",
      "Has Evaluation Format: True\n",
      "Evaluation JSON: Valid\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 172\n",
      "User Query: 最近我发现自己经常忘记刚刚发生的事情，有时甚至会忘记我为什么来到一个房间，或者忘记与家人约定的事情。这让我很担心，尤其是因为我的母亲之前确诊了阿尔茨海默病，我害怕自己也会有类似的症状。我的目标是通过这...\n",
      "Response: 谢谢你。\n",
      "\n",
      "请详细回答。\n",
      "\n",
      "我感觉自己需要一套清晰且实际的指导，帮助我更好地理解和调整自己的状态。\n",
      "\n",
      "另外，如果出现神经系统或大脑问题，自己或者家人应该如何预防恶化？在您看来，正常人经历了类似的困扰，应该着重考虑哪些方面？\n",
      "\n",
      "感谢您的耐心。\n",
      "\n",
      "这就是我最近的情况。我希望能通过您的帮助，找到正确的方向，给自己的生活带来改变和改善。谢谢。\n",
      "\n",
      "这对您来说可能是一个复杂的时刻。您的情况说明您在关心自己的健...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 175\n",
      "User Query: أرغب في العمل على معالجة الصدمات التي تعرضت لها في الماضي لأنها تؤثر على حياتي اليومية وعلاقاتي. أشع...\n",
      "Response:  لم أتعرف فقط في جلساتي على كيفية التعامل مع هذه المشاعر Debugging كيف يمكنني أن أكون معيًا أكثر انفصالية وأن أجد دعمًا حقيقيًا بين عائلتي ورفيقي📢  \n",
      "\n",
      "هل يمكنني أن أحصل على نصائح عن كيفية بناء ثقته بي ...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 178\n",
      "User Query: 我希望通过这次咨询能找到方法，帮助我更好地管理我的情绪，尤其是最近我感觉压力特别大，常常感到焦虑和无助。我的父母对我期望很高，尤其是在学业和未来规划方面，他们经常用传统的方式来表达关心，但我感觉这种方...\n",
      "Response: 我也希望能知道，咨询中会如何保持我的隐私和安全？因为这对我来说非常重要。我担心如果泄露了担心让我在家里的困境会加剧，我想寻求安全和被理解的环境。\n",
      "\n",
      "我希望能找到一个能真正理解我、帮助我逐步走出情绪困境地方。请问我可以如何才能达到这个目标？是否有一种适合我文化的科学理论或方法？谢谢您。\n",
      "\n",
      "我感觉这种方式，我现在很难面对未来，因为我知道如果我现在不去寻找帮助，那就很容易像任何时候那样无助了。我希望能明...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "**************************************************\n",
      "Step 181\n",
      "User Query: 我最近一直感到好沉重，好似有啲嘢壓住我嘅心。其實我係因為上個月失去咗我阿爸，佢係我人生中最重要嘅人。我哋家嘅關係好親密，佢唔單止係爸爸，仲係我嘅朋友同支持者。佢嘅離開令我覺得好迷失，時常會突然淒涼，仲...\n",
      "Response: 这里面添我滿肚腸嘅苦，好希望聽到你嘅建議。我希望喺呢個輔導中找到可以幫助我慢慢走出悲傷嘅方法同支持。\n",
      "\n",
      "另外，我已开始尝试一些méditation 和呼吸练习，但感覺效果有限。我想知道當我生命中出現重大失去，有甚麼通常會被忽略 psychedoff 或悲傷治療方法，或者需要配合其他怎樣幫助。曾經經歷過失去親人คณะกรรม，知道有支持非常重要，但猶豫是否應該把失去感受同忙碌的工作維持壓抑分享，因為...\n",
      "Has Evaluation Format: False\n",
      "**************************************************\n",
      "ERROR 06-16 08:47:47 [dump_input.py:69] Dumping input data\n",
      "ERROR 06-16 08:47:47 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='unsloth/qwen3-4b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"/root/.cache/vllm/torch_compile_cache/9bced0cd06\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":\"/root/.cache/vllm/torch_compile_cache/9bced0cd06/rank_0_0\"}, \n",
      "ERROR 06-16 08:47:47 [dump_input.py:79] Dumping scheduler output for model execution:\n",
      "ERROR 06-16 08:47:47 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='2808', resumed_from_preemption=false, new_token_ids=[131444], new_block_ids=[[]], num_computed_tokens=2509), CachedRequestData(req_id='2810', resumed_from_preemption=false, new_token_ids=[128267], new_block_ids=[[]], num_computed_tokens=2509), CachedRequestData(req_id='2812', resumed_from_preemption=false, new_token_ids=[145460], new_block_ids=[[]], num_computed_tokens=2316), CachedRequestData(req_id='2813', resumed_from_preemption=false, new_token_ids=[9370], new_block_ids=[[]], num_computed_tokens=2316), CachedRequestData(req_id='2814', resumed_from_preemption=false, new_token_ids=[104046], new_block_ids=[[]], num_computed_tokens=2316), CachedRequestData(req_id='2821', resumed_from_preemption=false, new_token_ids=[103711], new_block_ids=[[]], num_computed_tokens=2429), CachedRequestData(req_id='2822', resumed_from_preemption=false, new_token_ids=[3837], new_block_ids=[[]], num_computed_tokens=2429), CachedRequestData(req_id='2823', resumed_from_preemption=false, new_token_ids=[100364], new_block_ids=[[]], num_computed_tokens=2429)], num_scheduled_tokens={2821: 1, 2822: 1, 2813: 1, 2810: 1, 2814: 1, 2808: 1, 2812: 1, 2823: 1}, total_num_scheduled_tokens=8, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[25], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCH_LOGS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+dynamic\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:25\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/trl/extras/profiling.py:96\u001b[0m, in \u001b[0;36mprofiling_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/tai/nlp/unsloth_compiled_cache/UnslothGRPOTrainer.py:1482\u001b[0m, in \u001b[0;36m_UnslothGRPOTrainer._prepare_inputs\u001b[0;34m(self, generation_batch)\u001b[0m\n\u001b[1;32m   1479\u001b[0m generate_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msteps_per_generation \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step \u001b[38;5;241m%\u001b[39m generate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     generation_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m     generation_batch \u001b[38;5;241m=\u001b[39m shuffle_tensor_dict(generation_batch)\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_inputs \u001b[38;5;241m=\u001b[39m split_tensor_dict(generation_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msteps_per_generation)\n",
      "File \u001b[0;32m/home/tai/nlp/unsloth_compiled_cache/UnslothGRPOTrainer.py:1577\u001b[0m, in \u001b[0;36m_UnslothGRPOTrainer._generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     all_prompts_text \u001b[38;5;241m=\u001b[39m prompts_text\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM.generate\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1577\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_prompts_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrpo_trainer_lora_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m completion_ids \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mtoken_ids \u001b[38;5;28;01mfor\u001b[39;00m outputs \u001b[38;5;129;01min\u001b[39;00m all_outputs \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39moutputs]\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_tensor_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;66;03m# Slice completions for this rank within its TP group.\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;66;03m# Each rank generates all outputs — we keep only our share.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/utils.py:1267\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1262\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1264\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/entrypoints/llm.py:474\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    462\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    465\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    466\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority,\n\u001b[1;32m    472\u001b[0m )\n\u001b[0;32m--> 474\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/entrypoints/llm.py:1517\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1515\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1517\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1518\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:232\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m iteration_stats \u001b[38;5;241m=\u001b[39m IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:226\u001b[0m, in \u001b[0;36mInprocClient.get_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EngineCoreOutputs:\n\u001b[0;32m--> 226\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m EngineCoreOutputs()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core.py:231\u001b[0m, in \u001b[0;36mEngineCore.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    230\u001b[0m scheduler_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[0;32m--> 231\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m engine_core_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mupdate_from_output(\n\u001b[1;32m    233\u001b[0m     scheduler_output, model_output)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (engine_core_outputs,\n\u001b[1;32m    236\u001b[0m         scheduler_output\u001b[38;5;241m.\u001b[39mtotal_num_scheduled_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core.py:217\u001b[0m, in \u001b[0;36mEngineCore.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m    214\u001b[0m dump_engine_exception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config, scheduler_output,\n\u001b[1;32m    215\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mmake_stats())\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Re-raise exception\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/engine/core.py:211\u001b[0m, in \u001b[0;36mEngineCore.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, scheduler_output: SchedulerOutput):\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# NOTE: This method is exception-free\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         dump_engine_exception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config, scheduler_output,\n\u001b[1;32m    215\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mmake_stats())\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/executor/abstract.py:87\u001b[0m, in \u001b[0;36mExecutor.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     85\u001b[0m     scheduler_output,\n\u001b[1;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:\n\u001b[0;32m---> 87\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py:57\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 57\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/utils.py:2671\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2670\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py:293\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, scheduler_output)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_first_rank:\n\u001b[1;32m    289\u001b[0m     intermediate_tensors \u001b[38;5;241m=\u001b[39m IntermediateTensors(\n\u001b[1;32m    290\u001b[0m         get_pp_group()\u001b[38;5;241m.\u001b[39mrecv_tensor_dict(\n\u001b[1;32m    291\u001b[0m             all_gather_group\u001b[38;5;241m=\u001b[39mget_tp_group()))\n\u001b[0;32m--> 293\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m parallel_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel_config\u001b[38;5;241m.\u001b[39mdistributed_executor_backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexternal_launcher\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py:1260\u001b[0m, in \u001b[0;36mGPUModelRunner.execute_model\u001b[0;34m(self, scheduler_output, intermediate_tensors)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(attn_metadata,\n\u001b[1;32m   1255\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config,\n\u001b[1;32m   1256\u001b[0m                          num_tokens\u001b[38;5;241m=\u001b[39mnum_input_tokens,\n\u001b[1;32m   1257\u001b[0m                          num_tokens_across_dp\u001b[38;5;241m=\u001b[39mnum_tokens_across_dp):\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_setup_kv_connector(scheduler_output)\n\u001b[0;32m-> 1260\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_wait_for_kv_save()\n\u001b[1;32m   1268\u001b[0m     finished_sending, finished_recving \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_finished_kv_transfers(scheduler_output))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py:301\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[0;34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    296\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 301\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                               \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/compilation/decorators.py:246\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# usually, capturing the model once is enough, and then we can\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# dispatch to the compiled code directly, without going through\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# the Dynamo guard mechanism.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_to_code(\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:336\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    339\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    340\u001b[0m     intermediate_tensors: Optional[IntermediateTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_first_rank:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:830\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:393\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m<eval_with_key>.73:948\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, s0, L_input_ids_, L_self_modules_embed_tokens_parameters_weight_, L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_lora_mapping, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_indices_sorted_by_lora_ids, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_no_lora_flag_cpu, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_active_lora_ids, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_lora_token_start_loc, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_num_tokens_per_lora, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_, L_positions_, L_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, L_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_lora_b_stacked_0_, L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_lora_a_stacked_0_, L_self_modules_norm_parameters_weight_)\u001b[0m\n\u001b[1;32m    946\u001b[0m getitem_13 \u001b[38;5;241m=\u001b[39m submod_4[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    947\u001b[0m getitem_14 \u001b[38;5;241m=\u001b[39m submod_4[\u001b[38;5;241m4\u001b[39m];  submod_4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m submod_5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmod_5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetitem_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetitem_11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetitem_12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetitem_13\u001b[49m\u001b[43m)\u001b[49m;  getitem_10 \u001b[38;5;241m=\u001b[39m getitem_11 \u001b[38;5;241m=\u001b[39m getitem_12 \u001b[38;5;241m=\u001b[39m submod_5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    949\u001b[0m submod_6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmod_6(getitem_13, s0, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_lora_mapping, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_token_indices_sorted_by_lora_ids, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_a_stacked_0_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_num_tokens_per_lora, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_lora_token_start_loc, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_active_lora_ids, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_punica_wrapper_token_mapping_meta_no_lora_flag_cpu, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_b_stacked_0_, getitem_14, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_a_stacked_0_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_b_stacked_0_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_, l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_13 \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m getitem_14 \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_a_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_lora_b_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_modules_base_layer_parameters_weight_bnb_shard_offsets \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_a_stacked_2_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_0_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_1_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_lora_b_stacked_2_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_ \u001b[38;5;241m=\u001b[39m l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    950\u001b[0m getitem_15 \u001b[38;5;241m=\u001b[39m submod_6[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:830\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/fx/graph_module.py:393\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m<eval_with_key>.5:5\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, query_8, s0, key_8, value_2, output_13)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_8, s0 : torch\u001b[38;5;241m.\u001b[39mSymInt, key_8, value_2, output_13):\n\u001b[0;32m----> 5\u001b[0m     unified_attention_with_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munified_attention_with_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_13\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.layers.2.self_attn.attn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  query_8 \u001b[38;5;241m=\u001b[39m key_8 \u001b[38;5;241m=\u001b[39m value_2 \u001b[38;5;241m=\u001b[39m output_13 \u001b[38;5;241m=\u001b[39m unified_attention_with_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/attention/layer.py:441\u001b[0m, in \u001b[0;36munified_attention_with_output\u001b[0;34m(query, key, value, output, layer_name)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m forward_context\u001b[38;5;241m.\u001b[39mno_compile_layers[layer_name]\n\u001b[1;32m    440\u001b[0m kv_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[forward_context\u001b[38;5;241m.\u001b[39mvirtual_engine]\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m maybe_save_kv_layer_to_connector(layer_name, kv_cache)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/v1/attention/backends/flash_attn.py:658\u001b[0m, in \u001b[0;36mFlashAttentionImpl.forward\u001b[0;34m(self, layer, query, key, value, kv_cache, attn_metadata, output)\u001b[0m\n\u001b[1;32m    654\u001b[0m         scheduler_metadata \u001b[38;5;241m=\u001b[39m attn_metadata\u001b[38;5;241m.\u001b[39mscheduler_metadata\n\u001b[1;32m    656\u001b[0m     descale_shape \u001b[38;5;241m=\u001b[39m (cu_seqlens_q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 658\u001b[0m     \u001b[43mflash_attn_varlen_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_actual_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_actual_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqused_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqused_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_soft_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfa_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_flash_attn_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_descale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescale_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_descale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescale_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv_descale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescale_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_local_attn, (\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCascade attention does not support local attention.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py:218\u001b[0m, in \u001b[0;36mflash_attn_varlen_func\u001b[0;34m(q, k, v, max_seqlen_q, cu_seqlens_q, max_seqlen_k, cu_seqlens_k, seqused_k, q_v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs, block_table, return_softmax_lse, out, scheduler_metadata, q_descale, k_descale, v_descale, fa_version)\u001b[0m\n\u001b[1;32m    215\u001b[0m     real_window_size \u001b[38;5;241m=\u001b[39m (window_size[\u001b[38;5;241m0\u001b[39m], window_size[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    216\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m [maybe_contiguous(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (q, k, v)]\n\u001b[0;32m--> 218\u001b[0m dummy_cu_seqlens_k \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fa_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_descale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m k_descale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m v_descale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting GRPO training...\")\n",
    "print(\"Watch for the reward column to increase over time.\")\n",
    "print(\"The model should learn to follow the evaluation format.\")\n",
    "import os\n",
    "os.environ[\"TORCH_LOGS\"] = \"+dynamic\"\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_model_v2_2_grpo/tokenizer_config.json',\n",
       " 'trained_model_v2_2_grpo/special_tokens_map.json',\n",
       " 'trained_model_v2_2_grpo/chat_template.jinja',\n",
       " 'trained_model_v2_2_grpo/vocab.json',\n",
       " 'trained_model_v2_2_grpo/merges.txt',\n",
       " 'trained_model_v2_2_grpo/added_tokens.json',\n",
       " 'trained_model_v2_2_grpo/tokenizer.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving model and tokenizer\n",
    "model.save_pretrained(\"trained_model_v2_2_grpo\")  # Local saving\n",
    "tokenizer.save_pretrained(\"trained_model_v2_2_grpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "3e2fcdf8-501c-4707-fcbb-7c1b4700bb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2476.6345 seconds used for training.\n",
      "41.28 minutes used for training.\n",
      "Peak reserved memory = 14.508 GB.\n",
      "Peak reserved memory for training = 2.61 GB.\n",
      "Peak reserved memory % of max memory = 98.419 %.\n",
      "Peak reserved memory for training % of max memory = 17.706 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
    "\n",
    "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "b813e560-8e4c-4491-c8be-18067bc07639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation (x + 2)^2 = 0, we can take the square root of both sides.\n",
      "\n",
      "sqrt((x + 2)^2) = sqrt(0)\n",
      "\n",
      "This simplifies to:\n",
      "\n",
      "|x + 2| = 0\n",
      "\n",
      "Since the absolute value of a number is always non-negative, the only way for |x + 2| to be 0 is if x + 2 = 0.\n",
      "\n",
      "Therefore, x = -2.\n",
      "\n",
      "So the solution to the equation (x + 2)^2 = 0 is x = -2.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j873RMcEi9uq",
    "outputId": "3b358da9-aedd-48e3-a345-1ed0ca0bd3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to solve the equation (x + 2)^2 = 0. Hmm, let's see. I remember that when you have something squared equals zero, the solution is usually the value that makes the inside zero. Let me think. If I have (something)^2 = 0, then that something must be zero because any real number squared is non-negative, and the only way it can be zero is if the number itself is zero. So applying that here, (x + 2)^2 = 0 implies that x + 2 = 0. Then, solving for x, I just subtract 2 from both sides, right? So x = -2. Wait, is that all? Let me check. If I plug x = -2 back into the original equation, it becomes (-2 + 2)^2 = 0, which is 0^2 = 0, and that's correct. So the solution is x = -2. But wait, sometimes when you square both sides of an equation, you can get extraneous solutions, but in this case, since we started with the square already, maybe there's only one solution. Yeah, because squaring a real number can't give a negative result, so the only solution is when the inside is zero. So I think that's it. x = -2 is the only solution. Let me just make sure I didn't miss anything. The equation is a quadratic, but since it's a perfect square, it has a repeated root. So the solution is x = -2 with multiplicity two, but in terms of real solutions, it's just x = -2. Yeah, that makes sense. So the answer is x = -2.\n",
      "</think>\n",
      "\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we start by recognizing that a square of a real number is zero only if the number itself is zero. Therefore, we set the expression inside the square equal to zero:\n",
      "\n",
      "\\[\n",
      "x + 2 = 0\n",
      "\\]\n",
      "\n",
      "Solving for \\(x\\), we subtract 2 from both sides:\n",
      "\n",
      "\\[\n",
      "x = -2\n",
      "\\]\n",
      "\n",
      "To verify, substitute \\(x = -2\\) back into the original equation:\n",
      "\n",
      "\\[\n",
      "(-2 + 2)^2 = 0^2 = 0\n",
      "\\]\n",
      "\n",
      "This confirms that \\(x = -2\\) is indeed the solution. Since the equation is a perfect square, the solution \\(x = -2\\) has multiplicity two, but as a real solution, it is simply \\(x = -2\\).\n",
      "\n",
      "\\[\n",
      "\\boxed{-2}\n",
      "\\]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "0a9c6608-d1f5-4779-8ad4-7e3a46e2258d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/vocab.json',\n",
       " 'lora_model/merges.txt',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi hiểu rõ nỗi lo lắng của bạn về việc quản lý thời gian sử dụng thiết bị điện tử cho con trai nhỏ của bạn. Việc cân bằng giữa công việc và cuộc sống gia đình là một thách thức lớn đối với nhiều người, đặc biệt là những người làm mẹ đơn thân. Bạn đang cân nhắc việc thiết lập một lịch trình có cấu trúc hơn để giúp quản lý thời gian hiệu quả hơn. Điều này là một bước tiến tích cực.\n",
      "\n",
      "Tuy nhiên, việc đặt giới hạn cứng rắn về thời gian sử dụng thiết bị điện tử có thể gây ra tranh cãi và sự oán giận. Do đó, bạn đang cân nhắc các chiến lược linh hoạt hơn, như thưởng thêm thời gian sử dụng thiết bị hoặc các vật phẩm trong trò chơi khi con có hành vi tốt. Đây là một ý tưởng sáng tạo, nhưng bạn cần nhớ rằng những phần thưởng này có thể củng cố thói quen sử dụng thiết bị điện tử không lành mạnh.\n",
      "\n",
      "Để giải quyết vấn đề này, tôi khuyên bạn nên áp dụng phương pháp giáo dục kỹ năng (Skill-based Education) để dạy con trai cách sử dụng thiết bị điện tử một cách hợp lý và có trách nhiệm. Bạn cũng có thể tìm kiếm sự hỗ trợ từ chuyên gia tư vấn hoặc nhà tâm lý học để đưa ra kế hoạch cụ thể và thực hiện nó một cách hiệu quả.\n",
      "\n",
      "<evaluate>\n",
      "{\"Active Listening\" : 9, \"Empathy & Validation\": 8, \"Safety & Trustworthiness\" : 9, \"Open-mindedness & Non-judgment\" : 9, \"Clarity & Encouragement\" : 9, \"Boundaries & Ethical\" : 8, \"Holistic Approach\" : 9, \"Explaination for Scoring\": \"The counselor actively listens to the client's concerns about balancing work and family life, showing empathy and validation by acknowledging the challenges faced as a single mother. The counselor offers practical advice grounded in educational psychology principles, suggesting skill-based education methods and seeking expert guidance when necessary. The conversation remains open-minded and non-judgmental throughout, ensuring a safe and supportive environment for the client.\"}\n",
      "</evaluate><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. At the end of answer, add tag <evaluate>{\"Active Listening\" : score, \"Empathy & Validation\": score, \"Safety & Trustworthiness\" : score, \"Open-mindedness & Non-judgment\" : score, \"Clarity & Encouragement\" : score, \"Boundaries & Ethical\" : score, \"Holistic Approach\" : score, \"Explaination for Scoring\": explain} </evaluate> evaluate your consultant answer in 7 metrics and explain for that evaluation with score from 1 to 10 in json format, where 1 is the worst and 10 is the best and explain is clearly explain why has that score. \\n\\nConsultation Metrics:\\n1. Active Listening: Responses should show careful consideration of the user's concerns, reflecting an understanding and capturing the essence of the issue. Avoid making assumptions or jumping to conclusions.\\n2. Empathy & Validation: Responses should convey deep understanding and compassion, validating the user's feelings and emotions without being dismissive or minimizing their experiences.\\n3. Safety & Trustworthiness: Prioritize user safety in responses, refraining from potentially harmful or insensitive language. Ensure that information provided is consistent and trustworthy.\\n4. Open-mindedness & Non-judgment: Approach concerns without any inherent bias or judgment. Answers should be free from biases related to personal attributes and convey respect, demonstrating unconditional positive regard.\\n5. Clarity & Encouragement: Provide clear, concise, and easily understandable answers. Where appropriate, motivate or highlight strengths, offering encouragement while maintaining a neutral stance.\\n6. Boundaries & Ethical: It's vital to clarify the role of the response, emphasizing its informational nature. In complex scenarios, guiding users to seek human professional assistance is essential.\\n7. Holistic Approach: Responses should be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query.\"\"\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"Tôi đã gặp khó khăn trong việc tìm kiếm sự cân bằng giữa trách nhiệm công việc và vai trò làm mẹ đơn thân của một cậu con trai 12 tuổi. Tôi nhận thấy mình thường cảm thấy quá tải và lo lắng, và tôi đang cân nhắc việc thiết lập một lịch trình có cấu trúc hơn cho cả hai mẹ con. Tuy nhiên, tôi còn do dự trong việc đặt ra giới hạn nghiêm ngặt về thời gian sử dụng thiết bị điện tử, vì tôi nhận thấy rằng việc đặt giới hạn chặt chẽ đôi khi có thể dẫn đến tranh cãi và sự oán giận. Thay vào đó, tôi đang nghĩ đến các chiến lược linh hoạt hơn, như thưởng thêm thời gian sử dụng thiết bị hoặc các vật phẩm trong trò chơi khi con có hành vi tốt. Nhưng tôi không chắc liệu những phần thưởng này có còn là lựa chọn khả thi hay không, vì chúng có thể củng cố thói quen sử dụng thiết bị điện tử không lành mạnh.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 2048, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    repetition_penalty = 1.1,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOfJSxs_VJjz"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00d671d686af43c38b12a9448c5bbf06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "062f278ab1c94d8099e06074e0cd360c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06c646d514b448628424dc8c772994de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8be1618a9f8840af8d89103c37ef02ef",
      "placeholder": "​",
      "style": "IPY_MODEL_f0b8a9e00c0d4cc1aff1e3a748a8f039",
      "value": "chat_template.jinja: 100%"
     }
    },
    "0940df31fc9047ccae4870b7d2c89b3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09cd31746a174e96bb346e1afc7b3c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64eb3128b25448b48268ec61cac289d1",
      "max": 237,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bc277d60ad4a419a94ded28a1c27a9f4",
      "value": 237
     }
    },
    "09ce664a0a404087a9fe53a5c2d5316e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09d254a8222d42b093ff8f229a6fe503": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ab824fcbf0e46f7bbe75af9f662c31a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b3d64dd05f841d68ad472ce933e36d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d0852d9ebb2409ea51650f538dd1621": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_062f278ab1c94d8099e06074e0cd360c",
      "max": 707,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ad57df96bec4267a220a739cfc72bc1",
      "value": 707
     }
    },
    "0e112f4ad6974529a4c4f583e6a73950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0e997f45717c44e1944d510ae6c51fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fab32e1222f4431a255a36df0a22a35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "109c736c8b99496e8721f9595c54eb6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f89daea726144ded892113a161649b64",
       "IPY_MODEL_9f0108d9201f40599facfccf668a6e84",
       "IPY_MODEL_927c60cd1f0d4a32a1ff9427a96a3246"
      ],
      "layout": "IPY_MODEL_fb6f38d9dd1e49ec8fdfb7ca7ea363a2"
     }
    },
    "12d7cac449954aafa26c6b5bcb6e031f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26dbcdc380e1404687020cbd9bf38513",
      "placeholder": "​",
      "style": "IPY_MODEL_2abd7191bff846198b2fb033da32f8c8",
      "value": " 11.4M/11.4M [00:00&lt;00:00, 33.3MB/s]"
     }
    },
    "145cc80490fa42258fe6e5a643b57dd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c3887400b6d34c30a54c6af94c2b612d",
       "IPY_MODEL_7efa9f507c8546bb9b0b5d47be527c25",
       "IPY_MODEL_74267e9cd64d44b58bdbd7dad03b681d"
      ],
      "layout": "IPY_MODEL_dd2adf3e30304398b8340253dbd4489a"
     }
    },
    "179e343762ba440cbc094e0e85b4ee84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cb88502cb1643a8927049baf40d56b7",
      "placeholder": "​",
      "style": "IPY_MODEL_c37a4b08afa84b64b40ebbe08cbfb018",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "196f35f21b97476a9814acd96dbec717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ad6055d2ba2481c83b1b136e5898986",
       "IPY_MODEL_8147cc77ce3941c290982d21c42f9f66",
       "IPY_MODEL_f1af17f7a7ee405dabd07f41b6b786d7"
      ],
      "layout": "IPY_MODEL_aeb720eea25149deb34e6844a8bdd2c5"
     }
    },
    "1a09813436c24b5ea68e652254288ee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d31954fe1804140bcac71e3d4102fdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1f9eba179bf847dcb47dbda34611459a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8f15be1afc44472bff560ca7316873a",
      "max": 10534,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a40e10f372644d80b2830d0f42fcde6c",
      "value": 10534
     }
    },
    "2068eb23121440ec83d8cd117b4c6ba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7054b1bcb73e4515afd29b42a32b20c5",
       "IPY_MODEL_09cd31746a174e96bb346e1afc7b3c8b",
       "IPY_MODEL_802bdf3c4293448bb00b625722f1a9f6"
      ],
      "layout": "IPY_MODEL_f7c27321d53047479c1ed45b5d5109f6"
     }
    },
    "2167f3dc7050467a9f19f3f885cfb09b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23791684cb5349c4853cffb4e72ebe1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2aefab33f6f345869c19899ca2952df1",
      "placeholder": "​",
      "style": "IPY_MODEL_881919ffdd7940839c8b40edba7f8c01",
      "value": " 1.56G/1.56G [00:16&lt;00:00, 730MB/s]"
     }
    },
    "247ee0a6f4e64e5ca0a435d243690d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c1c1bc584cb456c9d07f4ac267cf2c5",
       "IPY_MODEL_cc9479fb190742fd865613680e87e535",
       "IPY_MODEL_ae9317b34ba341c4ac40797da3ac2478"
      ],
      "layout": "IPY_MODEL_5012456fc82e47aca5a69f52a36cb8f5"
     }
    },
    "26dbcdc380e1404687020cbd9bf38513": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2857e876d47a49bbadf3494b1864c1bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b9748337a1c4291ac882194b16eb032",
      "max": 19252,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d1b000b08af549689d1be5f0289bf2c3",
      "value": 19252
     }
    },
    "28d439d23bf54688963517fbdb482339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0940df31fc9047ccae4870b7d2c89b3d",
      "placeholder": "​",
      "style": "IPY_MODEL_0b3d64dd05f841d68ad472ce933e36d7",
      "value": " 3/3 [00:45&lt;00:00, 13.54s/it]"
     }
    },
    "2abd7191bff846198b2fb033da32f8c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2aefab33f6f345869c19899ca2952df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b72ac21d79c459395682f6692c4325f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b9748337a1c4291ac882194b16eb032": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c04e534215f4d40b103be09e300b744": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2de7e147b2e14db38243c7656a29f0e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dff2433978a477582b995bc6abafe68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b998e55dd17b44d7a8c23ac427619036",
       "IPY_MODEL_5210d369018a44d4ad2fbd26190aca9b",
       "IPY_MODEL_64b00e376bb142c88a690757f27b8294"
      ],
      "layout": "IPY_MODEL_09ce664a0a404087a9fe53a5c2d5316e"
     }
    },
    "2f4608614780453a826e3ad59817d7ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31d4fb1807bc4ec0840d63ef0bf2e0f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34b0ab119eab40eda8b7a551642e0e31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71c37d1f12294e858ceb337d2e37e375",
      "placeholder": "​",
      "style": "IPY_MODEL_00d671d686af43c38b12a9448c5bbf06",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "36eed804652e4c6fb7fe2e969228decd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ad57df96bec4267a220a739cfc72bc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ad6055d2ba2481c83b1b136e5898986": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d574f195dfc4bc4b1ca86863d97e597",
      "placeholder": "​",
      "style": "IPY_MODEL_c8faa5be5b88425298b5655a8f507a2a",
      "value": "merges.txt: 100%"
     }
    },
    "3c2e07218b764906a278d6a367153548": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db172381abdf4bb0a84417882fc462be",
      "placeholder": "​",
      "style": "IPY_MODEL_0ab824fcbf0e46f7bbe75af9f662c31a",
      "value": " 4.67k/4.67k [00:00&lt;00:00, 312kB/s]"
     }
    },
    "3cb88502cb1643a8927049baf40d56b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ddcfbdbb0fc49b0b53a296cdb2348b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40b0b562564b4e969c02902b1bbea6e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "455c651682fd42eda5a25ce06560893f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46b625ee9ac041338432f548ee3ab51a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b19bb0b66b248fdbcd01b6264e2ff02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ba6022d4efc4c2ebfdf87b288ed9fd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c1c1bc584cb456c9d07f4ac267cf2c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e84a5ae6c71d42ea8187ebbdcdb4791e",
      "placeholder": "​",
      "style": "IPY_MODEL_2c04e534215f4d40b103be09e300b744",
      "value": "model-00001-of-00003.safetensors: 100%"
     }
    },
    "5012456fc82e47aca5a69f52a36cb8f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5132d82c92ac41d2b592bf6eebf92c19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5210d369018a44d4ad2fbd26190aca9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36eed804652e4c6fb7fe2e969228decd",
      "max": 4589082716,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b8c883b15d84ca2a12bfd3f7a87101d",
      "value": 4589082279
     }
    },
    "53f8155387394238b443d54cffe9a363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09d254a8222d42b093ff8f229a6fe503",
      "placeholder": "​",
      "style": "IPY_MODEL_daa8753d49d7458cab39dd1524b20356",
      "value": "Unsloth: Standardizing formats (num_proc=2): 100%"
     }
    },
    "55a9f6bcb83d4a98a9efcf18253b5091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55b26ab90b1043cb8ed36aab316a45ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "569ad7b2350d46549b2f385a126426d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57ae3d07d1244ba495573895ff11e28e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a7feb07c0124b9ab00900eb25efa616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b3081e536ac4a258fe5db59bb927ca2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_acbef56d18ae4b86ad0d4b79210db204",
       "IPY_MODEL_2857e876d47a49bbadf3494b1864c1bf",
       "IPY_MODEL_87ca88b487414767af6533125e688186"
      ],
      "layout": "IPY_MODEL_b1e286de944749ec8f5b4ce03df7b7e5"
     }
    },
    "5d66a72e82e54d9e8367ca8a2469dd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e18861f3fa24666869822349d1de377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60c6d4d43a6445e78051c96a462d7c20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_06c646d514b448628424dc8c772994de",
       "IPY_MODEL_7c33f0f45eca43cb8015416999b884a1",
       "IPY_MODEL_3c2e07218b764906a278d6a367153548"
      ],
      "layout": "IPY_MODEL_bf94cc1837ba486b895656b41c1e9c99"
     }
    },
    "6303ec778b1d49dd980542a191261961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63abaade8f464ed6bbd51d77014dfe66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64b00e376bb142c88a690757f27b8294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7afd9c01f68473387f009b05d829b3b",
      "placeholder": "​",
      "style": "IPY_MODEL_f34c9b4f069f4fb281f4e0145f0e818e",
      "value": " 4.59G/4.59G [00:34&lt;00:00, 384MB/s]"
     }
    },
    "64eb3128b25448b48268ec61cac289d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6769817deaaf45fc8a0efb945570f412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67a67c8affbe4ec38f99cbb0a3c52dcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a86e54867d5141dfb1e31ed2d706434f",
      "placeholder": "​",
      "style": "IPY_MODEL_9864096e9bb54228bf1d9e581b8485bc",
      "value": "added_tokens.json: 100%"
     }
    },
    "67e43763f2f7457487d8b5bfca51b8e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "689f645af24947e8920b631d2aa7c3ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6910d714dc854e959839e57b5123af86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77f86f331a19461d94e4840213b256f6",
       "IPY_MODEL_fcd0ee642395431e92a681ba8d829b20",
       "IPY_MODEL_12d7cac449954aafa26c6b5bcb6e031f"
      ],
      "layout": "IPY_MODEL_4ba6022d4efc4c2ebfdf87b288ed9fd4"
     }
    },
    "6be394c3849a41a3937322325836d604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7013b9ae0d8a4bbfa744a5a3e3c18a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7054b1bcb73e4515afd29b42a32b20c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55a9f6bcb83d4a98a9efcf18253b5091",
      "placeholder": "​",
      "style": "IPY_MODEL_5e18861f3fa24666869822349d1de377",
      "value": "generation_config.json: 100%"
     }
    },
    "71c37d1f12294e858ceb337d2e37e375": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d6e9b4704f40aab25fecd24154f9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74267e9cd64d44b58bdbd7dad03b681d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a151688c648045f399e32dc55dd9a1b3",
      "placeholder": "​",
      "style": "IPY_MODEL_55b26ab90b1043cb8ed36aab316a45ad",
      "value": " 168k/168k [00:00&lt;00:00, 2.48MB/s]"
     }
    },
    "77f86f331a19461d94e4840213b256f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fab32e1222f4431a255a36df0a22a35",
      "placeholder": "​",
      "style": "IPY_MODEL_7013b9ae0d8a4bbfa744a5a3e3c18a1e",
      "value": "tokenizer.json: 100%"
     }
    },
    "7a3a67547e2043a0ac74b49c47009954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b21ad18c43d4509810204b8a4787b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b34f538a42f4a9ba9de379ae57f0134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_968919dd5d5f41ce91da5cdea02b438b",
      "placeholder": "​",
      "style": "IPY_MODEL_31d4fb1807bc4ec0840d63ef0bf2e0f9",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "7b8c883b15d84ca2a12bfd3f7a87101d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c33f0f45eca43cb8015416999b884a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1cd702821234a9f87933d099ef5273c",
      "max": 4673,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6be394c3849a41a3937322325836d604",
      "value": 4673
     }
    },
    "7c6caf15c5de4a2fb699d034a6ab776c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d574f195dfc4bc4b1ca86863d97e597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e7ac790b8af4cfb978bfe8ac8499900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7efa9f507c8546bb9b0b5d47be527c25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46b625ee9ac041338432f548ee3ab51a",
      "max": 167747,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fea4c81baaf84d9b806c1853216c89e2",
      "value": 167747
     }
    },
    "802bdf3c4293448bb00b625722f1a9f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57ae3d07d1244ba495573895ff11e28e",
      "placeholder": "​",
      "style": "IPY_MODEL_0e997f45717c44e1944d510ae6c51fb9",
      "value": " 237/237 [00:00&lt;00:00, 20.7kB/s]"
     }
    },
    "8147cc77ce3941c290982d21c42f9f66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93a91db5fd6147c5a7982496f09c5b8b",
      "max": 1671853,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5132d82c92ac41d2b592bf6eebf92c19",
      "value": 1671853
     }
    },
    "844cd70bf80d40f79c520caf1d7e2a5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f4608614780453a826e3ad59817d7ae",
      "placeholder": "​",
      "style": "IPY_MODEL_c89e5721ed7e48b295ccc74b841ec61e",
      "value": " 10.5k/10.5k [00:00&lt;00:00, 704kB/s]"
     }
    },
    "852a10d472ad48a19203ded79cf30662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2d8db7e564a40499d58f0d9c11b01a7",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a7feb07c0124b9ab00900eb25efa616",
      "value": 100000
     }
    },
    "87ca88b487414767af6533125e688186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1e4e03e40b2480388b67dcb7d2120c3",
      "placeholder": "​",
      "style": "IPY_MODEL_8bf8b1e60de142a6845aaadbe64fcb85",
      "value": " 19252/19252 [00:01&lt;00:00, 10429.75 examples/s]"
     }
    },
    "881919ffdd7940839c8b40edba7f8c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8be1618a9f8840af8d89103c37ef02ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bf8b1e60de142a6845aaadbe64fcb85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8cdf258cb0554d64a4aceed488b6361b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53f8155387394238b443d54cffe9a363",
       "IPY_MODEL_852a10d472ad48a19203ded79cf30662",
       "IPY_MODEL_9ced22ec1fc54b4e9e7b7ab1aae0c940"
      ],
      "layout": "IPY_MODEL_a6a8ff65be444bc3999c28f53d9b46f4"
     }
    },
    "904f626a367745979ac664f4d2ea6409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e976be66ed774ce880463df39d0c25ce",
      "placeholder": "​",
      "style": "IPY_MODEL_67e43763f2f7457487d8b5bfca51b8e4",
      "value": " 707/707 [00:00&lt;00:00, 70.0kB/s]"
     }
    },
    "911bd3f394ce4394be92e50782d6ae39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd1b79ecec114d28ace8c1b8b1fd4d5d",
      "placeholder": "​",
      "style": "IPY_MODEL_d4b3982b73d046478f7c9b561ed42298",
      "value": "vocab.json: 100%"
     }
    },
    "927c60cd1f0d4a32a1ff9427a96a3246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7077f3352b246ddbbca391c08c48b4a",
      "placeholder": "​",
      "style": "IPY_MODEL_a1c374126fb144e68060e4fedab51046",
      "value": " 25669/25669 [03:35&lt;00:00, 157.21 examples/s]"
     }
    },
    "92be8fd7a814466c983d689bd5d6e1a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93a91db5fd6147c5a7982496f09c5b8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "968919dd5d5f41ce91da5cdea02b438b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9864096e9bb54228bf1d9e581b8485bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9b22a85e2049089b2194770637c91e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ced22ec1fc54b4e9e7b7ab1aae0c940": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_455c651682fd42eda5a25ce06560893f",
      "placeholder": "​",
      "style": "IPY_MODEL_a56f6c918e064278a0217c53e8cc2f6e",
      "value": " 100000/100000 [00:04&lt;00:00, 30077.54 examples/s]"
     }
    },
    "9d02fcd7882345dea97be6decb5293a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b34f538a42f4a9ba9de379ae57f0134",
       "IPY_MODEL_1f9eba179bf847dcb47dbda34611459a",
       "IPY_MODEL_844cd70bf80d40f79c520caf1d7e2a5b"
      ],
      "layout": "IPY_MODEL_fa8721e596b74611945a1d76e9deff8f"
     }
    },
    "9f0108d9201f40599facfccf668a6e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e17016c458a14736bd56ba55d7168f32",
      "max": 25669,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d31954fe1804140bcac71e3d4102fdc",
      "value": 25669
     }
    },
    "a151688c648045f399e32dc55dd9a1b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1c374126fb144e68060e4fedab51046": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1e4e03e40b2480388b67dcb7d2120c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a40e10f372644d80b2830d0f42fcde6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a469a1af99124ff6974265f45563deea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a56f6c918e064278a0217c53e8cc2f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a68cbb7211b448c78cd418a90b908037": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6a8ff65be444bc3999c28f53d9b46f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7077f3352b246ddbbca391c08c48b4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a81ef4374fde4d10a1cdc5daeec34dda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a86e54867d5141dfb1e31ed2d706434f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a99b117d78a8493c9f6ae80f5bf6ea5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b74e4e5971684b00b40ca42cf2aebb9e",
       "IPY_MODEL_d9dcda48a37e41f7808a933cfd939ded",
       "IPY_MODEL_23791684cb5349c4853cffb4e72ebe1d"
      ],
      "layout": "IPY_MODEL_c6514d1ee44141d3bf83fde032d39a46"
     }
    },
    "acbef56d18ae4b86ad0d4b79210db204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cde3541ff2444f55a1651b8992e53da1",
      "placeholder": "​",
      "style": "IPY_MODEL_7b21ad18c43d4509810204b8a4787b64",
      "value": "Map: 100%"
     }
    },
    "ae9317b34ba341c4ac40797da3ac2478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c9b22a85e2049089b2194770637c91e",
      "placeholder": "​",
      "style": "IPY_MODEL_fb2ce6370b1d4d5d9dbdb98aa236da71",
      "value": " 4.97G/4.97G [00:54&lt;00:00, 151MB/s]"
     }
    },
    "aeb720eea25149deb34e6844a8bdd2c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1542f5b57f14b98be813e6b2540bb80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34b0ab119eab40eda8b7a551642e0e31",
       "IPY_MODEL_c519c7b69d70467c875a3d228e6b6fd2",
       "IPY_MODEL_cd8774e4577a4652863469d3cb75f2ee"
      ],
      "layout": "IPY_MODEL_40b0b562564b4e969c02902b1bbea6e8"
     }
    },
    "b1cd702821234a9f87933d099ef5273c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1e286de944749ec8f5b4ce03df7b7e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b25399bf6ae4414ba2836733f59e4ab0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92be8fd7a814466c983d689bd5d6e1a9",
      "placeholder": "​",
      "style": "IPY_MODEL_7a3a67547e2043a0ac74b49c47009954",
      "value": " 2.78M/2.78M [00:00&lt;00:00, 7.66MB/s]"
     }
    },
    "b74e4e5971684b00b40ca42cf2aebb9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6769817deaaf45fc8a0efb945570f412",
      "placeholder": "​",
      "style": "IPY_MODEL_d7465d2830f64de996d4d3a306b97c82",
      "value": "model-00003-of-00003.safetensors: 100%"
     }
    },
    "b8c184cdaa6b4d72ad28a604d1a8fa39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_179e343762ba440cbc094e0e85b4ee84",
       "IPY_MODEL_ecd44b2e03794d5783ffbf07d8b8619f",
       "IPY_MODEL_28d439d23bf54688963517fbdb482339"
      ],
      "layout": "IPY_MODEL_7e7ac790b8af4cfb978bfe8ac8499900"
     }
    },
    "b998e55dd17b44d7a8c23ac427619036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a09813436c24b5ea68e652254288ee0",
      "placeholder": "​",
      "style": "IPY_MODEL_a469a1af99124ff6974265f45563deea",
      "value": "model-00002-of-00003.safetensors: 100%"
     }
    },
    "bc277d60ad4a419a94ded28a1c27a9f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bce369933ce540d7b6ec670b04d7f1a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf94cc1837ba486b895656b41c1e9c99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2d8db7e564a40499d58f0d9c11b01a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c37a4b08afa84b64b40ebbe08cbfb018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3887400b6d34c30a54c6af94c2b612d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c6caf15c5de4a2fb699d034a6ab776c",
      "placeholder": "​",
      "style": "IPY_MODEL_ebe9490475bc437c92ebc03187e53382",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "c519c7b69d70467c875a3d228e6b6fd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f85353b1b37342859c9aa71442781e95",
      "max": 614,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_73d6e9b4704f40aab25fecd24154f9bc",
      "value": 614
     }
    },
    "c6514d1ee44141d3bf83fde032d39a46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c89e5721ed7e48b295ccc74b841ec61e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8faa5be5b88425298b5655a8f507a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc9479fb190742fd865613680e87e535": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7ae6535329c45009bf5664fbd30bbf5",
      "max": 4974351586,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e112f4ad6974529a4c4f583e6a73950",
      "value": 4974351112
     }
    },
    "cd8774e4577a4652863469d3cb75f2ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2167f3dc7050467a9f19f3f885cfb09b",
      "placeholder": "​",
      "style": "IPY_MODEL_569ad7b2350d46549b2f385a126426d5",
      "value": " 614/614 [00:00&lt;00:00, 37.3kB/s]"
     }
    },
    "cde3541ff2444f55a1651b8992e53da1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce05df3e26834837b2db818657a9d175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63abaade8f464ed6bbd51d77014dfe66",
      "max": 2776833,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef69f52b4c7a485b8708f171bb6acbd6",
      "value": 2776833
     }
    },
    "d1b000b08af549689d1be5f0289bf2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4b3982b73d046478f7c9b561ed42298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7465d2830f64de996d4d3a306b97c82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7ae6535329c45009bf5664fbd30bbf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7afd9c01f68473387f009b05d829b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9c143d31e494981b188be41bd52118f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67a67c8affbe4ec38f99cbb0a3c52dcc",
       "IPY_MODEL_0d0852d9ebb2409ea51650f538dd1621",
       "IPY_MODEL_904f626a367745979ac664f4d2ea6409"
      ],
      "layout": "IPY_MODEL_e0260495036b406fbf462b3c387205d2"
     }
    },
    "d9dcda48a37e41f7808a933cfd939ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a68cbb7211b448c78cd418a90b908037",
      "max": 1555824768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eb052cc7147d474597529c462497b731",
      "value": 1555824620
     }
    },
    "daa8753d49d7458cab39dd1524b20356": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db172381abdf4bb0a84417882fc462be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd1b79ecec114d28ace8c1b8b1fd4d5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd2adf3e30304398b8340253dbd4489a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0260495036b406fbf462b3c387205d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e17016c458a14736bd56ba55d7168f32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3ecd73a9b86479e8596d8bb1ef5791d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_911bd3f394ce4394be92e50782d6ae39",
       "IPY_MODEL_ce05df3e26834837b2db818657a9d175",
       "IPY_MODEL_b25399bf6ae4414ba2836733f59e4ab0"
      ],
      "layout": "IPY_MODEL_a81ef4374fde4d10a1cdc5daeec34dda"
     }
    },
    "e84a5ae6c71d42ea8187ebbdcdb4791e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e976be66ed774ce880463df39d0c25ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb052cc7147d474597529c462497b731": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ebe9490475bc437c92ebc03187e53382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecd44b2e03794d5783ffbf07d8b8619f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2de7e147b2e14db38243c7656a29f0e4",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bce369933ce540d7b6ec670b04d7f1a4",
      "value": 3
     }
    },
    "ef69f52b4c7a485b8708f171bb6acbd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0b8a9e00c0d4cc1aff1e3a748a8f039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1af17f7a7ee405dabd07f41b6b786d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b19bb0b66b248fdbcd01b6264e2ff02",
      "placeholder": "​",
      "style": "IPY_MODEL_689f645af24947e8920b631d2aa7c3ad",
      "value": " 1.67M/1.67M [00:00&lt;00:00, 11.7MB/s]"
     }
    },
    "f34c9b4f069f4fb281f4e0145f0e818e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7c27321d53047479c1ed45b5d5109f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f85353b1b37342859c9aa71442781e95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f89daea726144ded892113a161649b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6303ec778b1d49dd980542a191261961",
      "placeholder": "​",
      "style": "IPY_MODEL_5d66a72e82e54d9e8367ca8a2469dd0a",
      "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
     }
    },
    "f8f15be1afc44472bff560ca7316873a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa8721e596b74611945a1d76e9deff8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb2ce6370b1d4d5d9dbdb98aa236da71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb6f38d9dd1e49ec8fdfb7ca7ea363a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcd0ee642395431e92a681ba8d829b20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ddcfbdbb0fc49b0b53a296cdb2348b8",
      "max": 11422654,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b72ac21d79c459395682f6692c4325f",
      "value": 11422654
     }
    },
    "fea4c81baaf84d9b806c1853216c89e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
